2025-12-19 23:56:09,742 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-12-19 23:56:12,118 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-12-19 23:56:12,120 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-12-19 23:56:12,122 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-12-19 23:56:12,822 - vllm.entrypoints.openai.api_server - INFO - vLLM API server version 0.9.0.2.dev0+g5fbbfe9a4.d20251004
2025-12-19 23:56:13,397 - vllm.entrypoints.openai.cli_args - INFO - non-default args: {'model': 'Qwen/Qwen3-32B', 'trust_remote_code': True, 'max_model_len': 8192, 'enforce_eager': True, 'tensor_parallel_size': 4, 'swap_space': 16.0, 'max_num_seqs': 512, 'preemption_mode': 'swap'}
2025-12-19 23:56:18,029 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-12-19 23:56:20,209 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-12-19 23:56:20,211 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-12-19 23:56:20,212 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-12-19 23:56:21,999 - vllm.config - INFO - This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify', 'score'}. Defaulting to 'generate'.
2025-12-19 23:56:22,075 - vllm.config - INFO - Defaulting to use mp for distributed inference
2025-12-19 23:56:22,076 - vllm.platforms.cuda - WARNING - To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
2025-12-19 23:56:22,084 - vllm.entrypoints.openai.api_server - INFO - Started engine process with PID 1343047
2025-12-19 23:56:26,070 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-12-19 23:56:28,433 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-12-19 23:56:28,435 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-12-19 23:56:28,436 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-12-19 23:56:28,440 - vllm.engine.llm_engine - INFO - Initializing a V0 LLM engine (v0.9.0.2.dev0+g5fbbfe9a4.d20251004) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=Qwen/Qwen3-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={"compile_sizes": [], "cudagraph_capture_sizes": [], "max_capture_size": 0}, use_cached_outputs=True, 
2025-12-19 23:56:29,356 - vllm.executor.multiproc_worker_utils - WARNING - Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
2025-12-19 23:56:30,293 - vllm.platforms.cuda - INFO - Using Flash Attention backend.
2025-12-19 23:56:33,451 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-12-19 23:56:33,524 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-12-19 23:56:33,535 - vllm.platforms - INFO - Automatically detected platform cuda.
2025-12-19 23:56:35,796 - vllm.executor.multiproc_worker_utils - INFO - Worker ready; awaiting tasks
2025-12-19 23:56:35,813 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-12-19 23:56:35,814 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-12-19 23:56:35,815 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-12-19 23:56:35,929 - vllm.executor.multiproc_worker_utils - INFO - Worker ready; awaiting tasks
2025-12-19 23:56:35,944 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-12-19 23:56:35,946 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-12-19 23:56:35,947 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-12-19 23:56:35,992 - vllm.executor.multiproc_worker_utils - INFO - Worker ready; awaiting tasks
2025-12-19 23:56:36,007 - vllm.plugins - INFO - Available plugins for group vllm.general_plugins:
2025-12-19 23:56:36,008 - vllm.plugins - INFO - - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver
2025-12-19 23:56:36,009 - vllm.plugins - INFO - All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
2025-12-19 23:56:36,606 - vllm.platforms.cuda - INFO - Using Flash Attention backend.
2025-12-19 23:56:36,724 - vllm.platforms.cuda - INFO - Using Flash Attention backend.
2025-12-19 23:56:36,778 - vllm.platforms.cuda - INFO - Using Flash Attention backend.
2025-12-19 23:56:40,757 - vllm.utils - INFO - Found nccl from library libnccl.so.2
2025-12-19 23:56:40,757 - vllm.utils - INFO - Found nccl from library libnccl.so.2
2025-12-19 23:56:40,758 - vllm.utils - INFO - Found nccl from library libnccl.so.2
2025-12-19 23:56:40,758 - vllm.utils - INFO - Found nccl from library libnccl.so.2
2025-12-19 23:56:40,760 - vllm.distributed.device_communicators.pynccl - INFO - vLLM is using nccl==2.21.5
2025-12-19 23:56:40,761 - vllm.distributed.device_communicators.pynccl - INFO - vLLM is using nccl==2.21.5
2025-12-19 23:56:40,761 - vllm.distributed.device_communicators.pynccl - INFO - vLLM is using nccl==2.21.5
2025-12-19 23:56:40,761 - vllm.distributed.device_communicators.pynccl - INFO - vLLM is using nccl==2.21.5
2025-12-19 23:56:43,087 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - reading GPU P2P access cache from /global/homes/k/kvp4/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
2025-12-19 23:56:43,088 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - reading GPU P2P access cache from /global/homes/k/kvp4/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
2025-12-19 23:56:43,088 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - reading GPU P2P access cache from /global/homes/k/kvp4/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
2025-12-19 23:56:43,088 - vllm.distributed.device_communicators.custom_all_reduce_utils - INFO - reading GPU P2P access cache from /global/homes/k/kvp4/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json
2025-12-19 23:56:43,110 - vllm.distributed.device_communicators.shm_broadcast - INFO - vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_de27cf2a'), local_subscribe_addr='ipc:///tmp/fbaedb9e-1c7c-40c8-b55c-9c761884ddf0', remote_subscribe_addr=None, remote_addr_ipv6=False)
2025-12-19 23:56:43,126 - vllm.distributed.parallel_state - INFO - rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
2025-12-19 23:56:43,126 - vllm.distributed.parallel_state - INFO - rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2
2025-12-19 23:56:43,126 - vllm.distributed.parallel_state - INFO - rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3
2025-12-19 23:56:43,126 - vllm.distributed.parallel_state - INFO - rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
2025-12-19 23:56:43,128 - vllm.worker.model_runner - INFO - Starting to load model Qwen/Qwen3-32B...
2025-12-19 23:56:43,129 - vllm.worker.model_runner - INFO - Starting to load model Qwen/Qwen3-32B...
2025-12-19 23:56:43,129 - vllm.worker.model_runner - INFO - Starting to load model Qwen/Qwen3-32B...
2025-12-19 23:56:43,129 - vllm.worker.model_runner - INFO - Starting to load model Qwen/Qwen3-32B...
2025-12-19 23:56:43,653 - vllm.model_executor.model_loader.weight_utils - INFO - Using model weights format ['*.safetensors']
2025-12-19 23:56:43,781 - vllm.model_executor.model_loader.weight_utils - INFO - Using model weights format ['*.safetensors']
2025-12-19 23:56:43,788 - vllm.model_executor.model_loader.weight_utils - INFO - Using model weights format ['*.safetensors']
2025-12-19 23:56:43,789 - vllm.model_executor.model_loader.weight_utils - INFO - Using model weights format ['*.safetensors']
2025-12-19 23:57:14,370 - vllm.model_executor.model_loader.default_loader - INFO - Loading weights took 29.49 seconds
2025-12-19 23:57:14,390 - vllm.model_executor.model_loader.default_loader - INFO - Loading weights took 30.18 seconds
2025-12-19 23:57:14,414 - vllm.model_executor.model_loader.default_loader - INFO - Loading weights took 29.84 seconds
2025-12-19 23:57:14,422 - vllm.model_executor.model_loader.default_loader - INFO - Loading weights took 30.49 seconds
2025-12-19 23:57:14,600 - vllm.worker.model_runner - INFO - Model loading took 15.3920 GiB and 31.253031 seconds
2025-12-19 23:57:14,635 - vllm.worker.model_runner - INFO - Model loading took 15.3920 GiB and 31.294151 seconds
2025-12-19 23:57:14,639 - vllm.worker.model_runner - INFO - Model loading took 15.3920 GiB and 31.270902 seconds
2025-12-19 23:57:14,723 - vllm.worker.model_runner - INFO - Model loading took 15.3920 GiB and 31.304677 seconds
2025-12-19 23:57:19,914 - vllm.worker.worker - INFO - Memory profiling takes 5.01 seconds
the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB
model weights take 15.39GiB; non_torch_memory takes 2.43GiB; PyTorch activation peak memory takes 0.69GiB; the rest of the memory reserved for KV Cache is 16.93GiB.
2025-12-19 23:57:19,918 - vllm.worker.worker - INFO - Memory profiling takes 5.05 seconds
the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB
model weights take 15.39GiB; non_torch_memory takes 2.29GiB; PyTorch activation peak memory takes 0.69GiB; the rest of the memory reserved for KV Cache is 17.07GiB.
2025-12-19 23:57:19,921 - vllm.worker.worker - INFO - Memory profiling takes 5.04 seconds
the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB
model weights take 15.39GiB; non_torch_memory takes 2.43GiB; PyTorch activation peak memory takes 0.69GiB; the rest of the memory reserved for KV Cache is 16.93GiB.
2025-12-19 23:57:20,010 - vllm.worker.worker - INFO - Memory profiling takes 5.10 seconds
the current vLLM instance can use total_gpu_memory (39.38GiB) x gpu_memory_utilization (0.90) = 35.44GiB
model weights take 15.39GiB; non_torch_memory takes 2.39GiB; PyTorch activation peak memory takes 2.84GiB; the rest of the memory reserved for KV Cache is 14.82GiB.
2025-12-19 23:57:20,166 - vllm.executor.executor_base - INFO - # cuda blocks: 15179, # CPU blocks: 16384
2025-12-19 23:57:20,168 - vllm.executor.executor_base - INFO - Maximum concurrency for 8192 tokens per request: 29.65x
2025-12-19 23:57:31,150 - vllm.engine.llm_engine - INFO - init engine (profile, create kv cache, warmup model) took 16.43 seconds
2025-12-19 23:57:32,120 - vllm.config - WARNING - Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
2025-12-19 23:57:32,122 - vllm.entrypoints.openai.serving_chat - INFO - Using default chat sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
2025-12-19 23:57:32,292 - vllm.entrypoints.openai.serving_completion - INFO - Using default completion sampling params from model: {'temperature': 0.6, 'top_k': 20, 'top_p': 0.95}
2025-12-19 23:57:32,294 - vllm.entrypoints.openai.api_server - INFO - Starting vLLM API server on http://0.0.0.0:8000
2025-12-19 23:57:32,295 - vllm.entrypoints.launcher - INFO - Available routes are:
2025-12-19 23:57:32,296 - vllm.entrypoints.launcher - INFO - Route: /openapi.json, Methods: HEAD, GET
2025-12-19 23:57:32,297 - vllm.entrypoints.launcher - INFO - Route: /docs, Methods: HEAD, GET
2025-12-19 23:57:32,298 - vllm.entrypoints.launcher - INFO - Route: /docs/oauth2-redirect, Methods: HEAD, GET
2025-12-19 23:57:32,299 - vllm.entrypoints.launcher - INFO - Route: /redoc, Methods: HEAD, GET
2025-12-19 23:57:32,300 - vllm.entrypoints.launcher - INFO - Route: /health, Methods: GET
2025-12-19 23:57:32,301 - vllm.entrypoints.launcher - INFO - Route: /load, Methods: GET
2025-12-19 23:57:32,302 - vllm.entrypoints.launcher - INFO - Route: /ping, Methods: POST
2025-12-19 23:57:32,303 - vllm.entrypoints.launcher - INFO - Route: /ping, Methods: GET
2025-12-19 23:57:32,304 - vllm.entrypoints.launcher - INFO - Route: /tokenize, Methods: POST
2025-12-19 23:57:32,305 - vllm.entrypoints.launcher - INFO - Route: /detokenize, Methods: POST
2025-12-19 23:57:32,306 - vllm.entrypoints.launcher - INFO - Route: /v1/models, Methods: GET
2025-12-19 23:57:32,307 - vllm.entrypoints.launcher - INFO - Route: /version, Methods: GET
2025-12-19 23:57:32,308 - vllm.entrypoints.launcher - INFO - Route: /v1/chat/completions, Methods: POST
2025-12-19 23:57:32,309 - vllm.entrypoints.launcher - INFO - Route: /v1/completions, Methods: POST
2025-12-19 23:57:32,310 - vllm.entrypoints.launcher - INFO - Route: /v1/embeddings, Methods: POST
2025-12-19 23:57:32,311 - vllm.entrypoints.launcher - INFO - Route: /pooling, Methods: POST
2025-12-19 23:57:32,311 - vllm.entrypoints.launcher - INFO - Route: /classify, Methods: POST
2025-12-19 23:57:32,312 - vllm.entrypoints.launcher - INFO - Route: /score, Methods: POST
2025-12-19 23:57:32,313 - vllm.entrypoints.launcher - INFO - Route: /v1/score, Methods: POST
2025-12-19 23:57:32,314 - vllm.entrypoints.launcher - INFO - Route: /v1/audio/transcriptions, Methods: POST
2025-12-19 23:57:32,315 - vllm.entrypoints.launcher - INFO - Route: /rerank, Methods: POST
2025-12-19 23:57:32,315 - vllm.entrypoints.launcher - INFO - Route: /v1/rerank, Methods: POST
2025-12-19 23:57:32,316 - vllm.entrypoints.launcher - INFO - Route: /v2/rerank, Methods: POST
2025-12-19 23:57:32,317 - vllm.entrypoints.launcher - INFO - Route: /invocations, Methods: POST
2025-12-19 23:57:32,318 - vllm.entrypoints.launcher - INFO - Route: /metrics, Methods: GET
2025-12-19 23:58:24,446 - vllm.entrypoints.logger - INFO - Received request cmpl-5c5481e6237946df832b05bcc26a2ba8-0: prompt: 'Hey, have you ever experienced something weird like dropping something without any reason?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 10321, 2494, 16283, 1075, 25100, 2494, 2041, 894, 2874, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:24,459 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5c5481e6237946df832b05bcc26a2ba8-0.
2025-12-19 23:58:24,460 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:24,695 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:24,878 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:24,919 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:24,961 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,003 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,049 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,090 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,131 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,171 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,212 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,252 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,295 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,334 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,381 - vllm.entrypoints.logger - INFO - Received request cmpl-67adf39ab8274c3bb4c2e7369494c7c8-0: prompt: 'Hey, have you ever experienced something weird like dropping something without any reason?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 10321, 2494, 16283, 1075, 25100, 2494, 2041, 894, 2874, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,383 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-67adf39ab8274c3bb4c2e7369494c7c8-0.
2025-12-19 23:58:25,385 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:25,423 - vllm.entrypoints.logger - INFO - Received request cmpl-283b2a7dc69f4694903c8979881d53e1-0: prompt: 'I had always wanted to be a successful writer, so I decided to write a big book.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 2677, 4829, 311, 387, 264, 6849, 6916, 11, 773, 358, 6635, 311, 3270, 264, 2409, 2311, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,430 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-283b2a7dc69f4694903c8979881d53e1-0.
2025-12-19 23:58:25,431 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:25,475 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,500 - vllm.entrypoints.logger - INFO - Received request cmpl-cd694f565ce2460cb9a60ace46ce5fc9-0: prompt: "I'm really glad we can talk today. I've been feeling pretty overwhelmed lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 15713, 582, 646, 3061, 3351, 13, 358, 3003, 1012, 8266, 5020, 42106, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,522 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cd694f565ce2460cb9a60ace46ce5fc9-0.
2025-12-19 23:58:25,523 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:25,567 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,569 - vllm.entrypoints.logger - INFO - Received request cmpl-c15c105e5a06443eb06fc0ff5a137586-0: prompt: 'Hey, did I tell you that I can walk on my own now?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 429, 358, 646, 4227, 389, 847, 1828, 1431, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,600 - vllm.entrypoints.logger - INFO - Received request cmpl-bf19a7eb5fb44032ab7c78981177d29f-0: prompt: 'Are you feeling okay today, Co-workers B?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11526, 498, 8266, 16910, 3351, 11, 3539, 62284, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,615 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c15c105e5a06443eb06fc0ff5a137586-0.
2025-12-19 23:58:25,617 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bf19a7eb5fb44032ab7c78981177d29f-0.
2025-12-19 23:58:25,618 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:25,618 - vllm.entrypoints.logger - INFO - Received request cmpl-e21d411d7e3241d0836b64fe521c9551-0: prompt: 'Hi Mentor, I just wanted to tell you about my meeting with personY. I went to them for some advice on my upcoming project.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 1101, 4829, 311, 3291, 498, 911, 847, 6438, 448, 1697, 56, 13, 358, 3937, 311, 1105, 369, 1045, 9462, 389, 847, 14487, 2390, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,647 - vllm.entrypoints.logger - INFO - Received request cmpl-8735cf7b74cf42a88e21d491b2b4035f-0: prompt: 'Hey, I met this boy today who was wearing a green shirt and had dark hair.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 2270, 419, 8171, 3351, 879, 572, 12233, 264, 6176, 15478, 323, 1030, 6319, 6869, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,654 - vllm.entrypoints.logger - INFO - Received request cmpl-8e096499957d4200ab252b122eb42702-0: prompt: "I can't believe it, I actually won the tennis tournament! And to top it off, I won $10,000 in prize money.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 432, 11, 358, 3520, 2765, 279, 31415, 16129, 0, 1597, 311, 1909, 432, 1007, 11, 358, 2765, 400, 16, 15, 11, 15, 15, 15, 304, 21882, 3220, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,666 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e21d411d7e3241d0836b64fe521c9551-0.
2025-12-19 23:58:25,667 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8735cf7b74cf42a88e21d491b2b4035f-0.
2025-12-19 23:58:25,669 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8e096499957d4200ab252b122eb42702-0.
2025-12-19 23:58:25,670 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:25,715 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,759 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,802 - vllm.entrypoints.logger - INFO - Received request cmpl-bd608d3f094f49abb88acb587027a19c-0: prompt: 'I just came back from my ballroom dance class, and it was amazing! I love the feeling of being graceful and elegant while dancing.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 3697, 1182, 504, 847, 4935, 2966, 15254, 536, 11, 323, 432, 572, 7897, 0, 358, 2948, 279, 8266, 315, 1660, 77362, 323, 25777, 1393, 27966, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,803 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,846 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bd608d3f094f49abb88acb587027a19c-0.
2025-12-19 23:58:25,848 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:25,889 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,932 - vllm.entrypoints.logger - INFO - Received request cmpl-ecd1d1cf941a4145b3f3be8fa9983745-0: prompt: "You won't believe it, but I think I've finally found a solution to the problem I've been trying to solve for weeks now!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 2765, 944, 4411, 432, 11, 714, 358, 1744, 358, 3003, 5499, 1730, 264, 6291, 311, 279, 3491, 358, 3003, 1012, 4460, 311, 11625, 369, 5555, 1431, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,932 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:25,958 - vllm.entrypoints.logger - INFO - Received request cmpl-73ea839eafae49458d308c55982a7cd2-0: prompt: "I'm sorry I missed the meeting on Tuesday, Coach. I didn't mean to let the team down.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 14589, 358, 13628, 279, 6438, 389, 7589, 11, 27176, 13, 358, 3207, 944, 3076, 311, 1077, 279, 2083, 1495, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:25,976 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ecd1d1cf941a4145b3f3be8fa9983745-0.
2025-12-19 23:58:25,978 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-73ea839eafae49458d308c55982a7cd2-0.
2025-12-19 23:58:25,979 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:26,023 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,040 - vllm.entrypoints.logger - INFO - Received request cmpl-926529e087f042d9a3d6d59fcfbd5366-0: prompt: "Hey there, Neighbors B! How's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 4182, 24101, 425, 0, 2585, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,068 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-926529e087f042d9a3d6d59fcfbd5366-0.
2025-12-19 23:58:26,069 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,089 - vllm.entrypoints.logger - INFO - Received request cmpl-8314a8090ee149e1af553f7d583df89b-0: prompt: 'Hi there, do you need a ride home? ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 653, 498, 1184, 264, 11877, 2114, 30, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,114 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8314a8090ee149e1af553f7d583df89b-0.
2025-12-19 23:58:26,115 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,146 - vllm.entrypoints.logger - INFO - Received request cmpl-78dedd82255f48918dfc6327013ed0b6-0: prompt: "Hey babe, why don't we go outside and get some fresh air?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 40246, 11, 3170, 1513, 944, 582, 728, 4889, 323, 633, 1045, 7722, 3720, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,160 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-78dedd82255f48918dfc6327013ed0b6-0.
2025-12-19 23:58:26,161 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,177 - vllm.entrypoints.logger - INFO - Received request cmpl-4b4f24e2063a4bdeb24642f773e78085-0: prompt: 'Hi Teacher! Sorry for being a bit late, I had to put on my hard hat before coming in.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 29069, 0, 32286, 369, 1660, 264, 2699, 3309, 11, 358, 1030, 311, 2182, 389, 847, 2588, 8896, 1573, 5001, 304, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,206 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4b4f24e2063a4bdeb24642f773e78085-0.
2025-12-19 23:58:26,207 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,230 - vllm.entrypoints.logger - INFO - Received request cmpl-18746bbc8d584e79a9ac79ba88ec66af-0: prompt: 'Your cooking is amazing, Neighbors B. I always feel so satisfied after eating your food.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [7771, 17233, 374, 7897, 11, 4182, 24101, 425, 13, 358, 2677, 2666, 773, 19527, 1283, 12182, 697, 3607, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,251 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-18746bbc8d584e79a9ac79ba88ec66af-0.
2025-12-19 23:58:26,255 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,300 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,346 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,379 - vllm.entrypoints.logger - INFO - Received request cmpl-22cd9b5677b74a29abd0a367fe26387d-0: prompt: 'Hey, have you ever played softball before?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 6342, 95268, 1573, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,387 - vllm.entrypoints.logger - INFO - Received request cmpl-2af6b815850a4d289e85b409920c8fed-0: prompt: "Wow, it's so peaceful here in the garden. I love coming here to explore.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 432, 594, 773, 25650, 1588, 304, 279, 13551, 13, 358, 2948, 5001, 1588, 311, 13186, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,391 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-22cd9b5677b74a29abd0a367fe26387d-0.
2025-12-19 23:58:26,393 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2af6b815850a4d289e85b409920c8fed-0.
2025-12-19 23:58:26,394 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:26,438 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,443 - vllm.entrypoints.logger - INFO - Received request cmpl-ab41aeec9e84410f8b589bcb396f96d2-0: prompt: 'I love our pets so much- they bring so much joy to my life.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 1039, 25103, 773, 1753, 12, 807, 4446, 773, 1753, 15888, 311, 847, 2272, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,482 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ab41aeec9e84410f8b589bcb396f96d2-0.
2025-12-19 23:58:26,486 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,489 - vllm.entrypoints.logger - INFO - Received request cmpl-061aa8c67e2d4c90961135e7a8ca9b89-0: prompt: "Sorry, I'm feeling so sleepy.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [19152, 11, 358, 2776, 8266, 773, 81793, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,524 - vllm.entrypoints.logger - INFO - Received request cmpl-c35ccb985724489bb1ca3b79c2c12338-0: prompt: 'Hey, that was unfair! I wanted the last piece of cake too.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 429, 572, 27643, 0, 358, 4829, 279, 1537, 6573, 315, 19145, 2238, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,532 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-061aa8c67e2d4c90961135e7a8ca9b89-0.
2025-12-19 23:58:26,533 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c35ccb985724489bb1ca3b79c2c12338-0.
2025-12-19 23:58:26,534 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:26,549 - vllm.entrypoints.logger - INFO - Received request cmpl-7b27f3e48d97490e9f492682f65d597e-0: prompt: 'Hi there, what a beautiful day for a picnic!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1128, 264, 6233, 1899, 369, 264, 54462, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,578 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7b27f3e48d97490e9f492682f65d597e-0.
2025-12-19 23:58:26,579 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,625 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,669 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,702 - vllm.entrypoints.logger - INFO - Received request cmpl-e9dda815d8684214801c710e18967f17-0: prompt: "Teacher, I wanted to talk to you about something that's been bothering me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [45065, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 90159, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,712 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e9dda815d8684214801c710e18967f17-0.
2025-12-19 23:58:26,714 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,755 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,797 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,840 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,880 - vllm.entrypoints.logger - INFO - Received request cmpl-6b30b87c872e4a128da324db7ea73cef-0: prompt: "I just sold my company and I'm feeling lost. I don't know what to do with all this free time.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 6088, 847, 2813, 323, 358, 2776, 8266, 5558, 13, 358, 1513, 944, 1414, 1128, 311, 653, 448, 678, 419, 1910, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:26,883 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6b30b87c872e4a128da324db7ea73cef-0.
2025-12-19 23:58:26,884 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:26,927 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:26,971 - vllm.engine.metrics - INFO - Avg prompt throughput: 88.3 tokens/s, Avg generation throughput: 50.0 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
2025-12-19 23:58:26,973 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,016 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,048 - vllm.entrypoints.logger - INFO - Received request cmpl-b6c5516d18634a1ebcbd63158eb7384f-0: prompt: 'Hey Mentor! I had so much fun going to the movies with you last week.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 0, 358, 1030, 773, 1753, 2464, 2087, 311, 279, 9508, 448, 498, 1537, 2003, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:27,061 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b6c5516d18634a1ebcbd63158eb7384f-0.
2025-12-19 23:58:27,062 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:27,104 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,147 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,190 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,223 - vllm.entrypoints.logger - INFO - Received request cmpl-330a7a57128d4be68753f00bc24989c9-0: prompt: ', I went to the next town over yesterday and saw a store that I wanted to check out. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 3937, 311, 279, 1790, 6290, 916, 13671, 323, 5485, 264, 3553, 429, 358, 4829, 311, 1779, 700, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:27,235 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-330a7a57128d4be68753f00bc24989c9-0.
2025-12-19 23:58:27,236 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:27,280 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,325 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,369 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,411 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,453 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,497 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,523 - vllm.entrypoints.logger - INFO - Received request cmpl-cb369a083f1e4941a70468d523fa3156-0: prompt: "Hey, have you heard the news? I joined the Army and just finished basic training. I'm now stationed at Fort Benning.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 279, 3669, 30, 358, 10859, 279, 13011, 323, 1101, 8060, 6770, 4862, 13, 358, 2776, 1431, 62520, 518, 11002, 7355, 1229, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:27,544 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cb369a083f1e4941a70468d523fa3156-0.
2025-12-19 23:58:27,546 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:27,588 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,631 - vllm.entrypoints.logger - INFO - Received request cmpl-ba5f6ac6689e423db755752303d045cf-0: prompt: "Hey, I want to talk to you. I know I messed up, and I'm really sorry.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1366, 311, 3061, 311, 498, 13, 358, 1414, 358, 64202, 705, 11, 323, 358, 2776, 2167, 14589, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:27,634 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ba5f6ac6689e423db755752303d045cf-0.
2025-12-19 23:58:27,636 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:27,680 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,722 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,736 - vllm.entrypoints.logger - INFO - Received request cmpl-3ebe3f5033bc492abe07faabdb6a9b08-0: prompt: "Hey, Neighbors B. I heard you're really good at drawing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 13, 358, 6617, 498, 2299, 2167, 1661, 518, 13330, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:27,755 - vllm.entrypoints.logger - INFO - Received request cmpl-d5935c1e32dd4a1c9f4491399e8685c5-0: prompt: " I've been on my feet for six hours straight. I can't wait to go home and rest.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 3003, 1012, 389, 847, 7541, 369, 4743, 4115, 7678, 13, 358, 646, 944, 3783, 311, 728, 2114, 323, 2732, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:27,767 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3ebe3f5033bc492abe07faabdb6a9b08-0.
2025-12-19 23:58:27,772 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d5935c1e32dd4a1c9f4491399e8685c5-0.
2025-12-19 23:58:27,773 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:27,815 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,858 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,900 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,942 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:27,984 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,028 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,052 - vllm.entrypoints.logger - INFO - Received request cmpl-e04ad132e34c4f15a97cbdcfb989a28c-0: prompt: 'Hi, Neighbors B. How are you feeling today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 4182, 24101, 425, 13, 2585, 525, 498, 8266, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:28,057 - vllm.entrypoints.logger - INFO - Received request cmpl-f545b5fbd9bb4d47a3737ec442da2dde-0: prompt: "I've been practicing deep breathing techniques lately. It's amazing how much it helps me relax and clear my mind.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 35566, 5538, 25938, 12538, 30345, 13, 1084, 594, 7897, 1246, 1753, 432, 8609, 752, 11967, 323, 2797, 847, 3971, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:28,072 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e04ad132e34c4f15a97cbdcfb989a28c-0.
2025-12-19 23:58:28,074 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f545b5fbd9bb4d47a3737ec442da2dde-0.
2025-12-19 23:58:28,075 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:28,082 - vllm.entrypoints.logger - INFO - Received request cmpl-3006c4d6cc4f4b5b91fc6ebfa8dd229b-0: prompt: "Teacher, I've been feeling so much better lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [45065, 11, 358, 3003, 1012, 8266, 773, 1753, 2664, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:28,119 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3006c4d6cc4f4b5b91fc6ebfa8dd229b-0.
2025-12-19 23:58:28,121 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:28,161 - vllm.entrypoints.logger - INFO - Received request cmpl-3bc8c2a4a86840a4996d817cb61abb8c-0: prompt: "*sighs* I can't believe I messed up the electrical circuit. I thought I knew what I was doing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [33454, 1090, 82, 9, 358, 646, 944, 4411, 358, 64202, 705, 279, 19734, 16224, 13, 358, 3381, 358, 6876, 1128, 358, 572, 3730, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:28,164 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3bc8c2a4a86840a4996d817cb61abb8c-0.
2025-12-19 23:58:28,165 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:28,208 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,250 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,297 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,308 - vllm.entrypoints.logger - INFO - Received request cmpl-dccd1cfcc71f411d90fef0b1ef4c0066-0: prompt: 'I went to a cheap restaurant by myself for dinner last night. I wanted some alone time.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3937, 311, 264, 11872, 10729, 553, 7037, 369, 13856, 1537, 3729, 13, 358, 4829, 1045, 7484, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:28,339 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dccd1cfcc71f411d90fef0b1ef4c0066-0.
2025-12-19 23:58:28,341 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:28,382 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,424 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,465 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,498 - vllm.entrypoints.logger - INFO - Received request cmpl-d8b0a07dd3464371bef7d34f126366ef-0: prompt: 'Mentor, I wanted to talk to you about something that happened yesterday.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 6932, 13671, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:28,508 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d8b0a07dd3464371bef7d34f126366ef-0.
2025-12-19 23:58:28,512 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:28,553 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,583 - vllm.entrypoints.logger - INFO - Received request cmpl-fec73bff30224acca07fe3dcf5185cfc-0: prompt: "I was really worried about that math test, but I'm so glad I studied hard for it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 2167, 17811, 911, 429, 6888, 1273, 11, 714, 358, 2776, 773, 15713, 358, 19476, 2588, 369, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:28,597 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fec73bff30224acca07fe3dcf5185cfc-0.
2025-12-19 23:58:28,598 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:28,639 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,680 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,722 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,729 - vllm.entrypoints.logger - INFO - Received request cmpl-74760160275345a6a6759993e5d154b0-0: prompt: ' Hello, Boss.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [21927, 11, 31569, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:28,766 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-74760160275345a6a6759993e5d154b0-0.
2025-12-19 23:58:28,768 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:28,809 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,851 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,892 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,934 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:28,976 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:29,020 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:29,063 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:29,105 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:29,148 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:29,190 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:29,231 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:29,270 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,071 - vllm.entrypoints.logger - INFO - Received request cmpl-18a1620ac37648ff92058aa2f3aeae12-0: prompt: "I think I've set up the chairs perfectly for the meeting today. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1744, 358, 3003, 738, 705, 279, 25904, 13942, 369, 279, 6438, 3351, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:30,076 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-18a1620ac37648ff92058aa2f3aeae12-0.
2025-12-19 23:58:30,077 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:30,121 - vllm.entrypoints.logger - INFO - Received request cmpl-903397d2fd784da79a9bca764c801b70-0: prompt: "I'm excited for tonight's dinner. Are you hungry?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 12035, 369, 17913, 594, 13856, 13, 8713, 498, 28956, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:30,124 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-903397d2fd784da79a9bca764c801b70-0.
2025-12-19 23:58:30,125 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:30,156 - vllm.entrypoints.logger - INFO - Received request cmpl-9fa5fa9dd007405c8af91ae0c2ceb85b-0: prompt: "I'm so glad we're friends, B. That party a couple of years ago was really tough for me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 15713, 582, 2299, 4780, 11, 425, 13, 2938, 4614, 264, 5625, 315, 1635, 4134, 572, 2167, 11045, 369, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:30,169 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9fa5fa9dd007405c8af91ae0c2ceb85b-0.
2025-12-19 23:58:30,170 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:30,215 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,260 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,278 - vllm.entrypoints.logger - INFO - Received request cmpl-6c692f41b756496690192df83b40f787-0: prompt: "Hey, do you remember the time when I smacked your face? I'm really sorry about that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 6099, 279, 882, 979, 358, 1525, 11191, 697, 3579, 30, 358, 2776, 2167, 14589, 911, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:30,307 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6c692f41b756496690192df83b40f787-0.
2025-12-19 23:58:30,308 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:30,352 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,354 - vllm.entrypoints.logger - INFO - Received request cmpl-f6e3dc6e3e45431a9c3f38b26075d3b9-0: prompt: "I can't wait for our next trip into the city! There's always so much to see and do.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 369, 1039, 1790, 8411, 1119, 279, 3283, 0, 2619, 594, 2677, 773, 1753, 311, 1490, 323, 653, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:30,395 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f6e3dc6e3e45431a9c3f38b26075d3b9-0.
2025-12-19 23:58:30,396 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:30,441 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,483 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,525 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,566 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,607 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,649 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,690 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,731 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,772 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,813 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,854 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,895 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,936 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:30,977 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,018 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,059 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,098 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,140 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,180 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,219 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,258 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,406 - vllm.entrypoints.logger - INFO - Received request cmpl-293203fce68e432bb48cd99a65500979-0: prompt: "So, your parents are going to be away for a few days, and I'll be looking after you. I hope you're all set for the days ahead?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 697, 6562, 525, 2087, 311, 387, 3123, 369, 264, 2421, 2849, 11, 323, 358, 3278, 387, 3330, 1283, 498, 13, 358, 3900, 498, 2299, 678, 738, 369, 279, 2849, 8305, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:31,408 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-293203fce68e432bb48cd99a65500979-0.
2025-12-19 23:58:31,409 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:31,453 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,493 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,533 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,573 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,612 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,650 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,652 - vllm.entrypoints.logger - INFO - Received request cmpl-ffb9e6cf656947c5b3ff2041d51bd0fd-0: prompt: 'I lost my watch and I feel upset. I need it to tell time.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 5558, 847, 3736, 323, 358, 2666, 22459, 13, 358, 1184, 432, 311, 3291, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:31,691 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ffb9e6cf656947c5b3ff2041d51bd0fd-0.
2025-12-19 23:58:31,693 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:31,735 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,776 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,819 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,862 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,873 - vllm.entrypoints.logger - INFO - Received request cmpl-361b045cbbc843c8bb8c4cddabc61357-0: prompt: "Hey, have you heard the good news? I finally enrolled in a degree program and I'm starting classes soon!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 279, 1661, 3669, 30, 358, 5499, 36091, 304, 264, 8381, 2025, 323, 358, 2776, 5916, 6846, 5135, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:31,906 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-361b045cbbc843c8bb8c4cddabc61357-0.
2025-12-19 23:58:31,907 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:31,948 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:31,990 - vllm.engine.metrics - INFO - Avg prompt throughput: 82.9 tokens/s, Avg generation throughput: 86.5 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
2025-12-19 23:58:31,992 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,034 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,075 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,117 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,118 - vllm.entrypoints.logger - INFO - Received request cmpl-1e9e6df81898446e8ec635b44d829b5b-0: prompt: 'Hey, Neighbors B, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:32,161 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1e9e6df81898446e8ec635b44d829b5b-0.
2025-12-19 23:58:32,162 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:32,203 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,244 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,285 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,326 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,366 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,407 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,449 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,490 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,515 - vllm.entrypoints.logger - INFO - Received request cmpl-33df640cf02b4c38b2af5315c761e654-0: prompt: "This burger is really good, don't you think?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 44623, 374, 2167, 1661, 11, 1513, 944, 498, 1744, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:32,533 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-33df640cf02b4c38b2af5315c761e654-0.
2025-12-19 23:58:32,534 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:32,575 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,616 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,635 - vllm.entrypoints.logger - INFO - Received request cmpl-9e12b21f645a4633af4cdcc195e43e2f-0: prompt: 'I always try to finish my work as efficiently as possible. That way, I can stay on top of everything and make sure nothing falls through the cracks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=31, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 1430, 311, 6248, 847, 975, 438, 29720, 438, 3204, 13, 2938, 1616, 11, 358, 646, 4717, 389, 1909, 315, 4297, 323, 1281, 2704, 4302, 17066, 1526, 279, 44863, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:32,659 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9e12b21f645a4633af4cdcc195e43e2f-0.
2025-12-19 23:58:32,660 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:32,701 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,742 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,783 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,823 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,864 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,905 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,945 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:32,986 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,028 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,070 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,096 - vllm.entrypoints.logger - INFO - Received request cmpl-a377a9a77f264e8baf6df5001840b503-0: prompt: ", I wanted to talk to you about something that's been weighing on me. When I was diagnosed with cancer, I felt like my life was over.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 46726, 389, 752, 13, 3197, 358, 572, 28604, 448, 9387, 11, 358, 6476, 1075, 847, 2272, 572, 916, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:33,111 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a377a9a77f264e8baf6df5001840b503-0.
2025-12-19 23:58:33,113 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:33,153 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,196 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,240 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,282 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,324 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,365 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,408 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,450 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,491 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,532 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,572 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,615 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,656 - vllm.entrypoints.logger - INFO - Received request cmpl-0bea0edc829642f99515e4ba70dced08-0: prompt: 'I noticed that our new next-door neighbors have two young children. They seem really friendly.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 13686, 429, 1039, 501, 1790, 57090, 18709, 614, 1378, 3908, 2841, 13, 2379, 2803, 2167, 11657, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:33,658 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0bea0edc829642f99515e4ba70dced08-0.
2025-12-19 23:58:33,660 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:33,702 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,729 - vllm.entrypoints.logger - INFO - Received request cmpl-26e9009a4f584a1b859ca6ef0a8b4c0c-0: prompt: 'Did you see my latest post on the quantum mechanics newsgroup?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 1490, 847, 5535, 1736, 389, 279, 30128, 29026, 501, 1991, 886, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:33,745 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-26e9009a4f584a1b859ca6ef0a8b4c0c-0.
2025-12-19 23:58:33,746 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:33,789 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,806 - vllm.entrypoints.logger - INFO - Received request cmpl-10830a7168394b8d824333ce38055463-0: prompt: 'Ahhh, that nap was exactly what I needed. I feel like I can take on anything now.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [32, 90064, 11, 429, 25859, 572, 6896, 1128, 358, 4362, 13, 358, 2666, 1075, 358, 646, 1896, 389, 4113, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:33,832 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-10830a7168394b8d824333ce38055463-0.
2025-12-19 23:58:33,834 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:33,876 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:33,890 - vllm.entrypoints.logger - INFO - Received request cmpl-ab0e1a5278d543959f99be1d64640676-0: prompt: 'Can you believe we got on the wrong train today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 582, 2684, 389, 279, 4969, 5426, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:33,893 - vllm.entrypoints.logger - INFO - Received request cmpl-ff27421486e8493ca552445f8936ce26-0: prompt: 'Hey kiddo, did you hear about the tournament happening in town next month?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91415, 78, 11, 1521, 498, 6723, 911, 279, 16129, 12482, 304, 6290, 1790, 2254, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:33,921 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ab0e1a5278d543959f99be1d64640676-0.
2025-12-19 23:58:33,922 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ff27421486e8493ca552445f8936ce26-0.
2025-12-19 23:58:33,924 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:33,943 - vllm.entrypoints.logger - INFO - Received request cmpl-c85791ff00044353a58a3e0f07969f9a-0: prompt: 'Honey, I just want you to know that I take my responsibility of taking care of you very seriously. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [39, 2534, 11, 358, 1101, 1366, 498, 311, 1414, 429, 358, 1896, 847, 11752, 315, 4633, 2453, 315, 498, 1602, 13919, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:33,967 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c85791ff00044353a58a3e0f07969f9a-0.
2025-12-19 23:58:33,968 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:34,010 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,053 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,095 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,137 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,178 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,222 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,265 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,306 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,347 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,382 - vllm.entrypoints.logger - INFO - Received request cmpl-60cd6d60a99f4df2844322fed18e34dc-0: prompt: "Hi Doctor, I wanted to talk to you about my upcoming math test. I'm feeling really nervous and unprepared.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 18635, 11, 358, 4829, 311, 3061, 311, 498, 911, 847, 14487, 6888, 1273, 13, 358, 2776, 8266, 2167, 22596, 323, 650, 60112, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:34,389 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-60cd6d60a99f4df2844322fed18e34dc-0.
2025-12-19 23:58:34,391 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:34,433 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,476 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,486 - vllm.entrypoints.logger - INFO - Received request cmpl-419b958d6ce04c97916996393f3422eb-0: prompt: 'Hey, did you get a chance to clean the gutters yet?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 633, 264, 6012, 311, 4240, 279, 17859, 5045, 3602, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:34,518 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-419b958d6ce04c97916996393f3422eb-0.
2025-12-19 23:58:34,520 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:34,561 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,603 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,645 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,686 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,701 - vllm.entrypoints.logger - INFO - Received request cmpl-35e51e8b555748de83e3975ffbc19269-0: prompt: "Hey, how are you feeling? I heard you've been having nightmares about your surgery.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 525, 498, 8266, 30, 358, 6617, 498, 3003, 1012, 3432, 74262, 911, 697, 14829, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:34,727 - vllm.entrypoints.logger - INFO - Received request cmpl-5fc4ba15a8e047879e95d7f9c598e0a7-0: prompt: 'Hi, sweetie, how was your day at school?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 10226, 645, 11, 1246, 572, 697, 1899, 518, 2906, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:34,730 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-35e51e8b555748de83e3975ffbc19269-0.
2025-12-19 23:58:34,731 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5fc4ba15a8e047879e95d7f9c598e0a7-0.
2025-12-19 23:58:34,732 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:34,774 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,815 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,858 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,899 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,941 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:34,983 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,025 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,067 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,108 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,145 - vllm.entrypoints.logger - INFO - Received request cmpl-2b5dc97d08d34b4bbba527693c558c0f-0: prompt: "Hi, Teacher. I'm planning for my birthday party.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 29069, 13, 358, 2776, 9115, 369, 847, 15198, 4614, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:35,150 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2b5dc97d08d34b4bbba527693c558c0f-0.
2025-12-19 23:58:35,151 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:35,192 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,235 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,277 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,319 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,360 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,402 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,443 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,485 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,527 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,568 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,606 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:35,962 - vllm.entrypoints.logger - INFO - Received request cmpl-d417e2c4413e47af8a0c6c87b918ebb4-0: prompt: " I'm really nervous about taking this medication for my anxiety.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=35, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 2167, 22596, 911, 4633, 419, 23221, 369, 847, 18056, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:35,964 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d417e2c4413e47af8a0c6c87b918ebb4-0.
2025-12-19 23:58:35,965 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:36,010 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,051 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,090 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,131 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,172 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,179 - vllm.entrypoints.logger - INFO - Received request cmpl-493df0ed132e4621a42fe097c2078d33-0: prompt: "You know, I've been thinking a lot about what we talked about last time.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 3003, 1012, 7274, 264, 2696, 911, 1128, 582, 14897, 911, 1537, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,208 - vllm.entrypoints.logger - INFO - Received request cmpl-2d8da98b6bfd402ca1f998e1f775dbff-0: prompt: 'Hey, B, I have some good news. I got a job as a cashier at the grocery store.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 11, 358, 614, 1045, 1661, 3669, 13, 358, 2684, 264, 2618, 438, 264, 90355, 518, 279, 29587, 3553, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,217 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-493df0ed132e4621a42fe097c2078d33-0.
2025-12-19 23:58:36,218 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2d8da98b6bfd402ca1f998e1f775dbff-0.
2025-12-19 23:58:36,220 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:36,262 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,303 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,345 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,348 - vllm.entrypoints.logger - INFO - Received request cmpl-f8ad3e2ddf51449eb6579014eb6173a5-0: prompt: 'Hey, have you noticed my new scarf?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 13686, 847, 501, 67271, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,371 - vllm.entrypoints.logger - INFO - Received request cmpl-a0c0c6e217724ab2b9e340d775062639-0: prompt: "Mentor, you won't believe what happened to me the other day! I found a trunk full of treasure in my grandparents' attic.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 498, 2765, 944, 4411, 1128, 6932, 311, 752, 279, 1008, 1899, 0, 358, 1730, 264, 37311, 2480, 315, 31626, 304, 847, 55335, 6, 73621, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,374 - vllm.entrypoints.logger - INFO - Received request cmpl-a57fc1fc864a4573ba63dc77d48ba852-0: prompt: "Ah, I can't believe I ripped another bag! I swear, I'm so careless sometimes.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 358, 646, 944, 4411, 358, 42992, 2441, 8968, 0, 358, 46077, 11, 358, 2776, 773, 83734, 7025, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,390 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f8ad3e2ddf51449eb6579014eb6173a5-0.
2025-12-19 23:58:36,391 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a0c0c6e217724ab2b9e340d775062639-0.
2025-12-19 23:58:36,393 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a57fc1fc864a4573ba63dc77d48ba852-0.
2025-12-19 23:58:36,394 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:36,409 - vllm.entrypoints.logger - INFO - Received request cmpl-fd8ccaa8523b4e06afb14160934761e2-0: prompt: 'I had so much fun at the party last night! Thanks for taking me, Neighbors B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 773, 1753, 2464, 518, 279, 4614, 1537, 3729, 0, 11114, 369, 4633, 752, 11, 4182, 24101, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,438 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fd8ccaa8523b4e06afb14160934761e2-0.
2025-12-19 23:58:36,440 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:36,443 - vllm.entrypoints.logger - INFO - Received request cmpl-f67f928ae10e4a329bf238b0f8806418-0: prompt: "Mentor, you've always been there for me. I'm just returning the favor by being loyal to you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 498, 3003, 2677, 1012, 1052, 369, 752, 13, 358, 2776, 1101, 13451, 279, 4694, 553, 1660, 28847, 311, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,483 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f67f928ae10e4a329bf238b0f8806418-0.
2025-12-19 23:58:36,485 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:36,516 - vllm.entrypoints.logger - INFO - Received request cmpl-8cbd80f243954b21b4056dd760ac97b4-0: prompt: "Wow, that's beautiful. You wrote that yourself?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 429, 594, 6233, 13, 1446, 6139, 429, 6133, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,527 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8cbd80f243954b21b4056dd760ac97b4-0.
2025-12-19 23:58:36,529 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:36,538 - vllm.entrypoints.logger - INFO - Received request cmpl-f38043ce3ba74cc09c0c006d3df6ec77-0: prompt: "You know, I'm really happy that we're such good friends, B. We have so much in common and always have a good time together.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2776, 2167, 6247, 429, 582, 2299, 1741, 1661, 4780, 11, 425, 13, 1205, 614, 773, 1753, 304, 4185, 323, 2677, 614, 264, 1661, 882, 3786, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,540 - vllm.entrypoints.logger - INFO - Received request cmpl-7df9c78659dd4d3b9ee91122dcfc5927-0: prompt: 'I was so angry earlier that I almost attacked someone.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 773, 18514, 6788, 429, 358, 4558, 18349, 4325, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,573 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f38043ce3ba74cc09c0c006d3df6ec77-0.
2025-12-19 23:58:36,575 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7df9c78659dd4d3b9ee91122dcfc5927-0.
2025-12-19 23:58:36,576 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:36,591 - vllm.entrypoints.logger - INFO - Received request cmpl-936a58465f9e4f1287fdc03ed41f1bf2-0: prompt: "Hey, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,605 - vllm.entrypoints.logger - INFO - Received request cmpl-4bd8a6ebad28450d820bf014ba3e34b7-0: prompt: 'Wow, that was such a refreshing bike ride through campus today. I forgot how much I love being outside.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 429, 572, 1741, 264, 35918, 12963, 11877, 1526, 15319, 3351, 13, 358, 28595, 1246, 1753, 358, 2948, 1660, 4889, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,621 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-936a58465f9e4f1287fdc03ed41f1bf2-0.
2025-12-19 23:58:36,623 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4bd8a6ebad28450d820bf014ba3e34b7-0.
2025-12-19 23:58:36,624 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:36,667 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,709 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,733 - vllm.entrypoints.logger - INFO - Received request cmpl-e73016c9cfb64c08b6d6e57f66531413-0: prompt: "I was just thinking about the teapot I made. It's amazing how something so simple can make such a big difference in making tea.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 1101, 7274, 911, 279, 1013, 89901, 358, 1865, 13, 1084, 594, 7897, 1246, 2494, 773, 4285, 646, 1281, 1741, 264, 2409, 6672, 304, 3259, 15243, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,752 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e73016c9cfb64c08b6d6e57f66531413-0.
2025-12-19 23:58:36,754 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:36,781 - vllm.entrypoints.logger - INFO - Received request cmpl-9637360edce944f0a677f7d8f26b7f3b-0: prompt: 'You know, I really want to be friends with the kids in our neighborhood.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2167, 1366, 311, 387, 4780, 448, 279, 6837, 304, 1039, 12534, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,796 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9637360edce944f0a677f7d8f26b7f3b-0.
2025-12-19 23:58:36,797 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:36,839 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,882 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:36,886 - vllm.entrypoints.logger - INFO - Received request cmpl-4f0e883e4f0a4c58a4f840c0be0babed-0: prompt: 'Hey, have you heard? I opened a bank account yesterday and I deposited $50.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 30, 358, 8930, 264, 6073, 2692, 13671, 323, 358, 53468, 400, 20, 15, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:36,926 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4f0e883e4f0a4c58a4f840c0be0babed-0.
2025-12-19 23:58:36,928 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:36,969 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,013 - vllm.engine.metrics - INFO - Avg prompt throughput: 112.5 tokens/s, Avg generation throughput: 81.8 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
2025-12-19 23:58:37,015 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,059 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,102 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,144 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,186 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,212 - vllm.entrypoints.logger - INFO - Received request cmpl-c65bcab515be4320a93f07feee0150e9-0: prompt: "I'm feeling really motivated lately. I want to work harder than ever before and make a difference in the world through my work.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 2167, 26664, 30345, 13, 358, 1366, 311, 975, 15750, 1091, 3512, 1573, 323, 1281, 264, 6672, 304, 279, 1879, 1526, 847, 975, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:37,230 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c65bcab515be4320a93f07feee0150e9-0.
2025-12-19 23:58:37,232 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:37,274 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,318 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,322 - vllm.entrypoints.logger - INFO - Received request cmpl-d9d899d6accf49efb8d0ef3242e295ed-0: prompt: "I can't believe I finally have a new phone! It's so cool and shiny. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 5499, 614, 264, 501, 4540, 0, 1084, 594, 773, 7010, 323, 41199, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:37,345 - vllm.entrypoints.logger - INFO - Received request cmpl-2009a84f02ac47b8b116a1d51936ee04-0: prompt: 'Hey, did you hear about what I did with your dog?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 1128, 358, 1521, 448, 697, 5562, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:37,364 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d9d899d6accf49efb8d0ef3242e295ed-0.
2025-12-19 23:58:37,366 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2009a84f02ac47b8b116a1d51936ee04-0.
2025-12-19 23:58:37,367 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:37,410 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,452 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,454 - vllm.entrypoints.logger - INFO - Received request cmpl-4995fe58067b496a8284013547208827-0: prompt: "Hey neighbor, have you heard about the new event I'm planning?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 614, 498, 6617, 911, 279, 501, 1538, 358, 2776, 9115, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:37,496 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4995fe58067b496a8284013547208827-0.
2025-12-19 23:58:37,498 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:37,541 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,552 - vllm.entrypoints.logger - INFO - Received request cmpl-1c8622b33be743cebd8a300e089dfae8-0: prompt: "Mentor, I've been reading a lot of books lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 3003, 1012, 5290, 264, 2696, 315, 6467, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:37,585 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1c8622b33be743cebd8a300e089dfae8-0.
2025-12-19 23:58:37,586 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:37,627 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,634 - vllm.entrypoints.logger - INFO - Received request cmpl-7d38a82ac1f746588d1536c35b428555-0: prompt: ' Hi there! I was hoping we could talk about your business.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [21018, 1052, 0, 358, 572, 15652, 582, 1410, 3061, 911, 697, 2562, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:37,670 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7d38a82ac1f746588d1536c35b428555-0.
2025-12-19 23:58:37,671 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:37,672 - vllm.entrypoints.logger - INFO - Received request cmpl-7408e62fe13846bd93a9f9f0aaaf1a23-0: prompt: 'Thanks for the coffee, Neighbors B. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12658, 369, 279, 10799, 11, 4182, 24101, 425, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:37,714 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7408e62fe13846bd93a9f9f0aaaf1a23-0.
2025-12-19 23:58:37,716 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:37,756 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,798 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,840 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,882 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,924 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:37,968 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,011 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,055 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,098 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,141 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,184 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,226 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,268 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,309 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,351 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,393 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,404 - vllm.entrypoints.logger - INFO - Received request cmpl-02e23070ac5d4d5a8e2c7918e78c0ba3-0: prompt: 'Have you ever tried any exotic or new foods, B?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 3512, 6679, 894, 38318, 476, 501, 15298, 11, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:38,436 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-02e23070ac5d4d5a8e2c7918e78c0ba3-0.
2025-12-19 23:58:38,438 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:38,484 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,527 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,569 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,611 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,653 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,693 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,733 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,772 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,813 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,853 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,892 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,930 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:38,968 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:39,009 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:39,050 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:39,088 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:39,127 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,541 - vllm.entrypoints.logger - INFO - Received request cmpl-8d101110c2234dedab0a7b9c331ec799-0: prompt: 'Hey, do you remember when I invited you to that beach party a few months ago?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 6099, 979, 358, 18218, 498, 311, 429, 11321, 4614, 264, 2421, 3951, 4134, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:40,545 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8d101110c2234dedab0a7b9c331ec799-0.
2025-12-19 23:58:40,546 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:40,590 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,630 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,670 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,711 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,757 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,798 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,836 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,877 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,915 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,954 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:40,994 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:41,036 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:41,075 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:41,113 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:41,152 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:41,190 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:41,229 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,109 - vllm.entrypoints.logger - INFO - Received request cmpl-71d8485a8b2a4b7d94094f89a2814648-0: prompt: ' Thank you for meeting with me today, Doctor. I really needed someone to talk to.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9731, 498, 369, 6438, 448, 752, 3351, 11, 18635, 13, 358, 2167, 4362, 4325, 311, 3061, 311, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:45,114 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-71d8485a8b2a4b7d94094f89a2814648-0.
2025-12-19 23:58:45,115 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:45,159 - vllm.engine.metrics - INFO - Avg prompt throughput: 19.3 tokens/s, Avg generation throughput: 34.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
2025-12-19 23:58:45,162 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,196 - vllm.entrypoints.logger - INFO - Received request cmpl-38ab0b1649a6458abbb20bf679db5cb7-0: prompt: 'Hey, good morning!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1661, 6556, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:45,202 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-38ab0b1649a6458abbb20bf679db5cb7-0.
2025-12-19 23:58:45,203 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:45,245 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,287 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,329 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,370 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,412 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,453 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,497 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,539 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,581 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,603 - vllm.entrypoints.logger - INFO - Received request cmpl-b859859475374fab9810dcacd0baf72c-0: prompt: 'I hope you liked my house. I live here alone.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3900, 498, 14915, 847, 3753, 13, 358, 3887, 1588, 7484, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:45,623 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b859859475374fab9810dcacd0baf72c-0.
2025-12-19 23:58:45,624 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:45,665 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,706 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,707 - vllm.entrypoints.logger - INFO - Received request cmpl-9868a2de10d942acba305ac0bdb62f15-0: prompt: 'Hey, have you heard the news? I just got a patent for my water-purifying device!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 279, 3669, 30, 358, 1101, 2684, 264, 24571, 369, 847, 3015, 2268, 324, 7766, 3671, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:45,748 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9868a2de10d942acba305ac0bdb62f15-0.
2025-12-19 23:58:45,750 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:45,790 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,831 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,872 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,939 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:45,980 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,023 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,064 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,105 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,146 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,189 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,230 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,271 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,310 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,350 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,388 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,419 - vllm.entrypoints.logger - INFO - Received request cmpl-8ccfc30077fd4bc882979ff9235ce55a-0: prompt: "Coach, I'm so excited about the upcoming marathon. I've been training really hard for this and I'm determined to beat my personal best time.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 2776, 773, 12035, 911, 279, 14487, 44696, 13, 358, 3003, 1012, 4862, 2167, 2588, 369, 419, 323, 358, 2776, 10838, 311, 9382, 847, 4345, 1850, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:46,430 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8ccfc30077fd4bc882979ff9235ce55a-0.
2025-12-19 23:58:46,431 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:46,476 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,518 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,559 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,601 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,641 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,680 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,721 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,760 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,798 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,836 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,875 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,915 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,953 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:46,993 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:47,034 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:47,072 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:47,110 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:47,148 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:47,188 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:47,228 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:47,266 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,325 - vllm.entrypoints.logger - INFO - Received request cmpl-6f88d8aa48ae4e20ad868ec1303ddf0b-0: prompt: 'Alright everyone, thanks for coming. Today we need to discuss the preparations for the school fair.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [71486, 5019, 11, 9339, 369, 5001, 13, 11201, 582, 1184, 311, 4263, 279, 46879, 369, 279, 2906, 6624, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:53,330 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f88d8aa48ae4e20ad868ec1303ddf0b-0.
2025-12-19 23:58:53,331 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:53,374 - vllm.engine.metrics - INFO - Avg prompt throughput: 10.6 tokens/s, Avg generation throughput: 10.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
2025-12-19 23:58:53,375 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,414 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,421 - vllm.entrypoints.logger - INFO - Received request cmpl-08ecfded0c174aa381c7c22a9cafcfd4-0: prompt: "I have to tell you, I'm really bothered by the way you've been speaking to me lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 311, 3291, 498, 11, 358, 2776, 2167, 45276, 553, 279, 1616, 498, 3003, 1012, 12094, 311, 752, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:53,455 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-08ecfded0c174aa381c7c22a9cafcfd4-0.
2025-12-19 23:58:53,457 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:53,500 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,543 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,549 - vllm.entrypoints.logger - INFO - Received request cmpl-5c508167bd014793a4eb2843378a5db7-0: prompt: ' I really wish you would listen to me more.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2167, 6426, 498, 1035, 8844, 311, 752, 803, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:53,585 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5c508167bd014793a4eb2843378a5db7-0.
2025-12-19 23:58:53,587 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:53,627 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,668 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,700 - vllm.entrypoints.logger - INFO - Received request cmpl-85c49ce7525e4ef9ba2252bb0f303bc7-0: prompt: "I'm so excited for the dance competition this weekend! I've been practicing so hard.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 369, 279, 15254, 10707, 419, 9001, 0, 358, 3003, 1012, 35566, 773, 2588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:53,713 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-85c49ce7525e4ef9ba2252bb0f303bc7-0.
2025-12-19 23:58:53,715 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:53,758 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,799 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,839 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,881 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,922 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:53,963 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,005 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,047 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,087 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,134 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,174 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,213 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,252 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,291 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,308 - vllm.entrypoints.logger - INFO - Received request cmpl-6ad3a375066c4758bce2224e460d63ef-0: prompt: 'Hey, are you almost done packing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 525, 498, 4558, 2814, 35713, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:54,331 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6ad3a375066c4758bce2224e460d63ef-0.
2025-12-19 23:58:54,335 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:54,365 - vllm.entrypoints.logger - INFO - Received request cmpl-7f1cfda34d364e558009b4a624ac4872-0: prompt: 'Coach, I have some good news! I broke my previous records in both high jump and long jump.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 614, 1045, 1661, 3669, 0, 358, 14422, 847, 3681, 7424, 304, 2176, 1550, 7784, 323, 1293, 7784, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:54,376 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7f1cfda34d364e558009b4a624ac4872-0.
2025-12-19 23:58:54,378 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:54,418 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,460 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,501 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,542 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,583 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,623 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,664 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,704 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,749 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,790 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,821 - vllm.entrypoints.logger - INFO - Received request cmpl-277ec7a3c2b14966af8fa411d19a0885-0: prompt: " I'm feeling pretty confident about my outfit for the party tonight.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 8266, 5020, 16506, 911, 847, 27303, 369, 279, 4614, 17913, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:54,831 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-277ec7a3c2b14966af8fa411d19a0885-0.
2025-12-19 23:58:54,833 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:54,871 - vllm.entrypoints.logger - INFO - Received request cmpl-5caac0d3164f48f1b267abd01279bc03-0: prompt: 'Hi, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:54,874 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5caac0d3164f48f1b267abd01279bc03-0.
2025-12-19 23:58:54,875 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:54,917 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,958 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:54,999 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,011 - vllm.entrypoints.logger - INFO - Received request cmpl-c0bec3f70eb247f5b72ffcddb65a54de-0: prompt: "Hey, do you need any help with your homework? I'm pretty good at math, maybe I can assist you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 1184, 894, 1492, 448, 697, 28459, 30, 358, 2776, 5020, 1661, 518, 6888, 11, 7196, 358, 646, 7789, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:55,043 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c0bec3f70eb247f5b72ffcddb65a54de-0.
2025-12-19 23:58:55,045 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:55,088 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,132 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,173 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,213 - vllm.entrypoints.logger - INFO - Received request cmpl-6b2616ac249b4daba16d66fc792e137b-0: prompt: 'Hey, have you ever borrowed money from a friend and forgot to pay them back?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 45564, 3220, 504, 264, 4238, 323, 28595, 311, 2291, 1105, 1182, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:55,215 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,257 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6b2616ac249b4daba16d66fc792e137b-0.
2025-12-19 23:58:55,258 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:55,278 - vllm.entrypoints.logger - INFO - Received request cmpl-e6848b3a804a4ba1bf5967523b0e2687-0: prompt: 'Hey there,. Can I talk to you for a second?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 17515, 2980, 358, 3061, 311, 498, 369, 264, 2086, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:55,299 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e6848b3a804a4ba1bf5967523b0e2687-0.
2025-12-19 23:58:55,304 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:55,345 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,388 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,430 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,471 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,514 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,555 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,596 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,637 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,677 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,718 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,761 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,801 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,842 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,883 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:55,923 - vllm.entrypoints.logger - INFO - Received request cmpl-e9bbd845f9324b86a7de8c9847fb8df0-0: prompt: "I can't believe I got the part in the play! I'm so excited to finally have the chance to act.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 2684, 279, 949, 304, 279, 1486, 0, 358, 2776, 773, 12035, 311, 5499, 614, 279, 6012, 311, 1160, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:55,926 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e9bbd845f9324b86a7de8c9847fb8df0-0.
2025-12-19 23:58:55,927 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:55,969 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,010 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,025 - vllm.entrypoints.logger - INFO - Received request cmpl-c1afaabe2ab2424095979e620ef641d7-0: prompt: "The weather is amazing today! Don't you just love when the sun is shining like this?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [785, 9104, 374, 7897, 3351, 0, 4320, 944, 498, 1101, 2948, 979, 279, 7015, 374, 47925, 1075, 419, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,045 - vllm.entrypoints.logger - INFO - Received request cmpl-0bfa05272baa438b9a3857ce3cda0fa2-0: prompt: 'I was so motivated to run 10 miles yesterday.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 773, 26664, 311, 1598, 220, 16, 15, 8756, 13671, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,056 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c1afaabe2ab2424095979e620ef641d7-0.
2025-12-19 23:58:56,057 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0bfa05272baa438b9a3857ce3cda0fa2-0.
2025-12-19 23:58:56,058 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:56,099 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,129 - vllm.entrypoints.logger - INFO - Received request cmpl-92951c8a4cc843339116a46adc6fce8c-0: prompt: "I've been thinking a lot about the future lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 7274, 264, 2696, 911, 279, 3853, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,141 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-92951c8a4cc843339116a46adc6fce8c-0.
2025-12-19 23:58:56,143 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:56,183 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,224 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,265 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,306 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,309 - vllm.entrypoints.logger - INFO - Received request cmpl-1e47154531d84e4bad22b846afc8d7ad-0: prompt: 'Coach, I think someone might be following me. I just saw a car a few blocks back.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 1744, 4325, 2578, 387, 2701, 752, 13, 358, 1101, 5485, 264, 1803, 264, 2421, 10010, 1182, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,319 - vllm.entrypoints.logger - INFO - Received request cmpl-1fe6d6fa30b74bfc8247dd90c32d3f0d-0: prompt: 'Hey, did you bring any snacks with you today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 4446, 894, 42302, 448, 498, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,335 - vllm.entrypoints.logger - INFO - Received request cmpl-6d65ab0dce7746f3a3d456e1886f2e4f-0: prompt: 'Hey, have you heard from our other neighbors lately?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 504, 1039, 1008, 18709, 30345, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,351 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1e47154531d84e4bad22b846afc8d7ad-0.
2025-12-19 23:58:56,355 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1fe6d6fa30b74bfc8247dd90c32d3f0d-0.
2025-12-19 23:58:56,356 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6d65ab0dce7746f3a3d456e1886f2e4f-0.
2025-12-19 23:58:56,358 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:56,358 - vllm.entrypoints.logger - INFO - Received request cmpl-2afce6a8c52541dd9ea5819eb90114fd-0: prompt: "I'm going to the store. Do you need anything, dear?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2087, 311, 279, 3553, 13, 3155, 498, 1184, 4113, 11, 24253, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,397 - vllm.entrypoints.logger - INFO - Received request cmpl-609ee3b7953f445f941fcfca3f6cf0ab-0: prompt: 'I want to share something with you today, dear. Something that I hope will inspire you and help you throughout your life.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1366, 311, 4332, 2494, 448, 498, 3351, 11, 24253, 13, 24656, 429, 358, 3900, 686, 30640, 498, 323, 1492, 498, 6814, 697, 2272, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,400 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2afce6a8c52541dd9ea5819eb90114fd-0.
2025-12-19 23:58:56,402 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-609ee3b7953f445f941fcfca3f6cf0ab-0.
2025-12-19 23:58:56,403 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:56,445 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,479 - vllm.entrypoints.logger - INFO - Received request cmpl-16b99f9cb7104a2e908cc42202ade6c4-0: prompt: 'I have some exciting news, kiddo! We just updated our data and we have so many people signed up for our newsletter.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 1045, 13245, 3669, 11, 91415, 78, 0, 1205, 1101, 6049, 1039, 821, 323, 582, 614, 773, 1657, 1251, 8499, 705, 369, 1039, 20233, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,484 - vllm.entrypoints.logger - INFO - Received request cmpl-a48ad755e6be43f38421a24c0e82d8f8-0: prompt: 'Hey, Neighbors B! I have a little favor to ask you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 0, 358, 614, 264, 2632, 4694, 311, 2548, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,488 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-16b99f9cb7104a2e908cc42202ade6c4-0.
2025-12-19 23:58:56,489 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a48ad755e6be43f38421a24c0e82d8f8-0.
2025-12-19 23:58:56,491 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:56,519 - vllm.entrypoints.logger - INFO - Received request cmpl-15cc612c495445d1be7bf6a7263a0692-0: prompt: "I can't believe I spilled punch on my crush at the dance last night.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 73025, 20380, 389, 847, 5030, 518, 279, 15254, 1537, 3729, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,535 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-15cc612c495445d1be7bf6a7263a0692-0.
2025-12-19 23:58:56,536 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:56,577 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,592 - vllm.entrypoints.logger - INFO - Received request cmpl-7aeeb9bcbef349b9b26305b3bd18eaeb-0: prompt: "I can't believe I almost got into a fight with Cliff today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 4558, 2684, 1119, 264, 4367, 448, 47852, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,621 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7aeeb9bcbef349b9b26305b3bd18eaeb-0.
2025-12-19 23:58:56,623 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:56,665 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,707 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,749 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,791 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:56,829 - vllm.entrypoints.logger - INFO - Received request cmpl-4eb73f5ff661470ca9060a62d0959b71-0: prompt: "You know, when I was younger, life was pretty tough. But now that I'm older, I want to enjoy myself and have some fun.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 979, 358, 572, 14650, 11, 2272, 572, 5020, 11045, 13, 1988, 1431, 429, 358, 2776, 9014, 11, 358, 1366, 311, 4669, 7037, 323, 614, 1045, 2464, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,835 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4eb73f5ff661470ca9060a62d0959b71-0.
2025-12-19 23:58:56,836 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:56,848 - vllm.entrypoints.logger - INFO - Received request cmpl-265af3b39ec4440e8f741bc69cca2677-0: prompt: 'Hey, did I tell you about the track meet I went to last weekend?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 911, 279, 3754, 3367, 358, 3937, 311, 1537, 9001, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,853 - vllm.entrypoints.logger - INFO - Received request cmpl-96a88106e95c4732a7fb3cf89d28f706-0: prompt: 'Coach, I need to talk to you about something personal.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 4345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,875 - vllm.entrypoints.logger - INFO - Received request cmpl-2df12d21b54544439283e83a7bf88c3e-0: prompt: 'Hey, have I told you that I joined Ocelot Attack?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 358, 3229, 498, 429, 358, 10859, 506, 3672, 354, 20790, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,878 - vllm.entrypoints.logger - INFO - Received request cmpl-30567bbf04bf4e509015cb62d9be520e-0: prompt: 'Hey, good morning neighbor! Guess what happened last night?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1661, 6556, 9565, 0, 54279, 1128, 6932, 1537, 3729, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,883 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-265af3b39ec4440e8f741bc69cca2677-0.
2025-12-19 23:58:56,884 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-96a88106e95c4732a7fb3cf89d28f706-0.
2025-12-19 23:58:56,886 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2df12d21b54544439283e83a7bf88c3e-0.
2025-12-19 23:58:56,887 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-30567bbf04bf4e509015cb62d9be520e-0.
2025-12-19 23:58:56,888 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:58:56,900 - vllm.entrypoints.logger - INFO - Received request cmpl-6a361894d23c43e4b2d780b389c63047-0: prompt: "Why do you always have to be so sensitive? I'm just speaking my mind.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10234, 653, 498, 2677, 614, 311, 387, 773, 16216, 30, 358, 2776, 1101, 12094, 847, 3971, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,910 - vllm.entrypoints.logger - INFO - Received request cmpl-5e826521887e46969139e8f4c39a3ab7-0: prompt: ' Hey, do you remember when we were at the playground and I let you go ahead of me on the slide?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [27553, 11, 653, 498, 6099, 979, 582, 1033, 518, 279, 41615, 323, 358, 1077, 498, 728, 8305, 315, 752, 389, 279, 14983, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,922 - vllm.entrypoints.logger - INFO - Received request cmpl-3c7a2eb4a6524540a29d90aab45a6356-0: prompt: 'Mentor, I was in a dark place last year. I felt so alone.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 572, 304, 264, 6319, 1992, 1537, 1042, 13, 358, 6476, 773, 7484, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,926 - vllm.entrypoints.logger - INFO - Received request cmpl-d6d8f0804231437ab5899435a4d70309-0: prompt: "I've been thinking about the power of persuasion lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 7274, 911, 279, 2355, 315, 97124, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,931 - vllm.entrypoints.logger - INFO - Received request cmpl-2371ffb5a8e34b2da59beb8ad22aec09-0: prompt: 'Hey, have you ever tried nature photography?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 6679, 6993, 23751, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,934 - vllm.entrypoints.logger - INFO - Received request cmpl-cc58566593fb4060819baf3c88873f7c-0: prompt: "I'm so happy that the surgery went well. Dr. Johnson was really experienced and made me feel confident about the whole thing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 6247, 429, 279, 14829, 3937, 1632, 13, 2926, 13, 11351, 572, 2167, 10321, 323, 1865, 752, 2666, 16506, 911, 279, 4361, 3166, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,938 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6a361894d23c43e4b2d780b389c63047-0.
2025-12-19 23:58:56,939 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5e826521887e46969139e8f4c39a3ab7-0.
2025-12-19 23:58:56,940 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3c7a2eb4a6524540a29d90aab45a6356-0.
2025-12-19 23:58:56,942 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d6d8f0804231437ab5899435a4d70309-0.
2025-12-19 23:58:56,943 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2371ffb5a8e34b2da59beb8ad22aec09-0.
2025-12-19 23:58:56,944 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cc58566593fb4060819baf3c88873f7c-0.
2025-12-19 23:58:56,945 - vllm.core.scheduler - INFO - Pending queue size: (6)
2025-12-19 23:58:56,986 - vllm.entrypoints.logger - INFO - Received request cmpl-b24dcde9ec5b47b1b3526fd3e4092fff-0: prompt: "I still can't believe I won the lottery. It feels like a dream come true.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2058, 646, 944, 4411, 358, 2765, 279, 38239, 13, 1084, 11074, 1075, 264, 7904, 2525, 830, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:56,990 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,036 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b24dcde9ec5b47b1b3526fd3e4092fff-0.
2025-12-19 23:58:57,037 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:57,040 - vllm.entrypoints.logger - INFO - Received request cmpl-0f2fdca5fb284d5faa10a1e418c0b020-0: prompt: "Let me take your bag, Neighbors B. I don't want you to have to carry anything. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10061, 752, 1896, 697, 8968, 11, 4182, 24101, 425, 13, 358, 1513, 944, 1366, 498, 311, 614, 311, 6777, 4113, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,079 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0f2fdca5fb284d5faa10a1e418c0b020-0.
2025-12-19 23:58:57,081 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:57,116 - vllm.entrypoints.logger - INFO - Received request cmpl-943ca2761abe47b69928e663df83d3bf-0: prompt: "I'm so tired lately. I don't remember the last time I had a day off.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 19227, 30345, 13, 358, 1513, 944, 6099, 279, 1537, 882, 358, 1030, 264, 1899, 1007, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,123 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-943ca2761abe47b69928e663df83d3bf-0.
2025-12-19 23:58:57,125 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:57,128 - vllm.entrypoints.logger - INFO - Received request cmpl-999fd3d7ac7b4816b6196a532e5e5be7-0: prompt: "I've been really impressed with how well you handle tough situations,. You always seem to keep your cool and handle things with grace.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 2167, 24404, 448, 1246, 1632, 498, 3705, 11045, 14740, 17515, 1446, 2677, 2803, 311, 2506, 697, 7010, 323, 3705, 2513, 448, 20839, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,166 - vllm.entrypoints.logger - INFO - Received request cmpl-413518c2b7fd4c539fdeb2d2900da496-0: prompt: 'I love keeping my things in order. Seeing everything neatly arranged on my shelves just makes me so happy.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 10282, 847, 2513, 304, 1973, 13, 55024, 4297, 62166, 27802, 389, 847, 35210, 1101, 3643, 752, 773, 6247, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,167 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-999fd3d7ac7b4816b6196a532e5e5be7-0.
2025-12-19 23:58:57,168 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-413518c2b7fd4c539fdeb2d2900da496-0.
2025-12-19 23:58:57,169 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:57,171 - vllm.entrypoints.logger - INFO - Received request cmpl-659d587c1f5c42ec8eb69734bf30db85-0: prompt: " I understand that, but it just doesn't feel like something I'm capable of doing. I don't have the patience or motivation for it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 3535, 429, 11, 714, 432, 1101, 3171, 944, 2666, 1075, 2494, 358, 2776, 12875, 315, 3730, 13, 358, 1513, 944, 614, 279, 29301, 476, 24798, 369, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,195 - vllm.entrypoints.logger - INFO - Received request cmpl-9ea4507488844116bf2516e1a72c7048-0: prompt: 'Hey there, Neighbors B. What are you doing standing so close to the cliff edge?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 4182, 24101, 425, 13, 3555, 525, 498, 3730, 11259, 773, 3265, 311, 279, 43006, 6821, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,213 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-659d587c1f5c42ec8eb69734bf30db85-0.
2025-12-19 23:58:57,214 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9ea4507488844116bf2516e1a72c7048-0.
2025-12-19 23:58:57,216 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:57,258 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,300 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,319 - vllm.entrypoints.logger - INFO - Received request cmpl-0e9d73798c714a95b5fe4c71f03e0a68-0: prompt: 'Today, I helped a student who was struggling with math. It feels good to see them improving with the help of a tutor.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 8910, 264, 5458, 879, 572, 19962, 448, 6888, 13, 1084, 11074, 1661, 311, 1490, 1105, 18392, 448, 279, 1492, 315, 264, 25302, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,344 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0e9d73798c714a95b5fe4c71f03e0a68-0.
2025-12-19 23:58:57,346 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:57,377 - vllm.entrypoints.logger - INFO - Received request cmpl-b43899de085d4759979287fcadef43c8-0: prompt: 'Hey, did you hear about the story we were working on for English class?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 279, 3364, 582, 1033, 3238, 389, 369, 6364, 536, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,387 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b43899de085d4759979287fcadef43c8-0.
2025-12-19 23:58:57,389 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:57,404 - vllm.entrypoints.logger - INFO - Received request cmpl-7da8e16ec00a4e11b1891b9fe9b503a1-0: prompt: 'This kitten is so cute. I love how soft its fur is.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 76657, 374, 773, 18838, 13, 358, 2948, 1246, 8413, 1181, 18241, 374, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,426 - vllm.entrypoints.logger - INFO - Received request cmpl-c904bfea90584f929705bc5f1e299aaf-0: prompt: 'Did you see the latest post by Ariana Grande on Instagram?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 1490, 279, 5535, 1736, 553, 41526, 3362, 36523, 389, 13991, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,433 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7da8e16ec00a4e11b1891b9fe9b503a1-0.
2025-12-19 23:58:57,435 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c904bfea90584f929705bc5f1e299aaf-0.
2025-12-19 23:58:57,436 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:57,478 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,523 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,568 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,612 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,657 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,699 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,699 - vllm.entrypoints.logger - INFO - Received request cmpl-e42d96e10bcd474aaca6fda95db3f645-0: prompt: "I feel like I'm starting to get the hang of English now. I'm able to have basic conversations without feeling too nervous.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 1075, 358, 2776, 5916, 311, 633, 279, 14678, 315, 6364, 1431, 13, 358, 2776, 2952, 311, 614, 6770, 20959, 2041, 8266, 2238, 22596, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,742 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e42d96e10bcd474aaca6fda95db3f645-0.
2025-12-19 23:58:57,744 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:57,783 - vllm.entrypoints.logger - INFO - Received request cmpl-4e1ccab7c44d44fdb34a770b9e28362a-0: prompt: "*panting* Sorry, I just needed to borrow some money. I'll pay you back, I promise. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [31168, 517, 287, 9, 32286, 11, 358, 1101, 4362, 311, 17193, 1045, 3220, 13, 358, 3278, 2291, 498, 1182, 11, 358, 11222, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,785 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,829 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4e1ccab7c44d44fdb34a770b9e28362a-0.
2025-12-19 23:58:57,830 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:57,834 - vllm.entrypoints.logger - INFO - Received request cmpl-08a6b4ad737d4d238f39719c682e63a5-0: prompt: 'I am so happy we are in Mexico together!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1079, 773, 6247, 582, 525, 304, 12270, 3786, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,872 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-08a6b4ad737d4d238f39719c682e63a5-0.
2025-12-19 23:58:57,873 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:57,915 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:57,935 - vllm.entrypoints.logger - INFO - Received request cmpl-1c2b46149a6548c4a70c43f6a8d03015-0: prompt: "I can't stop thinking about that kiss. It was amazing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 2936, 7274, 911, 429, 21057, 13, 1084, 572, 7897, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:57,959 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1c2b46149a6548c4a70c43f6a8d03015-0.
2025-12-19 23:58:57,961 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,002 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,005 - vllm.entrypoints.logger - INFO - Received request cmpl-c4a9e09cd89b4765abfe8a200dde17e7-0: prompt: "I've been so focused on my business lately. It takes up all of my time and energy, but it's worth it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 773, 10735, 389, 847, 2562, 30345, 13, 1084, 4990, 705, 678, 315, 847, 882, 323, 4802, 11, 714, 432, 594, 5802, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,048 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c4a9e09cd89b4765abfe8a200dde17e7-0.
2025-12-19 23:58:58,050 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,092 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,135 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,176 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,220 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,262 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,303 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,345 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,349 - vllm.entrypoints.logger - INFO - Received request cmpl-f3b32015dd4740deb6b88fb38c20d258-0: prompt: 'Hey, I applied for a job at that big company downtown!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 9251, 369, 264, 2618, 518, 429, 2409, 2813, 18907, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,370 - vllm.entrypoints.logger - INFO - Received request cmpl-b177ed3718ea414ba596668fc8cc9c6d-0: prompt: 'Hey B, did you see that email from our editor about the article I submitted?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 425, 11, 1521, 498, 1490, 429, 2551, 504, 1039, 6440, 911, 279, 4549, 358, 14634, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,389 - vllm.engine.metrics - INFO - Avg prompt throughput: 175.5 tokens/s, Avg generation throughput: 133.0 tokens/s, Running: 9 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.
2025-12-19 23:58:58,387 - vllm.entrypoints.logger - INFO - Received request cmpl-a049a4d5d72b4b9ab51034576cfc43d3-0: prompt: 'Today, I met a really nice waitress at the caf. I always try to be kind to people in service positions.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 2270, 264, 2167, 6419, 95207, 518, 279, 51950, 13, 358, 2677, 1430, 311, 387, 3093, 311, 1251, 304, 2473, 9892, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,392 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f3b32015dd4740deb6b88fb38c20d258-0.
2025-12-19 23:58:58,393 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b177ed3718ea414ba596668fc8cc9c6d-0.
2025-12-19 23:58:58,395 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a049a4d5d72b4b9ab51034576cfc43d3-0.
2025-12-19 23:58:58,396 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:58,403 - vllm.entrypoints.logger - INFO - Received request cmpl-13b28fd20d154f17b1383ec34dfbe94d-0: prompt: "I can't believe I broke my guitar. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 14422, 847, 16986, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,439 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-13b28fd20d154f17b1383ec34dfbe94d-0.
2025-12-19 23:58:58,441 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,469 - vllm.entrypoints.logger - INFO - Received request cmpl-b82303220cf04e62991308f56e00feaf-0: prompt: 'I think you understand how I feel, B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1744, 498, 3535, 1246, 358, 2666, 11, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,485 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b82303220cf04e62991308f56e00feaf-0.
2025-12-19 23:58:58,487 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,527 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,552 - vllm.entrypoints.logger - INFO - Received request cmpl-3997e506c88644fd8659e1ef89dc1e33-0: prompt: 'I was hoping that joining the forum would help me make some new friends.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 15652, 429, 18169, 279, 11848, 1035, 1492, 752, 1281, 1045, 501, 4780, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,562 - vllm.entrypoints.logger - INFO - Received request cmpl-63aed0ab79b147619e740d70503cfac8-0: prompt: "Hey Mentor, what do you think about the current situation we're in?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 1128, 653, 498, 1744, 911, 279, 1482, 6534, 582, 2299, 304, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,571 - vllm.entrypoints.logger - INFO - Received request cmpl-6ce859ad92aa4d53820da68e8320ce3f-0: prompt: "Ugh, I can't believe I forgot to take my medicine this morning.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 646, 944, 4411, 358, 28595, 311, 1896, 847, 15712, 419, 6556, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,572 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3997e506c88644fd8659e1ef89dc1e33-0.
2025-12-19 23:58:58,573 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-63aed0ab79b147619e740d70503cfac8-0.
2025-12-19 23:58:58,574 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6ce859ad92aa4d53820da68e8320ce3f-0.
2025-12-19 23:58:58,576 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:58,602 - vllm.entrypoints.logger - INFO - Received request cmpl-490973b866754f818032e119767c0d19-0: prompt: "I'm so excited for my next trip! I'm headed to Japan next month.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 369, 847, 1790, 8411, 0, 358, 2776, 19383, 311, 6323, 1790, 2254, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,619 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-490973b866754f818032e119767c0d19-0.
2025-12-19 23:58:58,621 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,663 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,691 - vllm.entrypoints.logger - INFO - Received request cmpl-77f60d472e9f414c80fe8a53e8897575-0: prompt: "I've been trying to be more open to new ideas lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 4460, 311, 387, 803, 1787, 311, 501, 6708, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,705 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-77f60d472e9f414c80fe8a53e8897575-0.
2025-12-19 23:58:58,707 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,748 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,759 - vllm.entrypoints.logger - INFO - Received request cmpl-127b732aade74712b89e2879b01167f2-0: prompt: "Mentor, I've been feeling overwhelmed lately with all the attention I've been getting as a veteran. It's nice to be appreciated, but I don't feel like I deserve all of it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=47, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 3003, 1012, 8266, 42106, 30345, 448, 678, 279, 6529, 358, 3003, 1012, 3709, 438, 264, 20820, 13, 1084, 594, 6419, 311, 387, 25808, 11, 714, 358, 1513, 944, 2666, 1075, 358, 22695, 678, 315, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,792 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-127b732aade74712b89e2879b01167f2-0.
2025-12-19 23:58:58,794 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,823 - vllm.entrypoints.logger - INFO - Received request cmpl-6aa3efb83601419083f7d247fda0069f-0: prompt: "I just can't stop thinking about what happened between us. I regret not trusting you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 2936, 7274, 911, 1128, 6932, 1948, 601, 13, 358, 22231, 537, 68244, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,836 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6aa3efb83601419083f7d247fda0069f-0.
2025-12-19 23:58:58,837 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,877 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,890 - vllm.entrypoints.logger - INFO - Received request cmpl-7fa028b69a5f4d658e70f952e2eeaf3e-0: prompt: 'Hey, how was your football game last weekend?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 572, 697, 8964, 1809, 1537, 9001, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:58,920 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7fa028b69a5f4d658e70f952e2eeaf3e-0.
2025-12-19 23:58:58,922 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:58,963 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:58,969 - vllm.entrypoints.logger - INFO - Received request cmpl-fc655a5877c3463b91f54e0c40720f32-0: prompt: 'Hey there, I wanted to talk to you about something that happened to me recently.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 6932, 311, 752, 5926, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,007 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fc655a5877c3463b91f54e0c40720f32-0.
2025-12-19 23:58:59,009 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:59,052 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:59,074 - vllm.entrypoints.logger - INFO - Received request cmpl-94ef56f91f03474aaa945bb8c0acb7b4-0: prompt: ' I had a crazy morning today. I witnessed a car accident on my way to work.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1030, 264, 14264, 6556, 3351, 13, 358, 31026, 264, 1803, 11423, 389, 847, 1616, 311, 975, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,095 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-94ef56f91f03474aaa945bb8c0acb7b4-0.
2025-12-19 23:58:59,097 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:59,139 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:59,150 - vllm.entrypoints.logger - INFO - Received request cmpl-cdcc9f74d4da4ef2b0efc7e3e5d11ffe-0: prompt: 'I had the best time yesterday! I went for a run outside and it was so refreshing.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 279, 1850, 882, 13671, 0, 358, 3937, 369, 264, 1598, 4889, 323, 432, 572, 773, 35918, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,171 - vllm.entrypoints.logger - INFO - Received request cmpl-fd4c54faadaf4d5ea51326de128aef3d-0: prompt: "I'm sorry if I misread your facial expression earlier. I thought you were mad at me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 14589, 421, 358, 5786, 878, 697, 27800, 7493, 6788, 13, 358, 3381, 498, 1033, 12796, 518, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,184 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cdcc9f74d4da4ef2b0efc7e3e5d11ffe-0.
2025-12-19 23:58:59,188 - vllm.entrypoints.logger - INFO - Received request cmpl-c560b407b8c1410e8642718105f8068e-0: prompt: 'Hey, B, can we talk for a minute?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 11, 646, 582, 3061, 369, 264, 9383, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,195 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fd4c54faadaf4d5ea51326de128aef3d-0.
2025-12-19 23:58:59,197 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:58:59,197 - vllm.entrypoints.logger - INFO - Received request cmpl-ec29e7bfe8c44d75872615fc1dd4c52e-0: prompt: 'Hi Neighbor B, how are you today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 97163, 425, 11, 1246, 525, 498, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,216 - vllm.entrypoints.logger - INFO - Received request cmpl-a44b82a452964a76bc551cc41785c586-0: prompt: '*listens to music on the phone*', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9, 1607, 724, 311, 4627, 389, 279, 4540, 9], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,240 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c560b407b8c1410e8642718105f8068e-0.
2025-12-19 23:58:59,242 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ec29e7bfe8c44d75872615fc1dd4c52e-0.
2025-12-19 23:58:59,243 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a44b82a452964a76bc551cc41785c586-0.
2025-12-19 23:58:59,245 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:59,248 - vllm.entrypoints.logger - INFO - Received request cmpl-4949a031a0ce4427a03cc818e8e2f25a-0: prompt: 'Hi Teacher! How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 29069, 0, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,255 - vllm.entrypoints.logger - INFO - Received request cmpl-c679ef377c2e478db6fa1b74d0a35a02-0: prompt: 'My friend came over yesterday and asked to use my shower. I allowed her to have a bath.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5050, 4238, 3697, 916, 13671, 323, 4588, 311, 990, 847, 17196, 13, 358, 5420, 1059, 311, 614, 264, 8885, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,266 - vllm.entrypoints.logger - INFO - Received request cmpl-072a64453b69473d8aafc5d22d7d9ded-0: prompt: ', have you ever done something that you regretted later?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 614, 498, 3512, 2814, 2494, 429, 498, 22231, 6565, 2937, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,270 - vllm.entrypoints.logger - INFO - Received request cmpl-177f74495010467da02f4a7f66a58916-0: prompt: 'I always strive to be the best in everything I do. I want to meet the standard set by my teachers and mentors.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 36006, 311, 387, 279, 1850, 304, 4297, 358, 653, 13, 358, 1366, 311, 3367, 279, 5297, 738, 553, 847, 13336, 323, 75607, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,273 - vllm.entrypoints.logger - INFO - Received request cmpl-85d18554bff14214a32a0cc3c5cd23ea-0: prompt: " I want to talk to you about something that's been on my mind all day. I'm feeling really grateful for my mother. She's always been there for me, even when I was a child.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1366, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 389, 847, 3971, 678, 1899, 13, 358, 2776, 8266, 2167, 25195, 369, 847, 6554, 13, 2932, 594, 2677, 1012, 1052, 369, 752, 11, 1496, 979, 358, 572, 264, 1682, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,292 - vllm.entrypoints.logger - INFO - Received request cmpl-2d1cc18438784447b94570eed09cace5-0: prompt: 'Hi Mentor, I have some great news! I got promoted to manager at the fast food restaurant where I work.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 614, 1045, 2244, 3669, 0, 358, 2684, 28926, 311, 6645, 518, 279, 4937, 3607, 10729, 1380, 358, 975, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,296 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4949a031a0ce4427a03cc818e8e2f25a-0.
2025-12-19 23:58:59,297 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c679ef377c2e478db6fa1b74d0a35a02-0.
2025-12-19 23:58:59,299 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-072a64453b69473d8aafc5d22d7d9ded-0.
2025-12-19 23:58:59,300 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-177f74495010467da02f4a7f66a58916-0.
2025-12-19 23:58:59,301 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-85d18554bff14214a32a0cc3c5cd23ea-0.
2025-12-19 23:58:59,302 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2d1cc18438784447b94570eed09cace5-0.
2025-12-19 23:58:59,303 - vllm.core.scheduler - INFO - Pending queue size: (6)
2025-12-19 23:58:59,347 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:59,398 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:59,419 - vllm.entrypoints.logger - INFO - Received request cmpl-ab2b6ec5830445d89eda82b6edb6fdc3-0: prompt: "I've been looking for a new place to move to and I finally found a small town that I really like.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 3330, 369, 264, 501, 1992, 311, 3271, 311, 323, 358, 5499, 1730, 264, 2613, 6290, 429, 358, 2167, 1075, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,445 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ab2b6ec5830445d89eda82b6edb6fdc3-0.
2025-12-19 23:58:59,446 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:59,487 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:59,517 - vllm.entrypoints.logger - INFO - Received request cmpl-7b0e8a91f92a4942aba622665e19a628-0: prompt: "I can't believe how good I've gotten at using the bow and arrow.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1246, 1661, 358, 3003, 17019, 518, 1667, 279, 15273, 323, 17921, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,531 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7b0e8a91f92a4942aba622665e19a628-0.
2025-12-19 23:58:59,532 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:59,550 - vllm.entrypoints.logger - INFO - Received request cmpl-feeb13aab6454908b22f23d7f95872a9-0: prompt: "Ha! Gotcha, Neighbors B! You couldn't outrun me this time.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [32942, 0, 24528, 6447, 11, 4182, 24101, 425, 0, 1446, 7691, 944, 17852, 359, 752, 419, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,575 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-feeb13aab6454908b22f23d7f95872a9-0.
2025-12-19 23:58:59,577 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:59,618 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:59,622 - vllm.entrypoints.logger - INFO - Received request cmpl-24dff38184a041d2b4a3ef4a6d803d06-0: prompt: 'Hey, have you seen the portrait I have in the living room?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3884, 279, 33033, 358, 614, 304, 279, 5382, 3054, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,642 - vllm.entrypoints.logger - INFO - Received request cmpl-372cf29b32cb4c0da85626bdf74cbd83-0: prompt: "I can't believe it, I got accepted into college!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 432, 11, 358, 2684, 11666, 1119, 7770, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,648 - vllm.entrypoints.logger - INFO - Received request cmpl-a3f28f267daa4e7ca4f82f6d5680f990-0: prompt: 'I was working in the kitchen earlier and started feeling exhausted.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 3238, 304, 279, 9780, 6788, 323, 3855, 8266, 37919, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,666 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-24dff38184a041d2b4a3ef4a6d803d06-0.
2025-12-19 23:58:59,668 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-372cf29b32cb4c0da85626bdf74cbd83-0.
2025-12-19 23:58:59,669 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a3f28f267daa4e7ca4f82f6d5680f990-0.
2025-12-19 23:58:59,670 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:59,675 - vllm.entrypoints.logger - INFO - Received request cmpl-cbde4b5a891b4f25b19e283f40c73ab8-0: prompt: 'Good morning, B. How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 425, 13, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,680 - vllm.entrypoints.logger - INFO - Received request cmpl-487cb2b7037b4b35ab340d7ef25cef9c-0: prompt: 'Hey, have I told you about the time I saved a woman from getting attacked?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 358, 3229, 498, 911, 279, 882, 358, 6781, 264, 5220, 504, 3709, 18349, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,708 - vllm.entrypoints.logger - INFO - Received request cmpl-9b4873dfdacd4259bc2798b9bcfe772e-0: prompt: '*humming a tune*', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9, 27300, 5311, 264, 25240, 9], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,716 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cbde4b5a891b4f25b19e283f40c73ab8-0.
2025-12-19 23:58:59,717 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-487cb2b7037b4b35ab340d7ef25cef9c-0.
2025-12-19 23:58:59,719 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9b4873dfdacd4259bc2798b9bcfe772e-0.
2025-12-19 23:58:59,720 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:59,720 - vllm.entrypoints.logger - INFO - Received request cmpl-f1e8b9110b3f4c1287963d25d5648e45-0: prompt: "I don't know how much longer I can keep going through this IVF treatment. It's really taking a toll on me both physically and emotionally.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 1246, 1753, 5021, 358, 646, 2506, 2087, 1526, 419, 16824, 37, 6380, 13, 1084, 594, 2167, 4633, 264, 25851, 389, 752, 2176, 21893, 323, 37583, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,763 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f1e8b9110b3f4c1287963d25d5648e45-0.
2025-12-19 23:58:59,764 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:59,806 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:59,819 - vllm.entrypoints.logger - INFO - Received request cmpl-3198798e84094a0099415d79cb1d7e02-0: prompt: 'Hey there, guess what? I bought a razor today and shaved my face for the first time!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 7942, 1128, 30, 358, 10788, 264, 59130, 3351, 323, 65199, 847, 3579, 369, 279, 1156, 882, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,853 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3198798e84094a0099415d79cb1d7e02-0.
2025-12-19 23:58:59,855 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:58:59,897 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:58:59,929 - vllm.entrypoints.logger - INFO - Received request cmpl-6cd89a1db22c4631bad691781178ef2a-0: prompt: "I know it's silly, but I just can't help it. I always get scared when someone gets angry or confrontational.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1414, 432, 594, 29471, 11, 714, 358, 1101, 646, 944, 1492, 432, 13, 358, 2677, 633, 26115, 979, 4325, 5221, 18514, 476, 16877, 1663, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,931 - vllm.entrypoints.logger - INFO - Received request cmpl-853efa4145ec4e8c984d5e65b3d47946-0: prompt: "Hi Doctor, I'm feeling really upset. I was supposed to go on my vacation to Florida today, but I left my passport at home and missed my flight.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 18635, 11, 358, 2776, 8266, 2167, 22459, 13, 358, 572, 9966, 311, 728, 389, 847, 20161, 311, 9589, 3351, 11, 714, 358, 2115, 847, 25458, 518, 2114, 323, 13628, 847, 10971, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,940 - vllm.entrypoints.logger - INFO - Received request cmpl-b71bd6bf857f4f30977357abd0a75aea-0: prompt: "Hey there, I'm back. Sorry that took longer than I thought.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 358, 2776, 1182, 13, 32286, 429, 3867, 5021, 1091, 358, 3381, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,943 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6cd89a1db22c4631bad691781178ef2a-0.
2025-12-19 23:58:59,945 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-853efa4145ec4e8c984d5e65b3d47946-0.
2025-12-19 23:58:59,946 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b71bd6bf857f4f30977357abd0a75aea-0.
2025-12-19 23:58:59,947 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:58:59,989 - vllm.entrypoints.logger - INFO - Received request cmpl-f5fe69b35bb64d8f91c36607e830a9a7-0: prompt: 'Hey Mentor, I brought my guitar today. I was hoping I could teach you how to play a few chords.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 7117, 847, 16986, 3351, 13, 358, 572, 15652, 358, 1410, 4538, 498, 1246, 311, 1486, 264, 2421, 55659, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:58:59,990 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:00,027 - vllm.entrypoints.logger - INFO - Received request cmpl-d73d245f6eea4e12893073e77af1538f-0: prompt: 'Hey, Teacher. I wanted to tell you about something exciting that happened. I had enough money saved up, so I bought an electronic dictionary from the bookstore.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=34, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 29069, 13, 358, 4829, 311, 3291, 498, 911, 2494, 13245, 429, 6932, 13, 358, 1030, 3322, 3220, 6781, 705, 11, 773, 358, 10788, 458, 14346, 10997, 504, 279, 78661, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,031 - vllm.entrypoints.logger - INFO - Received request cmpl-8740e73408f140e88d578c5fa1c40e19-0: prompt: " Doctor, I've been feeling really down lately and I think I need some help.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18635, 11, 358, 3003, 1012, 8266, 2167, 1495, 30345, 323, 358, 1744, 358, 1184, 1045, 1492, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,034 - vllm.entrypoints.logger - INFO - Received request cmpl-2435d5b1451e43ab8f9969b7d4d18107-0: prompt: 'Hi, Teacher! I finished typing out my report. Do you want to take a look?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 29069, 0, 358, 8060, 19496, 700, 847, 1895, 13, 3155, 498, 1366, 311, 1896, 264, 1401, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,039 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f5fe69b35bb64d8f91c36607e830a9a7-0.
2025-12-19 23:59:00,040 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d73d245f6eea4e12893073e77af1538f-0.
2025-12-19 23:59:00,042 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8740e73408f140e88d578c5fa1c40e19-0.
2025-12-19 23:59:00,043 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2435d5b1451e43ab8f9969b7d4d18107-0.
2025-12-19 23:59:00,044 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:00,059 - vllm.entrypoints.logger - INFO - Received request cmpl-ce80bb09ca0c4a4b9401a35b4005a4a6-0: prompt: 'I think the blue one would look great on you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1744, 279, 6303, 825, 1035, 1401, 2244, 389, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,077 - vllm.entrypoints.logger - INFO - Received request cmpl-c9f30b05e91d433aa5e773f53e2ce7f9-0: prompt: "I have always been aware that I am different from you, Classmates B. What I didn't realize is that recently, you have started to feel differently about me too. We used to be so close, but now it feels like there is a gulf between us.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 2677, 1012, 7853, 429, 358, 1079, 2155, 504, 498, 11, 3228, 16457, 425, 13, 3555, 358, 3207, 944, 13083, 374, 429, 5926, 11, 498, 614, 3855, 311, 2666, 21303, 911, 752, 2238, 13, 1205, 1483, 311, 387, 773, 3265, 11, 714, 1431, 432, 11074, 1075, 1052, 374, 264, 342, 14308, 1948, 601, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,090 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ce80bb09ca0c4a4b9401a35b4005a4a6-0.
2025-12-19 23:59:00,092 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c9f30b05e91d433aa5e773f53e2ce7f9-0.
2025-12-19 23:59:00,093 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:00,104 - vllm.entrypoints.logger - INFO - Received request cmpl-d7df1b7b558a42f88db8665cb139e1b1-0: prompt: 'Ha! As if that thing ever tells the right time!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [32942, 0, 1634, 421, 429, 3166, 3512, 10742, 279, 1290, 882, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,134 - vllm.entrypoints.logger - INFO - Received request cmpl-4601fbf37a284763b41cd107f663fec8-0: prompt: 'Hey, are you okay? You look upset.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 525, 498, 16910, 30, 1446, 1401, 22459, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,135 - vllm.entrypoints.logger - INFO - Received request cmpl-3f5557ee78134731b26e85ac494a8686-0: prompt: "I feel like I always act before thinking things through. It's gotten me into trouble a lot.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 1075, 358, 2677, 1160, 1573, 7274, 2513, 1526, 13, 1084, 594, 17019, 752, 1119, 12264, 264, 2696, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,140 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d7df1b7b558a42f88db8665cb139e1b1-0.
2025-12-19 23:59:00,141 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4601fbf37a284763b41cd107f663fec8-0.
2025-12-19 23:59:00,143 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3f5557ee78134731b26e85ac494a8686-0.
2025-12-19 23:59:00,144 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:00,151 - vllm.entrypoints.logger - INFO - Received request cmpl-c81877a4504143bbbde449d1d775a2ef-0: prompt: 'Hey, did you see the commercial for the new blockbuster movie?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 1490, 279, 8353, 369, 279, 501, 70997, 5700, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,189 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c81877a4504143bbbde449d1d775a2ef-0.
2025-12-19 23:59:00,190 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:00,207 - vllm.entrypoints.logger - INFO - Received request cmpl-b17f6ea80a4240c9a8605ebe1797d6dc-0: prompt: 'Hi there, have you ever thought about building a playground for the kids in our town?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 614, 498, 3512, 3381, 911, 4752, 264, 41615, 369, 279, 6837, 304, 1039, 6290, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,229 - vllm.entrypoints.logger - INFO - Received request cmpl-9d500b7a5ae34b099aa9cd1b3f17e308-0: prompt: "Hey there, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,236 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b17f6ea80a4240c9a8605ebe1797d6dc-0.
2025-12-19 23:59:00,237 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9d500b7a5ae34b099aa9cd1b3f17e308-0.
2025-12-19 23:59:00,238 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:00,263 - vllm.entrypoints.logger - INFO - Received request cmpl-ea92bc1cdff84f2da73077891b24e686-0: prompt: 'Hi Mentor! How have you been doing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 0, 2585, 614, 498, 1012, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,265 - vllm.entrypoints.logger - INFO - Received request cmpl-efce61addfa6480e808e83bd9b49dbb2-0: prompt: ", I saw a spider crawling up my leg earlier and I freaked out. Now I'm trying to stay away from all spiders.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 5485, 264, 34354, 71079, 705, 847, 2472, 6788, 323, 358, 29205, 291, 700, 13, 4695, 358, 2776, 4460, 311, 4717, 3123, 504, 678, 62136, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,268 - vllm.entrypoints.logger - INFO - Received request cmpl-42ca7d85e7c5410cae0dd83d0fd635c1-0: prompt: 'I remember feeling really discouraged when I got a C on my math test a couple of years ago. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 6099, 8266, 2167, 63670, 979, 358, 2684, 264, 356, 389, 847, 6888, 1273, 264, 5625, 315, 1635, 4134, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,285 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ea92bc1cdff84f2da73077891b24e686-0.
2025-12-19 23:59:00,286 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-efce61addfa6480e808e83bd9b49dbb2-0.
2025-12-19 23:59:00,287 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-42ca7d85e7c5410cae0dd83d0fd635c1-0.
2025-12-19 23:59:00,288 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:00,327 - vllm.entrypoints.logger - INFO - Received request cmpl-683f0a1387db450fa1db17c1ede03240-0: prompt: "I really want to get this research proposal done soon. It's taking up so much of my time.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 1366, 311, 633, 419, 3412, 13734, 2814, 5135, 13, 1084, 594, 4633, 705, 773, 1753, 315, 847, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,332 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-683f0a1387db450fa1db17c1ede03240-0.
2025-12-19 23:59:00,334 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:00,376 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:00,379 - vllm.entrypoints.logger - INFO - Received request cmpl-277b7d483f6f473cbdee2dff13f8ebba-0: prompt: 'I went to the ice-cream parlor earlier today!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3937, 311, 279, 9853, 12, 46000, 1346, 9566, 6788, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,418 - vllm.entrypoints.logger - INFO - Received request cmpl-68b7376ae47b4cdfa67903ddde09fc88-0: prompt: "I always try to be polite to waitstaff. It's a tough job and I know a little kindness can go a long way.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=31, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 1430, 311, 387, 47787, 311, 3783, 27122, 13, 1084, 594, 264, 11045, 2618, 323, 358, 1414, 264, 2632, 44872, 646, 728, 264, 1293, 1616, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,424 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-277b7d483f6f473cbdee2dff13f8ebba-0.
2025-12-19 23:59:00,425 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-68b7376ae47b4cdfa67903ddde09fc88-0.
2025-12-19 23:59:00,427 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:00,445 - vllm.entrypoints.logger - INFO - Received request cmpl-da80684facb140e8acc7f982f2c6adac-0: prompt: ", I really think you should take a stand against the school bully. You don't deserve to be treated that way.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2167, 1744, 498, 1265, 1896, 264, 2498, 2348, 279, 2906, 63076, 13, 1446, 1513, 944, 22695, 311, 387, 11758, 429, 1616, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,471 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-da80684facb140e8acc7f982f2c6adac-0.
2025-12-19 23:59:00,472 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:00,486 - vllm.entrypoints.logger - INFO - Received request cmpl-26e45e7ea0b74236981bf666a8d1d6d2-0: prompt: 'So, did you like being cornered against the wall by me yesterday?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 1521, 498, 1075, 1660, 9131, 291, 2348, 279, 7002, 553, 752, 13671, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,504 - vllm.entrypoints.logger - INFO - Received request cmpl-d16a1cbdd17d4ed5b2115ff7db42f155-0: prompt: "You're so stupid, B! You spilled my drink!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 2299, 773, 18253, 11, 425, 0, 1446, 73025, 847, 7027, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,517 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-26e45e7ea0b74236981bf666a8d1d6d2-0.
2025-12-19 23:59:00,519 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d16a1cbdd17d4ed5b2115ff7db42f155-0.
2025-12-19 23:59:00,520 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:00,564 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:00,614 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:00,658 - vllm.entrypoints.logger - INFO - Received request cmpl-825176d7ce8944a69f70b15caffd2d0d-0: prompt: "I've been really getting into creative writing lately. Have you ever tried it, Classmates B?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 2167, 3709, 1119, 11521, 4378, 30345, 13, 12243, 498, 3512, 6679, 432, 11, 3228, 16457, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,660 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:00,660 - vllm.entrypoints.logger - INFO - Received request cmpl-b219530662f64e4d8c2ed75d9a430bc7-0: prompt: 'Remember that time when I was your lawyer and got you out of jail by posting bail?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [28590, 429, 882, 979, 358, 572, 697, 15417, 323, 2684, 498, 700, 315, 17540, 553, 17004, 24479, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,679 - vllm.entrypoints.logger - INFO - Received request cmpl-e5068d6fc403440bb64df8bc39f1f0cf-0: prompt: " Oh no, the power just went out. I can't see anything.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8670, 902, 11, 279, 2355, 1101, 3937, 700, 13, 358, 646, 944, 1490, 4113, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,695 - vllm.entrypoints.logger - INFO - Received request cmpl-3e995a0cd9544f7a8822bf71a8141c1b-0: prompt: "What's wrong? Why are you crying?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3838, 594, 4969, 30, 8429, 525, 498, 30199, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,711 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-825176d7ce8944a69f70b15caffd2d0d-0.
2025-12-19 23:59:00,712 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b219530662f64e4d8c2ed75d9a430bc7-0.
2025-12-19 23:59:00,714 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e5068d6fc403440bb64df8bc39f1f0cf-0.
2025-12-19 23:59:00,715 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3e995a0cd9544f7a8822bf71a8141c1b-0.
2025-12-19 23:59:00,716 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:00,717 - vllm.entrypoints.logger - INFO - Received request cmpl-f6a2d3f9f53d4a6e867af2436bab487a-0: prompt: 'Hi Mentor, today I interviewed John about his life.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 3351, 358, 29047, 3757, 911, 806, 2272, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,720 - vllm.entrypoints.logger - INFO - Received request cmpl-945e0762646942ada27e60530becd4f6-0: prompt: "Did you hear the one about the cat who couldn't jump over the fence?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 6723, 279, 825, 911, 279, 8251, 879, 7691, 944, 7784, 916, 279, 24650, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,722 - vllm.entrypoints.logger - INFO - Received request cmpl-4b300c0df66548f7b6b8d33ad9ca3b7f-0: prompt: 'Look at this banner I hung on my house! It says "Viva La Raza!" Doesn\'t it look amazing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10380, 518, 419, 23323, 358, 18295, 389, 847, 3753, 0, 1084, 2727, 330, 53, 9924, 4929, 431, 12707, 8958, 48832, 944, 432, 1401, 7897, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,732 - vllm.entrypoints.logger - INFO - Received request cmpl-52a7319ef3a34d79a4e8474616e46c57-0: prompt: ', I was at the grocery store earlier and I got what we needed for the group.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 572, 518, 279, 29587, 3553, 6788, 323, 358, 2684, 1128, 582, 4362, 369, 279, 1874, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,747 - vllm.entrypoints.logger - INFO - Received request cmpl-c8b69d4e609b4b11844480d02e417811-0: prompt: "I can't believe I struck out with the bases loaded. I feel like I let my team down.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 17490, 700, 448, 279, 23092, 6661, 13, 358, 2666, 1075, 358, 1077, 847, 2083, 1495, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,749 - vllm.entrypoints.logger - INFO - Received request cmpl-84751ddad4214f3bab5ebe8acea1deb3-0: prompt: "Hey, here's the document I was telling you about. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1588, 594, 279, 2197, 358, 572, 11629, 498, 911, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,751 - vllm.entrypoints.logger - INFO - Received request cmpl-5ce71f7c12f0417792ead24919cd5876-0: prompt: 'Thank you for helping me with these dishes, it means a lot.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13060, 498, 369, 10476, 752, 448, 1493, 25779, 11, 432, 3363, 264, 2696, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,763 - vllm.entrypoints.logger - INFO - Received request cmpl-ec156fe90b1c4dfaaaea716bc66304f4-0: prompt: 'Here you go, B. This is the book you needed for your project.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8420, 498, 728, 11, 425, 13, 1096, 374, 279, 2311, 498, 4362, 369, 697, 2390, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,768 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f6a2d3f9f53d4a6e867af2436bab487a-0.
2025-12-19 23:59:00,769 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-945e0762646942ada27e60530becd4f6-0.
2025-12-19 23:59:00,770 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4b300c0df66548f7b6b8d33ad9ca3b7f-0.
2025-12-19 23:59:00,771 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-52a7319ef3a34d79a4e8474616e46c57-0.
2025-12-19 23:59:00,772 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c8b69d4e609b4b11844480d02e417811-0.
2025-12-19 23:59:00,772 - vllm.entrypoints.logger - INFO - Received request cmpl-81595e45b4cc4b4e8fd9d5a58c60985e-0: prompt: "I can't believe it. I've been trying to sell this suit for weeks now, but no one seems interested.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 432, 13, 358, 3003, 1012, 4460, 311, 4559, 419, 7781, 369, 5555, 1431, 11, 714, 902, 825, 4977, 8014, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,773 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-84751ddad4214f3bab5ebe8acea1deb3-0.
2025-12-19 23:59:00,774 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5ce71f7c12f0417792ead24919cd5876-0.
2025-12-19 23:59:00,775 - vllm.entrypoints.logger - INFO - Received request cmpl-c2774afbb5f64a8d9863fc7303d3f0f8-0: prompt: " Doctor, I'm getting really annoyed. I feel like I can't take it anymore and want to scream.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18635, 11, 358, 2776, 3709, 2167, 56030, 13, 358, 2666, 1075, 358, 646, 944, 1896, 432, 14584, 323, 1366, 311, 45689, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,775 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ec156fe90b1c4dfaaaea716bc66304f4-0.
2025-12-19 23:59:00,776 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-81595e45b4cc4b4e8fd9d5a58c60985e-0.
2025-12-19 23:59:00,777 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c2774afbb5f64a8d9863fc7303d3f0f8-0.
2025-12-19 23:59:00,778 - vllm.core.scheduler - INFO - Pending queue size: (10)
2025-12-19 23:59:00,785 - vllm.entrypoints.logger - INFO - Received request cmpl-2c9bf3b4176e4d0984992216cb9cf057-0: prompt: "Hey, do you have a drill that I can borrow? I need it for a project I'm working on.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 614, 264, 30546, 429, 358, 646, 17193, 30, 358, 1184, 432, 369, 264, 2390, 358, 2776, 3238, 389, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,793 - vllm.entrypoints.logger - INFO - Received request cmpl-8f695dba08ab44b4bd9c08a2f990b311-0: prompt: 'This painting of the shipwreck really makes me feel for the people in the water. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 18824, 315, 279, 8284, 86, 24638, 2167, 3643, 752, 2666, 369, 279, 1251, 304, 279, 3015, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,822 - vllm.entrypoints.logger - INFO - Received request cmpl-44ccba675f4948c780a0e263bb2becfd-0: prompt: "It's been tough taking care of my dad, but I'm glad to be here for him.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 1012, 11045, 4633, 2453, 315, 847, 17760, 11, 714, 358, 2776, 15713, 311, 387, 1588, 369, 1435, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,824 - vllm.entrypoints.logger - INFO - Received request cmpl-b7d92e5057dc417c864b60add6ca5662-0: prompt: "I can't believe I had to miss work today. I was really looking forward to my shift, and now I'm going to miss out on the money.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 1030, 311, 3116, 975, 3351, 13, 358, 572, 2167, 3330, 4637, 311, 847, 6407, 11, 323, 1431, 358, 2776, 2087, 311, 3116, 700, 389, 279, 3220, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,828 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2c9bf3b4176e4d0984992216cb9cf057-0.
2025-12-19 23:59:00,829 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8f695dba08ab44b4bd9c08a2f990b311-0.
2025-12-19 23:59:00,831 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-44ccba675f4948c780a0e263bb2becfd-0.
2025-12-19 23:59:00,832 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b7d92e5057dc417c864b60add6ca5662-0.
2025-12-19 23:59:00,833 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:00,842 - vllm.entrypoints.logger - INFO - Received request cmpl-8885769369db410aaa4784c628f104ba-0: prompt: "I've been putting in a lot of extra hours this week. I really want that promotion.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 10687, 304, 264, 2696, 315, 4960, 4115, 419, 2003, 13, 358, 2167, 1366, 429, 20249, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,880 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8885769369db410aaa4784c628f104ba-0.
2025-12-19 23:59:00,882 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:00,897 - vllm.entrypoints.logger - INFO - Received request cmpl-d2a0f7b413c649b7a0573e923cd5f9da-0: prompt: "Hi there, Classmates B. I heard that you want to become a doctor. That's great! It's a noble profession.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 3228, 16457, 425, 13, 358, 6617, 429, 498, 1366, 311, 3635, 264, 10668, 13, 2938, 594, 2244, 0, 1084, 594, 264, 34382, 4808, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,908 - vllm.entrypoints.logger - INFO - Received request cmpl-ab094866761d4657b4486fd54c75d10b-0: prompt: "Hey, who were you talking to yesterday? I didn't recognize the guy.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 879, 1033, 498, 7404, 311, 13671, 30, 358, 3207, 944, 15282, 279, 7412, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,923 - vllm.entrypoints.logger - INFO - Received request cmpl-e6a99fdea8ed43f8b4830c5651a0e3a2-0: prompt: "Ah, it's nice to finally have a moment to myself. I needed to just unwind and relax.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 432, 594, 6419, 311, 5499, 614, 264, 4445, 311, 7037, 13, 358, 4362, 311, 1101, 81510, 323, 11967, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,926 - vllm.entrypoints.logger - INFO - Received request cmpl-808e963bbb2e4b24b7587ad05786ece4-0: prompt: 'I bought my niece a new toy yesterday.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 10788, 847, 79015, 264, 501, 21357, 13671, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,929 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d2a0f7b413c649b7a0573e923cd5f9da-0.
2025-12-19 23:59:00,930 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ab094866761d4657b4486fd54c75d10b-0.
2025-12-19 23:59:00,930 - vllm.entrypoints.logger - INFO - Received request cmpl-9874c6d143ec4e6fa8d4d9d770d243c7-0: prompt: "Oh my god, look at our neighbor's house! It's on fire!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 847, 9886, 11, 1401, 518, 1039, 9565, 594, 3753, 0, 1084, 594, 389, 3940, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,932 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e6a99fdea8ed43f8b4830c5651a0e3a2-0.
2025-12-19 23:59:00,933 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-808e963bbb2e4b24b7587ad05786ece4-0.
2025-12-19 23:59:00,934 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9874c6d143ec4e6fa8d4d9d770d243c7-0.
2025-12-19 23:59:00,935 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:00,944 - vllm.entrypoints.logger - INFO - Received request cmpl-0d0ec1e163574678ae6ae088e299ff60-0: prompt: 'I just feel so guilty for not helping my friend with their math homework. But I already have so much on my plate.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 2666, 773, 16007, 369, 537, 10476, 847, 4238, 448, 862, 6888, 28459, 13, 1988, 358, 2669, 614, 773, 1753, 389, 847, 11968, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,948 - vllm.entrypoints.logger - INFO - Received request cmpl-2977de02a1084c77b63a571eedd7dcb0-0: prompt: "I've been feeling really good lately, Wife. My optimism has increased, and I've been able to relax more.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 8266, 2167, 1661, 30345, 11, 42408, 13, 3017, 53408, 702, 7172, 11, 323, 358, 3003, 1012, 2952, 311, 11967, 803, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,974 - vllm.entrypoints.logger - INFO - Received request cmpl-72fd5803c6ce44039bfa56dc8b910a8d-0: prompt: 'I need to go home. I have to pack my bags and leave right away.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1184, 311, 728, 2114, 13, 358, 614, 311, 3769, 847, 17899, 323, 5274, 1290, 3123, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,977 - vllm.entrypoints.logger - INFO - Received request cmpl-0f14543162574bdda7839585fb0fec1f-0: prompt: "I can't believe how much I love my new coat. It's so warm and cozy.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1246, 1753, 358, 2948, 847, 501, 22875, 13, 1084, 594, 773, 8205, 323, 42435, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:00,983 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d0ec1e163574678ae6ae088e299ff60-0.
2025-12-19 23:59:00,985 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2977de02a1084c77b63a571eedd7dcb0-0.
2025-12-19 23:59:00,986 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-72fd5803c6ce44039bfa56dc8b910a8d-0.
2025-12-19 23:59:00,987 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0f14543162574bdda7839585fb0fec1f-0.
2025-12-19 23:59:00,988 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:01,002 - vllm.entrypoints.logger - INFO - Received request cmpl-0dc1f1ea5415436e9bade3eebd5c9215-0: prompt: ' That party a few weeks ago was crazy.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2938, 4614, 264, 2421, 5555, 4134, 572, 14264, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,006 - vllm.entrypoints.logger - INFO - Received request cmpl-7fa056a6bfe84a5ca0b24f361b2cace1-0: prompt: ' Hey, did I tell you about the prank call I made to Sarah last night?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [27553, 11, 1521, 358, 3291, 498, 911, 279, 75273, 1618, 358, 1865, 311, 20445, 1537, 3729, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,034 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0dc1f1ea5415436e9bade3eebd5c9215-0.
2025-12-19 23:59:01,035 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7fa056a6bfe84a5ca0b24f361b2cace1-0.
2025-12-19 23:59:01,036 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:01,040 - vllm.entrypoints.logger - INFO - Received request cmpl-5f9c0f934e3c4d3ca5a1208fcc5eee8f-0: prompt: "Hi, sorry to bother you, but I just got a call from my mother. She's sick and needs my help.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 14589, 311, 20147, 498, 11, 714, 358, 1101, 2684, 264, 1618, 504, 847, 6554, 13, 2932, 594, 14036, 323, 3880, 847, 1492, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,063 - vllm.entrypoints.logger - INFO - Received request cmpl-67d4bb441dd24678a91f3450cd3da249-0: prompt: 'Hey, do you have a minute to talk?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 614, 264, 9383, 311, 3061, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,078 - vllm.entrypoints.logger - INFO - Received request cmpl-68c97e0ddcc648bf9bedbdef1203dab2-0: prompt: "Hey, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,082 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5f9c0f934e3c4d3ca5a1208fcc5eee8f-0.
2025-12-19 23:59:01,084 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-67d4bb441dd24678a91f3450cd3da249-0.
2025-12-19 23:59:01,085 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-68c97e0ddcc648bf9bedbdef1203dab2-0.
2025-12-19 23:59:01,086 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:01,102 - vllm.entrypoints.logger - INFO - Received request cmpl-fd48ffa66c6d4d748fdfdaf980b25e51-0: prompt: 'Hey neighbor, have you ever carried a lucky charm with you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 614, 498, 3512, 11691, 264, 17605, 31253, 448, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,130 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fd48ffa66c6d4d748fdfdaf980b25e51-0.
2025-12-19 23:59:01,131 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:01,174 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:01,187 - vllm.entrypoints.logger - INFO - Received request cmpl-6033019d0d1e49768d075af01e0543a4-0: prompt: 'Today, I want to talk about my interest in learning new languages. I find it to be so exciting and rewarding.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 1366, 311, 3061, 911, 847, 2734, 304, 6832, 501, 15459, 13, 358, 1477, 432, 311, 387, 773, 13245, 323, 40993, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,195 - vllm.entrypoints.logger - INFO - Received request cmpl-7cd8c505e94544e680f0676753043215-0: prompt: 'Hey, have you ever been to that toy store on Main Street?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 1012, 311, 429, 21357, 3553, 389, 4697, 6686, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,204 - vllm.entrypoints.logger - INFO - Received request cmpl-f467573aa9bc483484d7b78d90365826-0: prompt: 'I really enjoy being competitive and getting things done quickly and efficiently.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 4669, 1660, 14680, 323, 3709, 2513, 2814, 6157, 323, 29720, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,223 - vllm.entrypoints.logger - INFO - Received request cmpl-35c537008b8b4ef693eb4a1ac904b22e-0: prompt: 'Ah, that water was so refreshing. I needed that.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 429, 3015, 572, 773, 35918, 13, 358, 4362, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,230 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6033019d0d1e49768d075af01e0543a4-0.
2025-12-19 23:59:01,231 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7cd8c505e94544e680f0676753043215-0.
2025-12-19 23:59:01,234 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f467573aa9bc483484d7b78d90365826-0.
2025-12-19 23:59:01,235 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-35c537008b8b4ef693eb4a1ac904b22e-0.
2025-12-19 23:59:01,236 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:01,266 - vllm.entrypoints.logger - INFO - Received request cmpl-3e843be9c9b145e3a2c22567fcbbe36d-0: prompt: "Hey, did you hear about Peter's computer issue yesterday?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 11044, 594, 6366, 4265, 13671, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,281 - vllm.entrypoints.logger - INFO - Received request cmpl-a95c1319a7224027a5ebc25595d915a7-0: prompt: "I'm so glad that I was able to get justice for what happened. It was really hard to go to the authorities, but it was the right thing to do.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 15713, 429, 358, 572, 2952, 311, 633, 12161, 369, 1128, 6932, 13, 1084, 572, 2167, 2588, 311, 728, 311, 279, 11276, 11, 714, 432, 572, 279, 1290, 3166, 311, 653, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,282 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3e843be9c9b145e3a2c22567fcbbe36d-0.
2025-12-19 23:59:01,284 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a95c1319a7224027a5ebc25595d915a7-0.
2025-12-19 23:59:01,285 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:01,306 - vllm.entrypoints.logger - INFO - Received request cmpl-366f55355d4c469683bbcc67f0135a1d-0: prompt: 'Hey, B, can I ask you something?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 11, 646, 358, 2548, 498, 2494, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,329 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-366f55355d4c469683bbcc67f0135a1d-0.
2025-12-19 23:59:01,331 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:01,348 - vllm.entrypoints.logger - INFO - Received request cmpl-3bfbf59bb5a74e8d862de60665949f15-0: prompt: "Doctor, I have been really busy lately. I've been trying to find a new tenant for my apartment complex.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 614, 1012, 2167, 13028, 30345, 13, 358, 3003, 1012, 4460, 311, 1477, 264, 501, 25239, 369, 847, 13154, 6351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,375 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3bfbf59bb5a74e8d862de60665949f15-0.
2025-12-19 23:59:01,376 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:01,415 - vllm.entrypoints.logger - INFO - Received request cmpl-967444f172c64bf9a320a42999083acf-0: prompt: "I'm really excited about my new business opportunity, B. I plan on investing my money in stocks and real estate ventures, and I'm hoping to make a fortune in the process.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 12035, 911, 847, 501, 2562, 6638, 11, 425, 13, 358, 3119, 389, 24965, 847, 3220, 304, 22488, 323, 1931, 12394, 65624, 11, 323, 358, 2776, 15652, 311, 1281, 264, 32315, 304, 279, 1882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,418 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-967444f172c64bf9a320a42999083acf-0.
2025-12-19 23:59:01,420 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:01,428 - vllm.entrypoints.logger - INFO - Received request cmpl-d670423857e543069f406e72df71dd2f-0: prompt: " Doctor, I don't know how to explain it, but something feels off. It's like I'm being watched or followed.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18635, 11, 358, 1513, 944, 1414, 1246, 311, 10339, 432, 11, 714, 2494, 11074, 1007, 13, 1084, 594, 1075, 358, 2776, 1660, 15384, 476, 8110, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,440 - vllm.entrypoints.logger - INFO - Received request cmpl-4d9ff33b07ce4577bc05f0af1f7c9fb0-0: prompt: "Hey, B! What's up?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 0, 3555, 594, 705, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,467 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d670423857e543069f406e72df71dd2f-0.
2025-12-19 23:59:01,468 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d9ff33b07ce4577bc05f0af1f7c9fb0-0.
2025-12-19 23:59:01,469 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:01,473 - vllm.entrypoints.logger - INFO - Received request cmpl-e671879ac4b14069a4849d6f189c78e6-0: prompt: "I'm so glad I sent that handwritten letter today. It felt good to put in the extra effort.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 15713, 358, 3208, 429, 98389, 6524, 3351, 13, 1084, 6476, 1661, 311, 2182, 304, 279, 4960, 5041, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,491 - vllm.entrypoints.logger - INFO - Received request cmpl-7b4fae8c055e4ab5ad98f744d27180a8-0: prompt: "I need to tell you something, and it's not going to be easy.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1184, 311, 3291, 498, 2494, 11, 323, 432, 594, 537, 2087, 311, 387, 4135, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,496 - vllm.entrypoints.logger - INFO - Received request cmpl-7239d1732b8d4650aab780323cd15005-0: prompt: "Doctor, it's good to see you again. I just came from a routine checkup and everything came back normal. I feel really good about my health.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=31, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 432, 594, 1661, 311, 1490, 498, 1549, 13, 358, 1101, 3697, 504, 264, 14021, 1779, 454, 323, 4297, 3697, 1182, 4622, 13, 358, 2666, 2167, 1661, 911, 847, 2820, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,505 - vllm.entrypoints.logger - INFO - Received request cmpl-2685ccfc328145e6b908aaaf391f1b8c-0: prompt: 'Ow! I just spilled hot tea all over my hand! This burns so much and now I have to deal with this painful injury. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [46, 86, 0, 358, 1101, 73025, 4017, 15243, 678, 916, 847, 1424, 0, 1096, 43054, 773, 1753, 323, 1431, 358, 614, 311, 3484, 448, 419, 25118, 10895, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,516 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e671879ac4b14069a4849d6f189c78e6-0.
2025-12-19 23:59:01,517 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7b4fae8c055e4ab5ad98f744d27180a8-0.
2025-12-19 23:59:01,518 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7239d1732b8d4650aab780323cd15005-0.
2025-12-19 23:59:01,520 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2685ccfc328145e6b908aaaf391f1b8c-0.
2025-12-19 23:59:01,521 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:01,542 - vllm.entrypoints.logger - INFO - Received request cmpl-eb39093d14504d09bd2aaf0dd5afe50d-0: prompt: "I really think I can make a difference in our school if I'm elected class president. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 1744, 358, 646, 1281, 264, 6672, 304, 1039, 2906, 421, 358, 2776, 16290, 536, 4767, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,550 - vllm.entrypoints.logger - INFO - Received request cmpl-db186b321bae4f62974577ec37736881-0: prompt: 'Hey, can we talk about what happened the other day?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 582, 3061, 911, 1128, 6932, 279, 1008, 1899, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,565 - vllm.entrypoints.logger - INFO - Received request cmpl-0d79e18bd6db45b0b821421298f77ef9-0: prompt: "I've been really focused on saving my money lately. I want to buy a house and a car, and also travel the world before I retire.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 2167, 10735, 389, 13997, 847, 3220, 30345, 13, 358, 1366, 311, 3695, 264, 3753, 323, 264, 1803, 11, 323, 1083, 5821, 279, 1879, 1573, 358, 15800, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,569 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eb39093d14504d09bd2aaf0dd5afe50d-0.
2025-12-19 23:59:01,570 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-db186b321bae4f62974577ec37736881-0.
2025-12-19 23:59:01,571 - vllm.entrypoints.logger - INFO - Received request cmpl-0d98358cf4bc453eb176812e75d7f340-0: prompt: 'Hey there, I love your shirt! That color looks great on you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 358, 2948, 697, 15478, 0, 2938, 1894, 5868, 2244, 389, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,571 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d79e18bd6db45b0b821421298f77ef9-0.
2025-12-19 23:59:01,573 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:01,587 - vllm.entrypoints.logger - INFO - Received request cmpl-e1883c69731e46aeb20a55a933d561e8-0: prompt: 'Mentor, do you remember when we used to hang out a lot and I became friends with your friends?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 653, 498, 6099, 979, 582, 1483, 311, 14678, 700, 264, 2696, 323, 358, 6116, 4780, 448, 697, 4780, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,595 - vllm.entrypoints.logger - INFO - Received request cmpl-137d271844ae446e8c25778583d1a0f7-0: prompt: "Wow, I can't believe how many new things there are to see here!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 358, 646, 944, 4411, 1246, 1657, 501, 2513, 1052, 525, 311, 1490, 1588, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,605 - vllm.entrypoints.logger - INFO - Received request cmpl-f26fa2c6ce3141eca3b9581f8dc28c67-0: prompt: 'Thank you for the compliment earlier, it means a lot coming from you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13060, 498, 369, 279, 31141, 6788, 11, 432, 3363, 264, 2696, 5001, 504, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,619 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d98358cf4bc453eb176812e75d7f340-0.
2025-12-19 23:59:01,620 - vllm.entrypoints.logger - INFO - Received request cmpl-2a44928420934a77be6f2a0eda70a375-0: prompt: "Coach, I'm feeling really upset right now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 2776, 8266, 2167, 22459, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,620 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e1883c69731e46aeb20a55a933d561e8-0.
2025-12-19 23:59:01,622 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-137d271844ae446e8c25778583d1a0f7-0.
2025-12-19 23:59:01,623 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f26fa2c6ce3141eca3b9581f8dc28c67-0.
2025-12-19 23:59:01,624 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2a44928420934a77be6f2a0eda70a375-0.
2025-12-19 23:59:01,625 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:01,654 - vllm.entrypoints.logger - INFO - Received request cmpl-0f00e5f482304b13b29703c04b7b9868-0: prompt: 'I went to the amusement park over the weekend and rode the coaster! It was so much fun.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3937, 311, 279, 61368, 6118, 916, 279, 9001, 323, 40661, 279, 81068, 0, 1084, 572, 773, 1753, 2464, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,664 - vllm.entrypoints.logger - INFO - Received request cmpl-b3b92c9c74464863877e835aa355b294-0: prompt: ' I felt so happy when I finally saw my program working smoothly. It took a lot of effort and time, but it was worth it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 6476, 773, 6247, 979, 358, 5499, 5485, 847, 2025, 3238, 38411, 13, 1084, 3867, 264, 2696, 315, 5041, 323, 882, 11, 714, 432, 572, 5802, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,667 - vllm.entrypoints.logger - INFO - Received request cmpl-fcfe3574d60c4efa885b26e73672ac37-0: prompt: 'I had a photoshoot yesterday for my modeling portfolio. It was so exciting!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 264, 7249, 90321, 13671, 369, 847, 33479, 19565, 13, 1084, 572, 773, 13245, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,672 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0f00e5f482304b13b29703c04b7b9868-0.
2025-12-19 23:59:01,673 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b3b92c9c74464863877e835aa355b294-0.
2025-12-19 23:59:01,674 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fcfe3574d60c4efa885b26e73672ac37-0.
2025-12-19 23:59:01,674 - vllm.entrypoints.logger - INFO - Received request cmpl-d2d3ef01adbf429b8234d410f38a7dc5-0: prompt: 'Hey, I wanted to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,675 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:01,697 - vllm.entrypoints.logger - INFO - Received request cmpl-5c8d7ab96aa841c0a32b1d0ed4b9a40b-0: prompt: "Mentor, I just wanted to thank you again for teaching me about love. It's made such a difference in my life.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1101, 4829, 311, 9702, 498, 1549, 369, 12629, 752, 911, 2948, 13, 1084, 594, 1865, 1741, 264, 6672, 304, 847, 2272, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,719 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d2d3ef01adbf429b8234d410f38a7dc5-0.
2025-12-19 23:59:01,720 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5c8d7ab96aa841c0a32b1d0ed4b9a40b-0.
2025-12-19 23:59:01,722 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:01,738 - vllm.entrypoints.logger - INFO - Received request cmpl-479d447d90c34a7fb0f28caa36f520ff-0: prompt: 'Hey, have you heard about my recent flight across the Atlantic in a small airplane?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 911, 847, 3213, 10971, 3941, 279, 22375, 304, 264, 2613, 42924, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,766 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-479d447d90c34a7fb0f28caa36f520ff-0.
2025-12-19 23:59:01,767 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:01,769 - vllm.entrypoints.logger - INFO - Received request cmpl-dccdccee366d4605a84848268fa52f00-0: prompt: 'Did you hear about the crime in the park near our house?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 6723, 911, 279, 9778, 304, 279, 6118, 3143, 1039, 3753, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,778 - vllm.entrypoints.logger - INFO - Received request cmpl-57294d9cd15943cb80746f730368d6f3-0: prompt: 'You know, I am really enjoying my work as a medical examiner.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 1079, 2167, 21413, 847, 975, 438, 264, 6457, 83632, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,811 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dccdccee366d4605a84848268fa52f00-0.
2025-12-19 23:59:01,813 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-57294d9cd15943cb80746f730368d6f3-0.
2025-12-19 23:59:01,814 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:01,816 - vllm.entrypoints.logger - INFO - Received request cmpl-6ffab10dc4f4462cbde48fb4844a47c1-0: prompt: "I can't believe I got punished for not putting away my toys. It's so embarrassing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 2684, 40898, 369, 537, 10687, 3123, 847, 23069, 13, 1084, 594, 773, 44005, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,818 - vllm.entrypoints.logger - INFO - Received request cmpl-fee254575c29468c953b02f5a8241201-0: prompt: ' I did something really brave yesterday.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1521, 2494, 2167, 33200, 13671, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,822 - vllm.entrypoints.logger - INFO - Received request cmpl-612ae64655c1412cb969e53b30dbdc84-0: prompt: 'Good morning, Boss.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 31569, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,826 - vllm.entrypoints.logger - INFO - Received request cmpl-14a989c254794382806e96aadcfe44b3-0: prompt: "This sunset is absolutely beautiful. It's amazing how colorful the sky is right now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 42984, 374, 10875, 6233, 13, 1084, 594, 7897, 1246, 33866, 279, 12884, 374, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,834 - vllm.entrypoints.logger - INFO - Received request cmpl-5bd1de092dbc4de1adfbe7eba8836560-0: prompt: 'Hey there, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,841 - vllm.entrypoints.logger - INFO - Received request cmpl-e8d5f33f842e4ef4993f0069b08530a5-0: prompt: 'I tried to hurry and finish the project so I could leave early today, but then I realized I made a mistake.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 6679, 311, 47235, 323, 6248, 279, 2390, 773, 358, 1410, 5274, 4124, 3351, 11, 714, 1221, 358, 15043, 358, 1865, 264, 16523, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,845 - vllm.entrypoints.logger - INFO - Received request cmpl-e4536eac309e473daaf52efc64835a79-0: prompt: 'Hey, have you noticed anything different about me lately?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 13686, 4113, 2155, 911, 752, 30345, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,853 - vllm.entrypoints.logger - INFO - Received request cmpl-b282d674e32447e4beecf8799704202b-0: prompt: 'Can you believe it? I got the lead role in the Broadway play!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 2684, 279, 2990, 3476, 304, 279, 36676, 1486, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,864 - vllm.entrypoints.logger - INFO - Received request cmpl-d3f65ba630dc4d17aace7b2857b413b3-0: prompt: 'Hey Mentor, can we talk for a bit?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 646, 582, 3061, 369, 264, 2699, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,866 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6ffab10dc4f4462cbde48fb4844a47c1-0.
2025-12-19 23:59:01,867 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fee254575c29468c953b02f5a8241201-0.
2025-12-19 23:59:01,868 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-612ae64655c1412cb969e53b30dbdc84-0.
2025-12-19 23:59:01,870 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-14a989c254794382806e96aadcfe44b3-0.
2025-12-19 23:59:01,869 - vllm.entrypoints.logger - INFO - Received request cmpl-ca5a58bcf7d5479e801d22becb49cadf-0: prompt: ' I just got back from a walk on the beach.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1101, 2684, 1182, 504, 264, 4227, 389, 279, 11321, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,871 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5bd1de092dbc4de1adfbe7eba8836560-0.
2025-12-19 23:59:01,872 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e8d5f33f842e4ef4993f0069b08530a5-0.
2025-12-19 23:59:01,873 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e4536eac309e473daaf52efc64835a79-0.
2025-12-19 23:59:01,874 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b282d674e32447e4beecf8799704202b-0.
2025-12-19 23:59:01,875 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d3f65ba630dc4d17aace7b2857b413b3-0.
2025-12-19 23:59:01,876 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ca5a58bcf7d5479e801d22becb49cadf-0.
2025-12-19 23:59:01,877 - vllm.core.scheduler - INFO - Pending queue size: (10)
2025-12-19 23:59:01,907 - vllm.entrypoints.logger - INFO - Received request cmpl-d6c0bc14352d40aea5521f9b0950d71c-0: prompt: 'Hi there! Have you read this book before?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 0, 12243, 498, 1349, 419, 2311, 1573, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,914 - vllm.entrypoints.logger - INFO - Received request cmpl-a10e3d882e04471e9269c17641be65c5-0: prompt: 'Hey, do you have a minute?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 614, 264, 9383, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,923 - vllm.entrypoints.logger - INFO - Received request cmpl-1e7891d5ab494754b83252cb438a3617-0: prompt: 'I have a confession. I did something terrible.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 264, 57373, 13, 358, 1521, 2494, 17478, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,924 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d6c0bc14352d40aea5521f9b0950d71c-0.
2025-12-19 23:59:01,925 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a10e3d882e04471e9269c17641be65c5-0.
2025-12-19 23:59:01,926 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1e7891d5ab494754b83252cb438a3617-0.
2025-12-19 23:59:01,928 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:01,932 - vllm.entrypoints.logger - INFO - Received request cmpl-328e345f9ba34bcb8106cf94b8f871a0-0: prompt: "This heat is killing me. I can't wait to jump into the pool.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 8628, 374, 13118, 752, 13, 358, 646, 944, 3783, 311, 7784, 1119, 279, 7314, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,934 - vllm.entrypoints.logger - INFO - Received request cmpl-ba5af20545cb4d08836528ddf45ff9e5-0: prompt: "I just can't believe how angry I'm feeling right now. I'm clenching my fists and gritting my teeth.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 4411, 1246, 18514, 358, 2776, 8266, 1290, 1431, 13, 358, 2776, 1185, 19762, 287, 847, 80684, 323, 40918, 1280, 847, 17832, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,949 - vllm.entrypoints.logger - INFO - Received request cmpl-41f3fef589364fa5a93474a12900fa23-0: prompt: "Winning the lottery has been such a blessing! I can't believe how much my life has changed.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [16970, 1229, 279, 38239, 702, 1012, 1741, 264, 39049, 0, 358, 646, 944, 4411, 1246, 1753, 847, 2272, 702, 5497, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,950 - vllm.entrypoints.logger - INFO - Received request cmpl-23f749f02ec8405d919a8f4ec19e416b-0: prompt: 'Mentor, I had the craziest dream just now.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1030, 279, 45041, 8345, 477, 7904, 1101, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,954 - vllm.entrypoints.logger - INFO - Received request cmpl-e05e632799c9429a89bfd6e79ad09a19-0: prompt: "Hey neighbor, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:01,976 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-328e345f9ba34bcb8106cf94b8f871a0-0.
2025-12-19 23:59:01,977 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ba5af20545cb4d08836528ddf45ff9e5-0.
2025-12-19 23:59:01,978 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-41f3fef589364fa5a93474a12900fa23-0.
2025-12-19 23:59:01,980 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-23f749f02ec8405d919a8f4ec19e416b-0.
2025-12-19 23:59:01,981 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e05e632799c9429a89bfd6e79ad09a19-0.
2025-12-19 23:59:01,982 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:01,983 - vllm.entrypoints.logger - INFO - Received request cmpl-801efa8a32124d9bbaccb36bbfc3f75e-0: prompt: "I don't know, Neighbors B. I feel like we don't have anything in common to talk about.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 11, 4182, 24101, 425, 13, 358, 2666, 1075, 582, 1513, 944, 614, 4113, 304, 4185, 311, 3061, 911, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,010 - vllm.entrypoints.logger - INFO - Received request cmpl-2cb7ce2cf5af438ba8644024b7d255da-0: prompt: 'I finally folded all my clothes and put them away in my dresser.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 5499, 47035, 678, 847, 15097, 323, 2182, 1105, 3123, 304, 847, 95537, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,029 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-801efa8a32124d9bbaccb36bbfc3f75e-0.
2025-12-19 23:59:02,030 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2cb7ce2cf5af438ba8644024b7d255da-0.
2025-12-19 23:59:02,032 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:02,064 - vllm.entrypoints.logger - INFO - Received request cmpl-db177255436045339322d3df4fd39343-0: prompt: 'Hey, guess what I found in the library yesterday?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 7942, 1128, 358, 1730, 304, 279, 6733, 13671, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,066 - vllm.entrypoints.logger - INFO - Received request cmpl-e95f103257924075bc7f5ed4bcc7d720-0: prompt: 'Doctor, I want to talk to you about something important.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 1366, 311, 3061, 311, 498, 911, 2494, 2989, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,069 - vllm.entrypoints.logger - INFO - Received request cmpl-b3eae21c9ee24522bcde1ffafd5bf415-0: prompt: 'Coach, I have a question. Do you think sending gifts to the shop is a good idea?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 614, 264, 3405, 13, 3155, 498, 1744, 11628, 20609, 311, 279, 8061, 374, 264, 1661, 4522, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,078 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-db177255436045339322d3df4fd39343-0.
2025-12-19 23:59:02,079 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e95f103257924075bc7f5ed4bcc7d720-0.
2025-12-19 23:59:02,080 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b3eae21c9ee24522bcde1ffafd5bf415-0.
2025-12-19 23:59:02,081 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:02,097 - vllm.entrypoints.logger - INFO - Received request cmpl-a2b022dcc882421a8af6165239838292-0: prompt: 'Good morning, Teacher.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 29069, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,105 - vllm.entrypoints.logger - INFO - Received request cmpl-44597e096105454eaed689ab92ff6754-0: prompt: "Hey Mentor, I just wanted to check in and see how you're doing. You've seemed a little distant lately and I'm worried about you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 1101, 4829, 311, 1779, 304, 323, 1490, 1246, 498, 2299, 3730, 13, 1446, 3003, 9324, 264, 2632, 28727, 30345, 323, 358, 2776, 17811, 911, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,109 - vllm.entrypoints.logger - INFO - Received request cmpl-c10c4a5b0cc241eaafa15619f9236712-0: prompt: 'Hey, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,120 - vllm.entrypoints.logger - INFO - Received request cmpl-4d78a3ab0bcc483b92ac0eaab1b07333-0: prompt: "I can't wait to spend some quality time with you today, my love.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 311, 8329, 1045, 4271, 882, 448, 498, 3351, 11, 847, 2948, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,123 - vllm.entrypoints.logger - INFO - Received request cmpl-d8b28277c8c2451f9c034658dd749213-0: prompt: "I've been going over these notes that I found again and I still can't figure out who they belonged to.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 2087, 916, 1493, 8388, 429, 358, 1730, 1549, 323, 358, 2058, 646, 944, 7071, 700, 879, 807, 45859, 311, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,131 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a2b022dcc882421a8af6165239838292-0.
2025-12-19 23:59:02,132 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-44597e096105454eaed689ab92ff6754-0.
2025-12-19 23:59:02,133 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c10c4a5b0cc241eaafa15619f9236712-0.
2025-12-19 23:59:02,134 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d78a3ab0bcc483b92ac0eaab1b07333-0.
2025-12-19 23:59:02,135 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d8b28277c8c2451f9c034658dd749213-0.
2025-12-19 23:59:02,136 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:02,149 - vllm.entrypoints.logger - INFO - Received request cmpl-7a3c2ea517734a58be0b14a940dc1c0f-0: prompt: 'It was amazing being out on the ocean, the sunset was so beautiful.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 572, 7897, 1660, 700, 389, 279, 17951, 11, 279, 42984, 572, 773, 6233, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,154 - vllm.entrypoints.logger - INFO - Received request cmpl-f5633fcafe69492bb4e22fbe3eaad43b-0: prompt: "Hey there Doctor, I'm glad to see you made it to the party.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 18635, 11, 358, 2776, 15713, 311, 1490, 498, 1865, 432, 311, 279, 4614, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,182 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7a3c2ea517734a58be0b14a940dc1c0f-0.
2025-12-19 23:59:02,183 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f5633fcafe69492bb4e22fbe3eaad43b-0.
2025-12-19 23:59:02,185 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:02,219 - vllm.entrypoints.logger - INFO - Received request cmpl-f3b87f541aae41e697a94137cba98f4b-0: prompt: 'Coach, I had a great day yesterday! I spent the afternoon giving out free hugs to anyone who wanted one.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 1030, 264, 2244, 1899, 13671, 0, 358, 7391, 279, 13354, 7086, 700, 1910, 79242, 311, 5489, 879, 4829, 825, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,228 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f3b87f541aae41e697a94137cba98f4b-0.
2025-12-19 23:59:02,230 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:02,241 - vllm.entrypoints.logger - INFO - Received request cmpl-1fdee2678fd2434f935cdf07d9c1f006-0: prompt: "I've been thinking a lot lately about climate change and what we can do to make a difference.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 7274, 264, 2696, 30345, 911, 9977, 2297, 323, 1128, 582, 646, 653, 311, 1281, 264, 6672, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,273 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1fdee2678fd2434f935cdf07d9c1f006-0.
2025-12-19 23:59:02,275 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:02,276 - vllm.entrypoints.logger - INFO - Received request cmpl-1eeae06617af414086c57c866cd21c6a-0: prompt: "Doctor, I just wanted to share some good news with you. I've really moved on from the past and I'm feeling much happier now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 1101, 4829, 311, 4332, 1045, 1661, 3669, 448, 498, 13, 358, 3003, 2167, 7726, 389, 504, 279, 3267, 323, 358, 2776, 8266, 1753, 43367, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,313 - vllm.entrypoints.logger - INFO - Received request cmpl-54a3c36706f8426990b76a146fc1caa8-0: prompt: "Doctor, I've been feeling really anxious lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 3003, 1012, 8266, 2167, 37000, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,319 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1eeae06617af414086c57c866cd21c6a-0.
2025-12-19 23:59:02,320 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-54a3c36706f8426990b76a146fc1caa8-0.
2025-12-19 23:59:02,322 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:02,350 - vllm.entrypoints.logger - INFO - Received request cmpl-d909d34329564814b850b0b8f36f18ed-0: prompt: "I think we should go outside and explore today. There are so many new places we haven't seen yet.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1744, 582, 1265, 728, 4889, 323, 13186, 3351, 13, 2619, 525, 773, 1657, 501, 7482, 582, 8990, 944, 3884, 3602, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,366 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d909d34329564814b850b0b8f36f18ed-0.
2025-12-19 23:59:02,367 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:02,408 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:02,453 - vllm.entrypoints.logger - INFO - Received request cmpl-53425911275e4ce6982ce004dc02ee26-0: prompt: "Sorry I'm late again. I was caught up in some work.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [19152, 358, 2776, 3309, 1549, 13, 358, 572, 10568, 705, 304, 1045, 975, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,468 - vllm.entrypoints.logger - INFO - Received request cmpl-d14750cb218c4e6e83ad9d80f010cbe4-0: prompt: "Mentor, I just want to say how proud I am of you and everything you've accomplished in your career. I've been your biggest fan since the beginning and I've loved watching you achieve all of your goals.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=35, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1101, 1366, 311, 1977, 1246, 12409, 358, 1079, 315, 498, 323, 4297, 498, 3003, 26237, 304, 697, 6931, 13, 358, 3003, 1012, 697, 8538, 8405, 2474, 279, 7167, 323, 358, 3003, 10245, 10099, 498, 11075, 678, 315, 697, 8845, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,484 - vllm.entrypoints.logger - INFO - Received request cmpl-267daf0da50b4520a5f1662761b8b430-0: prompt: "Hey, kiddo! Guess what? I talked to John, and he's interested in helping me build a nuclear fusion reactor!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 91415, 78, 0, 54279, 1128, 30, 358, 14897, 311, 3757, 11, 323, 566, 594, 8014, 304, 10476, 752, 1936, 264, 11249, 36508, 37629, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,496 - vllm.entrypoints.logger - INFO - Received request cmpl-51808c54fdbf459fa00363dd7dc75e3e-0: prompt: "Mentor, I just wanted to run something by you. I've been spending more time with your children lately and I realized how much I care for them. I feel like a protective father figure to them.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=37, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1101, 4829, 311, 1598, 2494, 553, 498, 13, 358, 3003, 1012, 10164, 803, 882, 448, 697, 2841, 30345, 323, 358, 15043, 1246, 1753, 358, 2453, 369, 1105, 13, 358, 2666, 1075, 264, 28119, 6981, 7071, 311, 1105, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,505 - vllm.entrypoints.logger - INFO - Received request cmpl-d9ea3bb862bf43989b348550ce8e6f8d-0: prompt: 'I pray every day, asking God to help me be a better person. I try to live according to His will, and dedicate my life to serving Him.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 23803, 1449, 1899, 11, 10161, 4264, 311, 1492, 752, 387, 264, 2664, 1697, 13, 358, 1430, 311, 3887, 4092, 311, 5301, 686, 11, 323, 73604, 847, 2272, 311, 13480, 20426, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,525 - vllm.entrypoints.logger - INFO - Received request cmpl-977448ed13c4479b838b1f81b2aa9e75-0: prompt: 'Hey there, I have some exciting news to share with you. I finally bought a house and took out a mortgage.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 358, 614, 1045, 13245, 3669, 311, 4332, 448, 498, 13, 358, 5499, 10788, 264, 3753, 323, 3867, 700, 264, 20846, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,548 - vllm.entrypoints.logger - INFO - Received request cmpl-f50b73ac9a9741339b47868f5bad1569-0: prompt: 'Hey, I was expecting to meet you at the library. I wanted to talk to you about our joint project.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 572, 22331, 311, 3367, 498, 518, 279, 6733, 13, 358, 4829, 311, 3061, 311, 498, 911, 1039, 10284, 2390, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,573 - vllm.entrypoints.logger - INFO - Received request cmpl-c641d874821b4a27a82c720bc3153983-0: prompt: "I've been thinking a lot about the changes I want to make to the news network since buying it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 7274, 264, 2696, 911, 279, 4344, 358, 1366, 311, 1281, 311, 279, 3669, 3922, 2474, 11833, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,587 - vllm.entrypoints.logger - INFO - Received request cmpl-645de0578a994bf1a771a355f55db6f1-0: prompt: " I've been thinking about therapy lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 3003, 1012, 7274, 911, 15069, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,606 - vllm.entrypoints.logger - INFO - Received request cmpl-55e0f5a256d647c68c796f5777c07a84-0: prompt: "I've been feeling so frustrated lately. Nothing seems to be going my way.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 8266, 773, 32530, 30345, 13, 12064, 4977, 311, 387, 2087, 847, 1616, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,611 - vllm.entrypoints.logger - INFO - Received request cmpl-07cec5692612410b9dd78c2121eb0fc4-0: prompt: "Alright, I'm going to give it a shot and clean the garage today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [71486, 11, 358, 2776, 2087, 311, 2968, 432, 264, 6552, 323, 4240, 279, 19277, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,635 - vllm.entrypoints.logger - INFO - Received request cmpl-97f6875c11cf4d429441d2132df24eb6-0: prompt: "I still can't believe I didn't get the job. They said I wasn't competent enough.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2058, 646, 944, 4411, 358, 3207, 944, 633, 279, 2618, 13, 2379, 1053, 358, 5710, 944, 39783, 3322, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,669 - vllm.entrypoints.logger - INFO - Received request cmpl-2b238eb338bc4599bff37fd6128c319b-0: prompt: "I really messed up, Mentor. I started a flame war on an online forum and it got out of control. I don't know why I did it. I was just really bored and wanted to stir up some trouble.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 64202, 705, 11, 91191, 13, 358, 3855, 264, 34578, 4116, 389, 458, 2860, 11848, 323, 432, 2684, 700, 315, 2524, 13, 358, 1513, 944, 1414, 3170, 358, 1521, 432, 13, 358, 572, 1101, 2167, 33286, 323, 4829, 311, 23065, 705, 1045, 12264, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,690 - vllm.entrypoints.logger - INFO - Received request cmpl-939ff4e22ebd47d9b072b6d96ec239d2-0: prompt: 'Whew, that was close. I almost missed the bus today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1639, 365, 11, 429, 572, 3265, 13, 358, 4558, 13628, 279, 5828, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,734 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-53425911275e4ce6982ce004dc02ee26-0.
2025-12-19 23:59:02,736 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d14750cb218c4e6e83ad9d80f010cbe4-0.
2025-12-19 23:59:02,737 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-267daf0da50b4520a5f1662761b8b430-0.
2025-12-19 23:59:02,738 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-51808c54fdbf459fa00363dd7dc75e3e-0.
2025-12-19 23:59:02,739 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d9ea3bb862bf43989b348550ce8e6f8d-0.
2025-12-19 23:59:02,740 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-977448ed13c4479b838b1f81b2aa9e75-0.
2025-12-19 23:59:02,742 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f50b73ac9a9741339b47868f5bad1569-0.
2025-12-19 23:59:02,743 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c641d874821b4a27a82c720bc3153983-0.
2025-12-19 23:59:02,744 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-645de0578a994bf1a771a355f55db6f1-0.
2025-12-19 23:59:02,745 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-55e0f5a256d647c68c796f5777c07a84-0.
2025-12-19 23:59:02,746 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-07cec5692612410b9dd78c2121eb0fc4-0.
2025-12-19 23:59:02,747 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-97f6875c11cf4d429441d2132df24eb6-0.
2025-12-19 23:59:02,747 - vllm.entrypoints.logger - INFO - Received request cmpl-396e57a8d5eb494093933648a7a00f37-0: prompt: "Mentor, I've been going to karate classes for a few months now and I absolutely love it! ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 3003, 1012, 2087, 311, 23425, 349, 6846, 369, 264, 2421, 3951, 1431, 323, 358, 10875, 2948, 432, 0, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,748 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2b238eb338bc4599bff37fd6128c319b-0.
2025-12-19 23:59:02,749 - vllm.entrypoints.logger - INFO - Received request cmpl-6bcc5d9d23f944b2960b8a520348ed5c-0: prompt: 'Hey, have you seen the banner on the side of my house?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3884, 279, 23323, 389, 279, 3108, 315, 847, 3753, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,749 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-939ff4e22ebd47d9b072b6d96ec239d2-0.
2025-12-19 23:59:02,750 - vllm.core.scheduler - INFO - Pending queue size: (14)
2025-12-19 23:59:02,761 - vllm.entrypoints.logger - INFO - Received request cmpl-9f7d243bfd5049559a05379eccf3f357-0: prompt: "Hey, have you heard the news? My parents are getting divorced, and I'll be moving into my mom's new house soon.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 279, 3669, 30, 3017, 6562, 525, 3709, 49492, 11, 323, 358, 3278, 387, 7218, 1119, 847, 3368, 594, 501, 3753, 5135, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,774 - vllm.entrypoints.logger - INFO - Received request cmpl-078b1664f70843498e3c7c69279664d5-0: prompt: " I didn't get the promotion I wanted.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 3207, 944, 633, 279, 20249, 358, 4829, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,788 - vllm.entrypoints.logger - INFO - Received request cmpl-9b664e8d33da4ea291b8ed4c63570c9a-0: prompt: " Doctor, I'm in a bit of a predicament. I got locked in the bathroom and the door won't budge. I'm starting to panic and I feel like I'm going to be sick.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=37, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18635, 11, 358, 2776, 304, 264, 2699, 315, 264, 78334, 2838, 13, 358, 2684, 16061, 304, 279, 14852, 323, 279, 6006, 2765, 944, 293, 19561, 13, 358, 2776, 5916, 311, 21975, 323, 358, 2666, 1075, 358, 2776, 2087, 311, 387, 14036, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,793 - vllm.entrypoints.logger - INFO - Received request cmpl-1e50c6b3b9844bbdbccb930571111b4c-0: prompt: 'I had a really uncomfortable conversation with my friend the other day.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 264, 2167, 28113, 10435, 448, 847, 4238, 279, 1008, 1899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,803 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-396e57a8d5eb494093933648a7a00f37-0.
2025-12-19 23:59:02,804 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6bcc5d9d23f944b2960b8a520348ed5c-0.
2025-12-19 23:59:02,805 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9f7d243bfd5049559a05379eccf3f357-0.
2025-12-19 23:59:02,806 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-078b1664f70843498e3c7c69279664d5-0.
2025-12-19 23:59:02,807 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9b664e8d33da4ea291b8ed4c63570c9a-0.
2025-12-19 23:59:02,808 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1e50c6b3b9844bbdbccb930571111b4c-0.
2025-12-19 23:59:02,810 - vllm.core.scheduler - INFO - Pending queue size: (6)
2025-12-19 23:59:02,817 - vllm.entrypoints.logger - INFO - Received request cmpl-16fb56464a334f1a843c6a7f15252adb-0: prompt: "Hey, thanks for coming to the library with me. I'm really excited to spend some time here with you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 9339, 369, 5001, 311, 279, 6733, 448, 752, 13, 358, 2776, 2167, 12035, 311, 8329, 1045, 882, 1588, 448, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,836 - vllm.entrypoints.logger - INFO - Received request cmpl-f1915f012ff94a90b1946ba35409e7a0-0: prompt: 'Hello Boss, I have something I would like to talk about. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9707, 31569, 11, 358, 614, 2494, 358, 1035, 1075, 311, 3061, 911, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,838 - vllm.entrypoints.logger - INFO - Received request cmpl-24d6b46fb6644f0d913ca55669dd2af0-0: prompt: "Doctor, I'm so ready for battle today! I've got my sword, my shield, and my armor.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 2776, 773, 5527, 369, 8049, 3351, 0, 358, 3003, 2684, 847, 20214, 11, 847, 20245, 11, 323, 847, 20033, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,841 - vllm.entrypoints.logger - INFO - Received request cmpl-a8bc408d66314bb68975d8d5bfee04d9-0: prompt: "Hey Co-worker B, check out my new piggy bank! It's so cute and I can't wait to fill it up with all my savings.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 3539, 65516, 425, 11, 1779, 700, 847, 501, 23694, 4577, 6073, 0, 1084, 594, 773, 18838, 323, 358, 646, 944, 3783, 311, 5155, 432, 705, 448, 678, 847, 18986, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,858 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-16fb56464a334f1a843c6a7f15252adb-0.
2025-12-19 23:59:02,860 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f1915f012ff94a90b1946ba35409e7a0-0.
2025-12-19 23:59:02,861 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-24d6b46fb6644f0d913ca55669dd2af0-0.
2025-12-19 23:59:02,862 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a8bc408d66314bb68975d8d5bfee04d9-0.
2025-12-19 23:59:02,863 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:02,888 - vllm.entrypoints.logger - INFO - Received request cmpl-821c037739f2469ab78a77bddc163a2a-0: prompt: ", I'm so excited! I finally got accepted into the college of my choice!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 773, 12035, 0, 358, 5499, 2684, 11666, 1119, 279, 7770, 315, 847, 5754, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,895 - vllm.entrypoints.logger - INFO - Received request cmpl-7d901599316945fba641a3857616e817-0: prompt: 'Hey, Classmates B. Can I talk to you for a minute?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 3228, 16457, 425, 13, 2980, 358, 3061, 311, 498, 369, 264, 9383, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,910 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-821c037739f2469ab78a77bddc163a2a-0.
2025-12-19 23:59:02,911 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7d901599316945fba641a3857616e817-0.
2025-12-19 23:59:02,913 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:02,919 - vllm.entrypoints.logger - INFO - Received request cmpl-4d2f77a490d3463ebdba49014b46a784-0: prompt: 'I feel so left out and lonely. I was excluded from the group chat and my friends are making plans for the weekend without me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 773, 2115, 700, 323, 39566, 13, 358, 572, 27444, 504, 279, 1874, 6236, 323, 847, 4780, 525, 3259, 6649, 369, 279, 9001, 2041, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:02,957 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d2f77a490d3463ebdba49014b46a784-0.
2025-12-19 23:59:02,958 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:02,972 - vllm.entrypoints.logger - INFO - Received request cmpl-c013dee2c5a94902a16508211f9e8d47-0: prompt: "Hi Mentor, it's me again. I wanted to call and tell you more about my trip.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 432, 594, 752, 1549, 13, 358, 4829, 311, 1618, 323, 3291, 498, 803, 911, 847, 8411, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,002 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c013dee2c5a94902a16508211f9e8d47-0.
2025-12-19 23:59:03,003 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:03,009 - vllm.entrypoints.logger - INFO - Received request cmpl-14ec77f924a7457eb341e0fdffab0588-0: prompt: 'Hi Mentor, I wanted to thank you again for all your help with my application. I just received an email saying that it was accepted!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 4829, 311, 9702, 498, 1549, 369, 678, 697, 1492, 448, 847, 3766, 13, 358, 1101, 3949, 458, 2551, 5488, 429, 432, 572, 11666, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,012 - vllm.entrypoints.logger - INFO - Received request cmpl-c5ed25dd92104f7da78496893d602962-0: prompt: 'Hey, did you know that today is Guy Fawkes Day?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 1414, 429, 3351, 374, 25273, 434, 672, 12556, 6059, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,037 - vllm.entrypoints.logger - INFO - Received request cmpl-1d13dc9e6f8345cea91475864b1c70f1-0: prompt: 'Mentor, I still cant believe I ran into that burning building to save you. It was the scariest thing Ive ever done.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 2058, 646, 1405, 4411, 358, 10613, 1119, 429, 19675, 4752, 311, 3581, 498, 13, 1084, 572, 279, 1136, 2780, 477, 3166, 358, 3982, 3512, 2814, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,039 - vllm.entrypoints.logger - INFO - Received request cmpl-8c26800beb534623adf8bc2ed8a52420-0: prompt: "I've been trying to be more mindful of my water intake lately. I usually try to drink eight cups a day, but if I'm feeling bloated I might cut it down to six or seven.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 4460, 311, 387, 803, 62342, 315, 847, 3015, 22881, 30345, 13, 358, 5990, 1430, 311, 7027, 8063, 25374, 264, 1899, 11, 714, 421, 358, 2776, 8266, 14211, 657, 358, 2578, 3931, 432, 1495, 311, 4743, 476, 8094, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,052 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-14ec77f924a7457eb341e0fdffab0588-0.
2025-12-19 23:59:03,054 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c5ed25dd92104f7da78496893d602962-0.
2025-12-19 23:59:03,055 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1d13dc9e6f8345cea91475864b1c70f1-0.
2025-12-19 23:59:03,056 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8c26800beb534623adf8bc2ed8a52420-0.
2025-12-19 23:59:03,057 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:03,072 - vllm.entrypoints.logger - INFO - Received request cmpl-73e12a719d954a9c94eb6da17d017b83-0: prompt: "Isn't it amazing how organized and efficient those leaf-cutter ants are?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [87941, 944, 432, 7897, 1246, 16645, 323, 11050, 1846, 15933, 1786, 6207, 60395, 525, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,081 - vllm.entrypoints.logger - INFO - Received request cmpl-08146afa996647b0a4b07e95b39f2cbe-0: prompt: "Mentor, I have some exciting news! I'm going on an expedition to explore the Amazon rainforest!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 1045, 13245, 3669, 0, 358, 2776, 2087, 389, 458, 50164, 311, 13186, 279, 8176, 11174, 50655, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,091 - vllm.entrypoints.logger - INFO - Received request cmpl-23e1fafc173144d2879dd557151e5167-0: prompt: "I think it's time for me to move on from just looking good and start doing something innovative with my life.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1744, 432, 594, 882, 369, 752, 311, 3271, 389, 504, 1101, 3330, 1661, 323, 1191, 3730, 2494, 18199, 448, 847, 2272, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,101 - vllm.entrypoints.logger - INFO - Received request cmpl-0ac936a6988f4a4bbbc09afdea4569c7-0: prompt: 'Hey Neighbors B, did you know that the world is round?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 4182, 24101, 425, 11, 1521, 498, 1414, 429, 279, 1879, 374, 4778, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,105 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-73e12a719d954a9c94eb6da17d017b83-0.
2025-12-19 23:59:03,106 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-08146afa996647b0a4b07e95b39f2cbe-0.
2025-12-19 23:59:03,107 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-23e1fafc173144d2879dd557151e5167-0.
2025-12-19 23:59:03,109 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0ac936a6988f4a4bbbc09afdea4569c7-0.
2025-12-19 23:59:03,110 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:03,115 - vllm.entrypoints.logger - INFO - Received request cmpl-2cd2a405d87143d5a3e2a14a0aa5c75d-0: prompt: 'Hey, did I tell you about the conversation I had with my friend the other day? She said something that made me laugh so hard.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 911, 279, 10435, 358, 1030, 448, 847, 4238, 279, 1008, 1899, 30, 2932, 1053, 2494, 429, 1865, 752, 12550, 773, 2588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,157 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2cd2a405d87143d5a3e2a14a0aa5c75d-0.
2025-12-19 23:59:03,158 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:03,168 - vllm.entrypoints.logger - INFO - Received request cmpl-9cefc18569d54b7e937ab02466d3ce48-0: prompt: "Mentor, I had to use a fire extinguisher yesterday to stop a fire. I saw smoke coming from the neighbor's house and I didn't want anyone to get hurt.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1030, 311, 990, 264, 3940, 55707, 38572, 13671, 311, 2936, 264, 3940, 13, 358, 5485, 16205, 5001, 504, 279, 9565, 594, 3753, 323, 358, 3207, 944, 1366, 5489, 311, 633, 12898, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,180 - vllm.entrypoints.logger - INFO - Received request cmpl-86fb6d6837e6445dba2ba56e6c7ff709-0: prompt: 'Mentor, I just got down on my knees and thanked God for all the blessings in my life. It made me feel so happy and grateful.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1101, 2684, 1495, 389, 847, 30524, 323, 56495, 4264, 369, 678, 279, 55550, 304, 847, 2272, 13, 1084, 1865, 752, 2666, 773, 6247, 323, 25195, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,203 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9cefc18569d54b7e937ab02466d3ce48-0.
2025-12-19 23:59:03,205 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-86fb6d6837e6445dba2ba56e6c7ff709-0.
2025-12-19 23:59:03,205 - vllm.entrypoints.logger - INFO - Received request cmpl-ed8074b76a2b4933b4dcf2985e94f092-0: prompt: "I've been looking over my employment contract and I think there are a few things that need to be changed.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 3330, 916, 847, 14402, 5116, 323, 358, 1744, 1052, 525, 264, 2421, 2513, 429, 1184, 311, 387, 5497, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,206 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:03,213 - vllm.entrypoints.logger - INFO - Received request cmpl-f412bdc978744f4eb0f6872b326c8575-0: prompt: "I can't believe he said that to me. I was so angry, I could feel the heat rising in my face.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 566, 1053, 429, 311, 752, 13, 358, 572, 773, 18514, 11, 358, 1410, 2666, 279, 8628, 16062, 304, 847, 3579, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,250 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ed8074b76a2b4933b4dcf2985e94f092-0.
2025-12-19 23:59:03,251 - vllm.entrypoints.logger - INFO - Received request cmpl-918a7c79c73f490d908c2bf85dd87b96-0: prompt: " Hi doctor, it's been a while since we've talked.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [21018, 10668, 11, 432, 594, 1012, 264, 1393, 2474, 582, 3003, 14897, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,252 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f412bdc978744f4eb0f6872b326c8575-0.
2025-12-19 23:59:03,253 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:03,296 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-918a7c79c73f490d908c2bf85dd87b96-0.
2025-12-19 23:59:03,298 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:03,329 - vllm.entrypoints.logger - INFO - Received request cmpl-0c79863a4a4e41c98b4cdf85184fcfaa-0: prompt: 'Thank you for noticing, Mentor! I have a special event to attend tonight.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13060, 498, 369, 61364, 11, 91191, 0, 358, 614, 264, 3281, 1538, 311, 9417, 17913, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,341 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0c79863a4a4e41c98b4cdf85184fcfaa-0.
2025-12-19 23:59:03,343 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:03,348 - vllm.entrypoints.logger - INFO - Received request cmpl-c8ba36c2f1da43e587bda64b0fa897ad-0: prompt: "I am so sorry about the lamp. I didn't mean to hit it with my ball.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1079, 773, 14589, 911, 279, 27962, 13, 358, 3207, 944, 3076, 311, 4201, 432, 448, 847, 4935, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,357 - vllm.entrypoints.logger - INFO - Received request cmpl-68a422f084294857825ed6d0cbf0609a-0: prompt: "We work well together, don't we?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1654, 975, 1632, 3786, 11, 1513, 944, 582, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,366 - vllm.entrypoints.logger - INFO - Received request cmpl-ced0f835f394452fa5c35991e8f2786e-0: prompt: "(sighs) I'm just waiting for a text from my friend. It's getting pretty late though, and I have to be up early for work tomorrow.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1141, 1090, 82, 8, 358, 2776, 1101, 8580, 369, 264, 1467, 504, 847, 4238, 13, 1084, 594, 3709, 5020, 3309, 3498, 11, 323, 358, 614, 311, 387, 705, 4124, 369, 975, 16577, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,378 - vllm.entrypoints.logger - INFO - Received request cmpl-7d45882ee8ca4d03a37ef46b7a7827c8-0: prompt: 'Hey, I need to talk to you about something important.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 2989, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,387 - vllm.entrypoints.logger - INFO - Received request cmpl-191612b0b9374dcc9422ee4fc0653509-0: prompt: "I have to stand up on my bus ride to work every day. It's always so crowded.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 311, 2498, 705, 389, 847, 5828, 11877, 311, 975, 1449, 1899, 13, 1084, 594, 2677, 773, 38213, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,389 - vllm.entrypoints.logger - INFO - Received request cmpl-e9c4881cc2a047f4bf61235bb52e6cc4-0: prompt: 'Have you tried the wine from the new winery down the road?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 6679, 279, 13078, 504, 279, 501, 3164, 722, 1495, 279, 5636, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,392 - vllm.engine.metrics - INFO - Avg prompt throughput: 772.1 tokens/s, Avg generation throughput: 184.7 tokens/s, Running: 179 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.
2025-12-19 23:59:03,394 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c8ba36c2f1da43e587bda64b0fa897ad-0.
2025-12-19 23:59:03,395 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-68a422f084294857825ed6d0cbf0609a-0.
2025-12-19 23:59:03,396 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ced0f835f394452fa5c35991e8f2786e-0.
2025-12-19 23:59:03,397 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7d45882ee8ca4d03a37ef46b7a7827c8-0.
2025-12-19 23:59:03,400 - vllm.entrypoints.logger - INFO - Received request cmpl-1dc3b2e954894c7fb3645621b32c8223-0: prompt: "I can't believe she said that to me! It was so rude and uncalled for.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1340, 1053, 429, 311, 752, 0, 1084, 572, 773, 46001, 323, 20815, 4736, 369, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,400 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-191612b0b9374dcc9422ee4fc0653509-0.
2025-12-19 23:59:03,401 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e9c4881cc2a047f4bf61235bb52e6cc4-0.
2025-12-19 23:59:03,402 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1dc3b2e954894c7fb3645621b32c8223-0.
2025-12-19 23:59:03,404 - vllm.core.scheduler - INFO - Pending queue size: (7)
2025-12-19 23:59:03,406 - vllm.entrypoints.logger - INFO - Received request cmpl-9702c0db478443cab442d3a773e59e59-0: prompt: " I can't believe what happened to me. I'm still in so much pain.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 646, 944, 4411, 1128, 6932, 311, 752, 13, 358, 2776, 2058, 304, 773, 1753, 6646, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,414 - vllm.entrypoints.logger - INFO - Received request cmpl-e71e7ab54c0d4b14a77f55f9c1cb1852-0: prompt: 'Whew, I am so glad I finished all my work for today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1639, 365, 11, 358, 1079, 773, 15713, 358, 8060, 678, 847, 975, 369, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,444 - vllm.entrypoints.logger - INFO - Received request cmpl-191578a3346f46c698a91c5919b55765-0: prompt: ' I found her pulse, thank goodness. I was so scared that she might not make it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1730, 1059, 27235, 11, 9702, 38426, 13, 358, 572, 773, 26115, 429, 1340, 2578, 537, 1281, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,447 - vllm.entrypoints.logger - INFO - Received request cmpl-b03f2a842204416eb593155e71fa3cfe-0: prompt: 'Hey Mentor, I have a really important task that I need your help with. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 614, 264, 2167, 2989, 3383, 429, 358, 1184, 697, 1492, 448, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,452 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9702c0db478443cab442d3a773e59e59-0.
2025-12-19 23:59:03,453 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e71e7ab54c0d4b14a77f55f9c1cb1852-0.
2025-12-19 23:59:03,454 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-191578a3346f46c698a91c5919b55765-0.
2025-12-19 23:59:03,455 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b03f2a842204416eb593155e71fa3cfe-0.
2025-12-19 23:59:03,456 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:03,458 - vllm.entrypoints.logger - INFO - Received request cmpl-0f6f2e94a561456facf2df063102e5fd-0: prompt: " I needed a change, so I packed up my things and left. I just couldn't stay in that town any longer.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 4362, 264, 2297, 11, 773, 358, 19375, 705, 847, 2513, 323, 2115, 13, 358, 1101, 7691, 944, 4717, 304, 429, 6290, 894, 5021, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,483 - vllm.entrypoints.logger - INFO - Received request cmpl-67975f1f978048c8af60bdc4658714c3-0: prompt: "I can't believe I didn't get my full tax refund. I was really counting on that money.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 3207, 944, 633, 847, 2480, 3742, 20965, 13, 358, 572, 2167, 25009, 389, 429, 3220, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,498 - vllm.entrypoints.logger - INFO - Received request cmpl-e3713bd1ecba4af38464f262f7367e95-0: prompt: 'Hey, do you remember that mystery I was telling you about yesterday?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 6099, 429, 22531, 358, 572, 11629, 498, 911, 13671, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,502 - vllm.entrypoints.logger - INFO - Received request cmpl-f588c9cf809a48e68de6e6d41835496f-0: prompt: 'Wow, your painting is incredible, Neighbors B. The use of light and dark colors really makes it come alive.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 697, 18824, 374, 15050, 11, 4182, 24101, 425, 13, 576, 990, 315, 3100, 323, 6319, 7987, 2167, 3643, 432, 2525, 13675, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,506 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0f6f2e94a561456facf2df063102e5fd-0.
2025-12-19 23:59:03,507 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-67975f1f978048c8af60bdc4658714c3-0.
2025-12-19 23:59:03,508 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e3713bd1ecba4af38464f262f7367e95-0.
2025-12-19 23:59:03,509 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f588c9cf809a48e68de6e6d41835496f-0.
2025-12-19 23:59:03,510 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:03,537 - vllm.entrypoints.logger - INFO - Received request cmpl-96fec6dc748d4f21a9599882aee96ddd-0: prompt: 'Can you believe we actually did it? We ran away from home.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 582, 3520, 1521, 432, 30, 1205, 10613, 3123, 504, 2114, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,538 - vllm.entrypoints.logger - INFO - Received request cmpl-a1b0d09ec8364070a2265c36b2c005d8-0: prompt: "Child, I have some exciting news to tell you. I made the decision to move out of my parents' house.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3652, 11, 358, 614, 1045, 13245, 3669, 311, 3291, 498, 13, 358, 1865, 279, 5480, 311, 3271, 700, 315, 847, 6562, 6, 3753, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,551 - vllm.entrypoints.logger - INFO - Received request cmpl-04cfe9f24ce84dfdb3398f26a6cc32e6-0: prompt: 'Doctor, I made some delicious meatballs yesterday for lunch. They turned out to be quite delicious.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 1865, 1045, 17923, 13041, 45518, 13671, 369, 15786, 13, 2379, 6519, 700, 311, 387, 5008, 17923, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,552 - vllm.entrypoints.logger - INFO - Received request cmpl-cd14a4c388814a0e8c4bb017c4a68de5-0: prompt: 'Hey, have you heard about the lawsuit I filed against my former employer?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 911, 279, 19275, 358, 12729, 2348, 847, 4741, 19136, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,559 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-96fec6dc748d4f21a9599882aee96ddd-0.
2025-12-19 23:59:03,560 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a1b0d09ec8364070a2265c36b2c005d8-0.
2025-12-19 23:59:03,561 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-04cfe9f24ce84dfdb3398f26a6cc32e6-0.
2025-12-19 23:59:03,562 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cd14a4c388814a0e8c4bb017c4a68de5-0.
2025-12-19 23:59:03,563 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:03,583 - vllm.entrypoints.logger - INFO - Received request cmpl-c2c11b3802e04db38c7ce45b8db27663-0: prompt: "I've been so busy lately taking care of my younger siblings. It's been tough since our dad moved away after the divorce.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 773, 13028, 30345, 4633, 2453, 315, 847, 14650, 36683, 13, 1084, 594, 1012, 11045, 2474, 1039, 17760, 7726, 3123, 1283, 279, 24532, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,597 - vllm.entrypoints.logger - INFO - Received request cmpl-e0eaebdf422144f29c5468e6e46170ab-0: prompt: "Mentor, I have a really bad stomachache today and I'm feeling really down. I just want to lay in bed and not do anything.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 264, 2167, 3873, 22350, 1777, 3351, 323, 358, 2776, 8266, 2167, 1495, 13, 358, 1101, 1366, 311, 10962, 304, 4845, 323, 537, 653, 4113, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,610 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c2c11b3802e04db38c7ce45b8db27663-0.
2025-12-19 23:59:03,611 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e0eaebdf422144f29c5468e6e46170ab-0.
2025-12-19 23:59:03,612 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:03,616 - vllm.entrypoints.logger - INFO - Received request cmpl-bf55f9cb60104d0db5a751747ca0eff3-0: prompt: "I can't believe how relieved I am that everything worked out for you in court.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1246, 50412, 358, 1079, 429, 4297, 6439, 700, 369, 498, 304, 5473, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,632 - vllm.entrypoints.logger - INFO - Received request cmpl-71350db3e1e94f728eec01fcccd19bbc-0: prompt: 'I always make sure to include a prayer for those in need when I pray.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 1281, 2704, 311, 2924, 264, 22936, 369, 1846, 304, 1184, 979, 358, 23803, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,659 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bf55f9cb60104d0db5a751747ca0eff3-0.
2025-12-19 23:59:03,660 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-71350db3e1e94f728eec01fcccd19bbc-0.
2025-12-19 23:59:03,662 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:03,664 - vllm.entrypoints.logger - INFO - Received request cmpl-6c7a3c6e472b4886a84a3d1ccd6ac993-0: prompt: 'Hey, guess what? I woke up feeling so energetic today!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 7942, 1128, 30, 358, 38726, 705, 8266, 773, 44855, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,672 - vllm.entrypoints.logger - INFO - Received request cmpl-7f6f451dcb224498a38ffaeeb18c2bb3-0: prompt: 'This wine is amazing. I feel so much more relaxed now.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 13078, 374, 7897, 13, 358, 2666, 773, 1753, 803, 30367, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,708 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6c7a3c6e472b4886a84a3d1ccd6ac993-0.
2025-12-19 23:59:03,709 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7f6f451dcb224498a38ffaeeb18c2bb3-0.
2025-12-19 23:59:03,711 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:03,721 - vllm.entrypoints.logger - INFO - Received request cmpl-83bd13b6a12a4c37a18af837399cc07d-0: prompt: 'Hey, B! You know, I was having some trouble with my lawn mower last week, and I asked my neighbor for help. They were more than happy to lend a hand.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 0, 1446, 1414, 11, 358, 572, 3432, 1045, 12264, 448, 847, 36025, 98678, 1537, 2003, 11, 323, 358, 4588, 847, 9565, 369, 1492, 13, 2379, 1033, 803, 1091, 6247, 311, 38480, 264, 1424, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,757 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-83bd13b6a12a4c37a18af837399cc07d-0.
2025-12-19 23:59:03,758 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:03,800 - vllm.entrypoints.logger - INFO - Received request cmpl-940e57282f2c4ba9b6b190714eccb908-0: prompt: 'The increase in energy made me feel excited. I felt my heart rate increase and my breathing quicken. My mind also felt more alert and I had a strong urge to move.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [785, 5263, 304, 4802, 1865, 752, 2666, 12035, 13, 358, 6476, 847, 4746, 4379, 5263, 323, 847, 25938, 3974, 268, 13, 3017, 3971, 1083, 6476, 803, 5115, 323, 358, 1030, 264, 3746, 32047, 311, 3271, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,802 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:03,803 - vllm.entrypoints.logger - INFO - Received request cmpl-87eb866670614cbc8bbc75b84e2a9d03-0: prompt: " It was such a scary experience. I never thought I'd end up at a police station, being suspected of a crime I didn't commit.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1084, 572, 1741, 264, 28465, 3139, 13, 358, 2581, 3381, 358, 4172, 835, 705, 518, 264, 4282, 8056, 11, 1660, 23804, 315, 264, 9778, 358, 3207, 944, 5266, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,804 - vllm.entrypoints.logger - INFO - Received request cmpl-32e0490220a54d04aab293848bebc976-0: prompt: 'Hi, how are you doing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 1246, 525, 498, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,808 - vllm.entrypoints.logger - INFO - Received request cmpl-b1dcb9a11c9048bf8b9b7134f19896e7-0: prompt: 'Mentor, I was walking in the park earlier and noticed something in the tree. It looked like a cat was stuck up there!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 572, 11435, 304, 279, 6118, 6788, 323, 13686, 2494, 304, 279, 4916, 13, 1084, 6966, 1075, 264, 8251, 572, 15700, 705, 1052, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,810 - vllm.entrypoints.logger - INFO - Received request cmpl-34c239804d01433fbae29aa92d91f382-0: prompt: 'Look, Child, I bought a new blouse today. Do you like it?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10380, 11, 9391, 11, 358, 10788, 264, 501, 95945, 3351, 13, 3155, 498, 1075, 432, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,812 - vllm.entrypoints.logger - INFO - Received request cmpl-3c388478f1384d8d900c947fe8a8c71d-0: prompt: "Doctor, I wanted to talk to you about something that happened to me recently. I was walking on a trail and I fell, injuring my spine. Now I'm in a wheelchair and I want to be able to walk again.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 6932, 311, 752, 5926, 13, 358, 572, 11435, 389, 264, 8849, 323, 358, 11052, 11, 85585, 847, 34676, 13, 4695, 358, 2776, 304, 264, 53518, 323, 358, 1366, 311, 387, 2952, 311, 4227, 1549, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,814 - vllm.entrypoints.logger - INFO - Received request cmpl-b77ac9230bab4f9bb341c35f3fbc09f4-0: prompt: "Mentor, have you ever rearranged the furniture in your house just because you didn't like the way it looked?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 614, 498, 3512, 55327, 3726, 279, 14549, 304, 697, 3753, 1101, 1576, 498, 3207, 944, 1075, 279, 1616, 432, 6966, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,816 - vllm.entrypoints.logger - INFO - Received request cmpl-38b02822e4074b178a167baca85aa96a-0: prompt: "Mentor, I have been reading some old letters lately and it's making me want to write some of my own.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 1012, 5290, 1045, 2310, 11931, 30345, 323, 432, 594, 3259, 752, 1366, 311, 3270, 1045, 315, 847, 1828, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,837 - vllm.entrypoints.logger - INFO - Received request cmpl-5f95fb55acbd4c94a1d5357eec95855d-0: prompt: 'I made sure to put my math and social studies books in my backpack tonight.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1865, 2704, 311, 2182, 847, 6888, 323, 3590, 7822, 6467, 304, 847, 33136, 17913, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,863 - vllm.entrypoints.logger - INFO - Received request cmpl-57536dba3c8041a48d758af771f203ff-0: prompt: 'Hi Mentor, I have been reading this amazing advice book lately, and it has really helped me understand myself and others better.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 614, 1012, 5290, 419, 7897, 9462, 2311, 30345, 11, 323, 432, 702, 2167, 8910, 752, 3535, 7037, 323, 3800, 2664, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,870 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-940e57282f2c4ba9b6b190714eccb908-0.
2025-12-19 23:59:03,871 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-87eb866670614cbc8bbc75b84e2a9d03-0.
2025-12-19 23:59:03,872 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-32e0490220a54d04aab293848bebc976-0.
2025-12-19 23:59:03,874 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b1dcb9a11c9048bf8b9b7134f19896e7-0.
2025-12-19 23:59:03,875 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-34c239804d01433fbae29aa92d91f382-0.
2025-12-19 23:59:03,876 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3c388478f1384d8d900c947fe8a8c71d-0.
2025-12-19 23:59:03,877 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b77ac9230bab4f9bb341c35f3fbc09f4-0.
2025-12-19 23:59:03,878 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-38b02822e4074b178a167baca85aa96a-0.
2025-12-19 23:59:03,879 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5f95fb55acbd4c94a1d5357eec95855d-0.
2025-12-19 23:59:03,880 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-57536dba3c8041a48d758af771f203ff-0.
2025-12-19 23:59:03,881 - vllm.core.scheduler - INFO - Pending queue size: (10)
2025-12-19 23:59:03,885 - vllm.entrypoints.logger - INFO - Received request cmpl-b13b690c56f84121b88ff10ddab2b1fd-0: prompt: "Boss, I need to talk to you about the way you've been speaking to me lately. It's been really disrespectful and I won't tolerate it any longer.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 358, 1184, 311, 3061, 311, 498, 911, 279, 1616, 498, 3003, 1012, 12094, 311, 752, 30345, 13, 1084, 594, 1012, 2167, 98522, 323, 358, 2765, 944, 49034, 432, 894, 5021, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,899 - vllm.entrypoints.logger - INFO - Received request cmpl-4a79f3a57b2241a1821358e11b04d927-0: prompt: "Did you hear about the food drive I'm organizing for the local families in need?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 6723, 911, 279, 3607, 6541, 358, 2776, 34721, 369, 279, 2205, 8521, 304, 1184, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,907 - vllm.entrypoints.logger - INFO - Received request cmpl-48a4d2f657464c24ac923f52e35c0e26-0: prompt: 'Did you see the new employee that started today? I made sure to introduce myself and make her feel welcome.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 1490, 279, 501, 9364, 429, 3855, 3351, 30, 358, 1865, 2704, 311, 19131, 7037, 323, 1281, 1059, 2666, 10565, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,930 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b13b690c56f84121b88ff10ddab2b1fd-0.
2025-12-19 23:59:03,931 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4a79f3a57b2241a1821358e11b04d927-0.
2025-12-19 23:59:03,933 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-48a4d2f657464c24ac923f52e35c0e26-0.
2025-12-19 23:59:03,934 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:03,938 - vllm.entrypoints.logger - INFO - Received request cmpl-7006a5f243c4430a9652e93eeec687d3-0: prompt: "Hey there, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,943 - vllm.entrypoints.logger - INFO - Received request cmpl-f25572721a88417f8b4ea7878caab123-0: prompt: "Coach, I've been wanting to tell you something for a while now, but I'm really nervous.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 3003, 1012, 19211, 311, 3291, 498, 2494, 369, 264, 1393, 1431, 11, 714, 358, 2776, 2167, 22596, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,944 - vllm.entrypoints.logger - INFO - Received request cmpl-888e77653a0846a09bb006769a0e0bfe-0: prompt: " I can't focus on work today. I just can't stop thinking about the fight I had with my girlfriend.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 646, 944, 5244, 389, 975, 3351, 13, 358, 1101, 646, 944, 2936, 7274, 911, 279, 4367, 358, 1030, 448, 847, 22761, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,949 - vllm.entrypoints.logger - INFO - Received request cmpl-56f5d797d42a4861b7ab3b3f4ae0f864-0: prompt: 'Hi there! Can I join in and play some Frisbee with you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 0, 2980, 358, 5138, 304, 323, 1486, 1045, 2869, 285, 32031, 448, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,958 - vllm.entrypoints.logger - INFO - Received request cmpl-24e179632aac4b0c95ed44a887d19b3c-0: prompt: 'Today on the way to school, I passed a couple people on the street. I felt a sense of pride knowing that I was walking faster than them.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 389, 279, 1616, 311, 2906, 11, 358, 5823, 264, 5625, 1251, 389, 279, 8592, 13, 358, 6476, 264, 5530, 315, 21770, 14063, 429, 358, 572, 11435, 10596, 1091, 1105, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,962 - vllm.entrypoints.logger - INFO - Received request cmpl-528afd5d0d814d86bcc0b4cf32fc5e3f-0: prompt: "Alright, I washed your car. Where's my payment?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [71486, 11, 358, 37493, 697, 1803, 13, 10967, 594, 847, 8160, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,980 - vllm.entrypoints.logger - INFO - Received request cmpl-1694d33df15a472c9675e20568cff3e8-0: prompt: 'I had such a great dinner tonight.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 1741, 264, 2244, 13856, 17913, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,986 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7006a5f243c4430a9652e93eeec687d3-0.
2025-12-19 23:59:03,987 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f25572721a88417f8b4ea7878caab123-0.
2025-12-19 23:59:03,988 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-888e77653a0846a09bb006769a0e0bfe-0.
2025-12-19 23:59:03,989 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-56f5d797d42a4861b7ab3b3f4ae0f864-0.
2025-12-19 23:59:03,989 - vllm.entrypoints.logger - INFO - Received request cmpl-6ea06c8718ca4f96a545557b4c7232ae-0: prompt: 'Today, I wrote a letter to an old friend.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 6139, 264, 6524, 311, 458, 2310, 4238, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,990 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-24e179632aac4b0c95ed44a887d19b3c-0.
2025-12-19 23:59:03,991 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-528afd5d0d814d86bcc0b4cf32fc5e3f-0.
2025-12-19 23:59:03,992 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1694d33df15a472c9675e20568cff3e8-0.
2025-12-19 23:59:03,993 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6ea06c8718ca4f96a545557b4c7232ae-0.
2025-12-19 23:59:03,994 - vllm.core.scheduler - INFO - Pending queue size: (8)
2025-12-19 23:59:03,997 - vllm.entrypoints.logger - INFO - Received request cmpl-6b4e72ae887944468d8e5b30aa8c02af-0: prompt: "I can't believe I failed the entrance exam. I worked so hard for it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 4641, 279, 19809, 7006, 13, 358, 6439, 773, 2588, 369, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:03,999 - vllm.entrypoints.logger - INFO - Received request cmpl-937bc6773f7143c6a8944ccb2e3ad262-0: prompt: 'Today, I saw an old woman in pain and helped her walk to her house. It felt really good to help her.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 5485, 458, 2310, 5220, 304, 6646, 323, 8910, 1059, 4227, 311, 1059, 3753, 13, 1084, 6476, 2167, 1661, 311, 1492, 1059, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,006 - vllm.entrypoints.logger - INFO - Received request cmpl-9bb42b0a59f649288110f0c7dc7418c0-0: prompt: 'I finally posted my item online for $10, I am excited to see someone purchase it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 5499, 8454, 847, 1509, 2860, 369, 400, 16, 15, 11, 358, 1079, 12035, 311, 1490, 4325, 7627, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,019 - vllm.entrypoints.logger - INFO - Received request cmpl-aa2f76143e7c44b6a1a2a4f111261f67-0: prompt: "Child, I need to tell you something. Recently, I've been having some trouble with my bowels. It's been really uncomfortable.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3652, 11, 358, 1184, 311, 3291, 498, 2494, 13, 40996, 11, 358, 3003, 1012, 3432, 1045, 12264, 448, 847, 15273, 2010, 13, 1084, 594, 1012, 2167, 28113, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,040 - vllm.entrypoints.logger - INFO - Received request cmpl-270f43fadcbd442eb7b6d0e000bc7e50-0: prompt: 'How was your day today, love? ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4340, 572, 697, 1899, 3351, 11, 2948, 30, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,041 - vllm.entrypoints.logger - INFO - Received request cmpl-6170cc5dab2b4235b76468888ab58780-0: prompt: 'Thanks for the cigarette, man.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12658, 369, 279, 35113, 11, 883, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,043 - vllm.entrypoints.logger - INFO - Received request cmpl-7782e470a3e240c3a88f513d2c9a13a9-0: prompt: "I'm really sad today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 12421, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,046 - vllm.entrypoints.logger - INFO - Received request cmpl-ccc56640351a4195a76cd6fdb7347adc-0: prompt: 'Hey, have you noticed that I always wear a bulletproof vest to class?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 13686, 429, 358, 2677, 9850, 264, 17432, 15780, 27605, 311, 536, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,046 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6b4e72ae887944468d8e5b30aa8c02af-0.
2025-12-19 23:59:04,047 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-937bc6773f7143c6a8944ccb2e3ad262-0.
2025-12-19 23:59:04,049 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9bb42b0a59f649288110f0c7dc7418c0-0.
2025-12-19 23:59:04,050 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-aa2f76143e7c44b6a1a2a4f111261f67-0.
2025-12-19 23:59:04,051 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-270f43fadcbd442eb7b6d0e000bc7e50-0.
2025-12-19 23:59:04,053 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6170cc5dab2b4235b76468888ab58780-0.
2025-12-19 23:59:04,055 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7782e470a3e240c3a88f513d2c9a13a9-0.
2025-12-19 23:59:04,054 - vllm.entrypoints.logger - INFO - Received request cmpl-1aa5f0ff344c40be9dab90aaa42735dd-0: prompt: 'Hi, Teacher. How are you today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 29069, 13, 2585, 525, 498, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,056 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ccc56640351a4195a76cd6fdb7347adc-0.
2025-12-19 23:59:04,057 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1aa5f0ff344c40be9dab90aaa42735dd-0.
2025-12-19 23:59:04,058 - vllm.core.scheduler - INFO - Pending queue size: (9)
2025-12-19 23:59:04,079 - vllm.entrypoints.logger - INFO - Received request cmpl-84d2e1dbbb244f6c8e547769fe6438be-0: prompt: 'Hey, look what I found in my drawer today!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1401, 1128, 358, 1730, 304, 847, 26482, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,082 - vllm.entrypoints.logger - INFO - Received request cmpl-e5bb18dae6a147779f08961a27da6540-0: prompt: 'I tried to hurry and finish the project so I could be done, but then I realized that I had made a mistake. I went back and corrected it before continuing on so that my work would be accurate.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 6679, 311, 47235, 323, 6248, 279, 2390, 773, 358, 1410, 387, 2814, 11, 714, 1221, 358, 15043, 429, 358, 1030, 1865, 264, 16523, 13, 358, 3937, 1182, 323, 35965, 432, 1573, 14354, 389, 773, 429, 847, 975, 1035, 387, 13382, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,103 - vllm.entrypoints.logger - INFO - Received request cmpl-c70c8bad61e04c81a866182dd320cfd9-0: prompt: "Hey, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,105 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-84d2e1dbbb244f6c8e547769fe6438be-0.
2025-12-19 23:59:04,106 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e5bb18dae6a147779f08961a27da6540-0.
2025-12-19 23:59:04,107 - vllm.entrypoints.logger - INFO - Received request cmpl-dde9e993f83646c7add0fc700147322b-0: prompt: "I always try to keep a smile on my face even when things hurt because I don't want to appear weak.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 1430, 311, 2506, 264, 15289, 389, 847, 3579, 1496, 979, 2513, 12898, 1576, 358, 1513, 944, 1366, 311, 4994, 7469, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,108 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c70c8bad61e04c81a866182dd320cfd9-0.
2025-12-19 23:59:04,109 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dde9e993f83646c7add0fc700147322b-0.
2025-12-19 23:59:04,109 - vllm.entrypoints.logger - INFO - Received request cmpl-6264f034d5344a47876450c0cf0ee01f-0: prompt: ' Playing with those kids was really fun.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [39451, 448, 1846, 6837, 572, 2167, 2464, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,110 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:04,143 - vllm.entrypoints.logger - INFO - Received request cmpl-d22abb1f9155431b8de1940b57b783ae-0: prompt: " Whew, I can't believe I made it around the block!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1232, 365, 11, 358, 646, 944, 4411, 358, 1865, 432, 2163, 279, 2504, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,146 - vllm.entrypoints.logger - INFO - Received request cmpl-55a902adba19423dbd67a7f153dca668-0: prompt: "I just couldn't get on that plane today, I'm sorry.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 7691, 944, 633, 389, 429, 11031, 3351, 11, 358, 2776, 14589, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,149 - vllm.entrypoints.logger - INFO - Received request cmpl-38c4a34e5d934b83ae31e3ac0e96552a-0: prompt: 'I was just sentenced to four years in prison for embezzlement.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 1101, 29131, 311, 3040, 1635, 304, 9343, 369, 976, 1371, 10400, 986, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,156 - vllm.entrypoints.logger - INFO - Received request cmpl-7ce2c45dc081493da7978f1bb9abd670-0: prompt: 'Hey, do you remember when I told you about my fear of being alone and not being able to find my way home?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 6099, 979, 358, 3229, 498, 911, 847, 8679, 315, 1660, 7484, 323, 537, 1660, 2952, 311, 1477, 847, 1616, 2114, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,158 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6264f034d5344a47876450c0cf0ee01f-0.
2025-12-19 23:59:04,159 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d22abb1f9155431b8de1940b57b783ae-0.
2025-12-19 23:59:04,160 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-55a902adba19423dbd67a7f153dca668-0.
2025-12-19 23:59:04,161 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-38c4a34e5d934b83ae31e3ac0e96552a-0.
2025-12-19 23:59:04,162 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7ce2c45dc081493da7978f1bb9abd670-0.
2025-12-19 23:59:04,163 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:04,177 - vllm.entrypoints.logger - INFO - Received request cmpl-eff21f673a784387ac392e82b3c4ac65-0: prompt: 'Sorry, I just want to be quiet for a while.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [19152, 11, 358, 1101, 1366, 311, 387, 11340, 369, 264, 1393, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,208 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eff21f673a784387ac392e82b3c4ac65-0.
2025-12-19 23:59:04,210 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:04,212 - vllm.entrypoints.logger - INFO - Received request cmpl-279829442a314ba1b919bbdfee74f394-0: prompt: 'Hey, Neighbors B! I wanted to tell you about my job. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 0, 358, 4829, 311, 3291, 498, 911, 847, 2618, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,226 - vllm.entrypoints.logger - INFO - Received request cmpl-7179b3b9aa0b4199832655966bc95282-0: prompt: ', I have to apologize. I was wrong about what I said last time we spoke.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 614, 311, 36879, 13, 358, 572, 4969, 911, 1128, 358, 1053, 1537, 882, 582, 12290, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,253 - vllm.entrypoints.logger - INFO - Received request cmpl-71d177c852024c7688b1c9bf47fb7e8d-0: prompt: 'Ah, that was a refreshing shower. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 429, 572, 264, 35918, 17196, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,255 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-279829442a314ba1b919bbdfee74f394-0.
2025-12-19 23:59:04,256 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7179b3b9aa0b4199832655966bc95282-0.
2025-12-19 23:59:04,257 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-71d177c852024c7688b1c9bf47fb7e8d-0.
2025-12-19 23:59:04,259 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:04,301 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:04,311 - vllm.entrypoints.logger - INFO - Received request cmpl-82ac6fbfb3c04fa3a76d7bfd703d5324-0: prompt: "I'm really glad we went to the bakery today. The bread there is always so fresh and delicious.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 15713, 582, 3937, 311, 279, 65144, 3351, 13, 576, 16002, 1052, 374, 2677, 773, 7722, 323, 17923, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,359 - vllm.entrypoints.logger - INFO - Received request cmpl-b2741f247446446496099f86bb4c282a-0: prompt: "Hey, sorry I can't make it to the party tonight. I'm feeling really sick.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 14589, 358, 646, 944, 1281, 432, 311, 279, 4614, 17913, 13, 358, 2776, 8266, 2167, 14036, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,361 - vllm.entrypoints.logger - INFO - Received request cmpl-af38cdbfa20c4eb8b6b4065d3d27342e-0: prompt: "I'm really worried that our team is going to lose the game.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 17811, 429, 1039, 2083, 374, 2087, 311, 9052, 279, 1809, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,368 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-82ac6fbfb3c04fa3a76d7bfd703d5324-0.
2025-12-19 23:59:04,370 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b2741f247446446496099f86bb4c282a-0.
2025-12-19 23:59:04,371 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-af38cdbfa20c4eb8b6b4065d3d27342e-0.
2025-12-19 23:59:04,372 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:04,419 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:04,454 - vllm.entrypoints.logger - INFO - Received request cmpl-2553e44246284c2693122889cd495fda-0: prompt: 'Do you remember our field trip to the zoo a few weeks ago? It was so much fun!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5404, 498, 6099, 1039, 2070, 8411, 311, 279, 40914, 264, 2421, 5555, 4134, 30, 1084, 572, 773, 1753, 2464, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,483 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2553e44246284c2693122889cd495fda-0.
2025-12-19 23:59:04,486 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:04,536 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:04,567 - vllm.entrypoints.logger - INFO - Received request cmpl-73864d31851540bf8329dde5df487619-0: prompt: "This is such a nice day to be outside, isn't it?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 1741, 264, 6419, 1899, 311, 387, 4889, 11, 4436, 944, 432, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,570 - vllm.entrypoints.logger - INFO - Received request cmpl-b1b843694b7049b2ba4265b6a3023c52-0: prompt: ', I wanted to talk to you about something that happened with my friend recently.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 6932, 448, 847, 4238, 5926, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,589 - vllm.entrypoints.logger - INFO - Received request cmpl-45f7e7163e844ca78ae9bfa684d565c0-0: prompt: 'I have been practicing my art skills a lot lately.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 1012, 35566, 847, 1947, 7361, 264, 2696, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,593 - vllm.entrypoints.logger - INFO - Received request cmpl-b0546e8efb6d4f5fbadde15e34766145-0: prompt: 'Can you believe he left after only an hour? I thought he would be there for at least a few more hours.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 566, 2115, 1283, 1172, 458, 6460, 30, 358, 3381, 566, 1035, 387, 1052, 369, 518, 3245, 264, 2421, 803, 4115, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,605 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-73864d31851540bf8329dde5df487619-0.
2025-12-19 23:59:04,606 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b1b843694b7049b2ba4265b6a3023c52-0.
2025-12-19 23:59:04,607 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-45f7e7163e844ca78ae9bfa684d565c0-0.
2025-12-19 23:59:04,608 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b0546e8efb6d4f5fbadde15e34766145-0.
2025-12-19 23:59:04,610 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:04,627 - vllm.entrypoints.logger - INFO - Received request cmpl-08b9ec8de93c4322a484acb13782bcdb-0: prompt: "I never want to go back to my hometown. It's just so boring there.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2581, 1366, 311, 728, 1182, 311, 847, 43016, 13, 1084, 594, 1101, 773, 27759, 1052, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,628 - vllm.entrypoints.logger - INFO - Received request cmpl-f40e92a0535c41978486028812b1b0b1-0: prompt: 'I played the best practical joke on Sarah the other day.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 6342, 279, 1850, 14976, 21646, 389, 20445, 279, 1008, 1899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,631 - vllm.entrypoints.logger - INFO - Received request cmpl-911b7800f66745b68b414cbc7918cb75-0: prompt: "I've learned to appreciate nature so much more since I started going on walks and noticing all the different animals and plants.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 9498, 311, 15401, 6993, 773, 1753, 803, 2474, 358, 3855, 2087, 389, 22479, 323, 61364, 678, 279, 2155, 9898, 323, 10779, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,633 - vllm.entrypoints.logger - INFO - Received request cmpl-4eaf4bd81e0d4cdd92e57a9d2148d29e-0: prompt: "This is such a nice day at the beach, isn't it?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 1741, 264, 6419, 1899, 518, 279, 11321, 11, 4436, 944, 432, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,647 - vllm.entrypoints.logger - INFO - Received request cmpl-c35c68ecb0914a52999257380c017c8b-0: prompt: "I'm loving being back in Dallas and spending more time with my family.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 21258, 1660, 1182, 304, 18542, 323, 10164, 803, 882, 448, 847, 2997, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,654 - vllm.entrypoints.logger - INFO - Received request cmpl-ac25beae86d648c6881ca9766bd0ecc6-0: prompt: ", I wanted to tell you that I really like you. I think you're really pretty and I would like to go out with you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 4829, 311, 3291, 498, 429, 358, 2167, 1075, 498, 13, 358, 1744, 498, 2299, 2167, 5020, 323, 358, 1035, 1075, 311, 728, 700, 448, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,659 - vllm.entrypoints.logger - INFO - Received request cmpl-b0f66464590d4a96a618f203e60961a1-0: prompt: "I'm really anxious about this decision I have to make.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 37000, 911, 419, 5480, 358, 614, 311, 1281, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,667 - vllm.entrypoints.logger - INFO - Received request cmpl-0923012c9667489083565642aec8ccdf-0: prompt: "I'm completely lost right now. I've been walking around for an hour and I can't find my hotel anywhere.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 6587, 5558, 1290, 1431, 13, 358, 3003, 1012, 11435, 2163, 369, 458, 6460, 323, 358, 646, 944, 1477, 847, 9500, 12379, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,667 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-08b9ec8de93c4322a484acb13782bcdb-0.
2025-12-19 23:59:04,668 - vllm.entrypoints.logger - INFO - Received request cmpl-caed6e15c1784b16aaafa3a0b9ec834f-0: prompt: "I'm so excited to be here at the political rally. It feels like we're part of something big.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 311, 387, 1588, 518, 279, 4948, 19217, 13, 1084, 11074, 1075, 582, 2299, 949, 315, 2494, 2409, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,669 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f40e92a0535c41978486028812b1b0b1-0.
2025-12-19 23:59:04,670 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-911b7800f66745b68b414cbc7918cb75-0.
2025-12-19 23:59:04,671 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4eaf4bd81e0d4cdd92e57a9d2148d29e-0.
2025-12-19 23:59:04,672 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c35c68ecb0914a52999257380c017c8b-0.
2025-12-19 23:59:04,672 - vllm.entrypoints.logger - INFO - Received request cmpl-3c41d49367e94d75a83ba2b4ce41ccfb-0: prompt: "I'm happy that I can be helpful to you, Classmates B.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 6247, 429, 358, 646, 387, 10950, 311, 498, 11, 3228, 16457, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,672 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ac25beae86d648c6881ca9766bd0ecc6-0.
2025-12-19 23:59:04,673 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b0f66464590d4a96a618f203e60961a1-0.
2025-12-19 23:59:04,674 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0923012c9667489083565642aec8ccdf-0.
2025-12-19 23:59:04,675 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-caed6e15c1784b16aaafa3a0b9ec834f-0.
2025-12-19 23:59:04,676 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3c41d49367e94d75a83ba2b4ce41ccfb-0.
2025-12-19 23:59:04,677 - vllm.core.scheduler - INFO - Pending queue size: (10)
2025-12-19 23:59:04,686 - vllm.entrypoints.logger - INFO - Received request cmpl-c9538c7e44e4426e941a97e41e76f20c-0: prompt: 'Ugh, I think I caught a cold.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 1744, 358, 10568, 264, 9255, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,701 - vllm.entrypoints.logger - INFO - Received request cmpl-ff11de6ac073453e871a8e8b1c01fa9e-0: prompt: 'My nose itches, so I licked it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5050, 19142, 432, 8528, 11, 773, 358, 92935, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,717 - vllm.entrypoints.logger - INFO - Received request cmpl-bd44f9e433b14aa8bb9ab6ff7e5e7de7-0: prompt: 'These Nike shoes are amazing. I feel like I can run forever in them.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9485, 33144, 15294, 525, 7897, 13, 358, 2666, 1075, 358, 646, 1598, 15683, 304, 1105, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,725 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c9538c7e44e4426e941a97e41e76f20c-0.
2025-12-19 23:59:04,726 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ff11de6ac073453e871a8e8b1c01fa9e-0.
2025-12-19 23:59:04,727 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bd44f9e433b14aa8bb9ab6ff7e5e7de7-0.
2025-12-19 23:59:04,728 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:04,771 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:04,772 - vllm.entrypoints.logger - INFO - Received request cmpl-463e606c77c64cbcac1e80c44be8f124-0: prompt: "Hi there, Neighbors B. I hope you're having a good day.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 4182, 24101, 425, 13, 358, 3900, 498, 2299, 3432, 264, 1661, 1899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,788 - vllm.entrypoints.logger - INFO - Received request cmpl-3561870f3fc240a7bbaaa24d7e6516cc-0: prompt: "I've been really trying to challenge myself creatively lately. It's been so much fun trying new things.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 2167, 4460, 311, 8645, 7037, 92296, 30345, 13, 1084, 594, 1012, 773, 1753, 2464, 4460, 501, 2513, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,807 - vllm.entrypoints.logger - INFO - Received request cmpl-0d90dbf994c24255a4fea0da16cd950c-0: prompt: 'Hey, have you met the new student, Classmates B?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 2270, 279, 501, 5458, 11, 3228, 16457, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,810 - vllm.entrypoints.logger - INFO - Received request cmpl-bb8c38d42e5f4b19a6748f6a0c835130-0: prompt: 'Did you know there are 1,023 possible ways to arrange ten toy cars in a line?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 1414, 1052, 525, 220, 16, 11, 15, 17, 18, 3204, 5510, 311, 30893, 5779, 21357, 9331, 304, 264, 1555, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,819 - vllm.entrypoints.logger - INFO - Received request cmpl-7517772be93b4b53b021a927e229d37b-0: prompt: "I've been working really hard to establish my company. I've made sure to pick the perfect location, and I'm hiring people that I know will work hard.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 3238, 2167, 2588, 311, 5695, 847, 2813, 13, 358, 3003, 1865, 2704, 311, 3735, 279, 4727, 3728, 11, 323, 358, 2776, 23134, 1251, 429, 358, 1414, 686, 975, 2588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,822 - vllm.entrypoints.logger - INFO - Received request cmpl-e9deb8c1cfd9486f9b909e5616c739f4-0: prompt: 'Hey, I just found out that my father was born in Mexico and moved to the U.S. when he was young. He also has two brothers and one sister.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1101, 1730, 700, 429, 847, 6981, 572, 9223, 304, 12270, 323, 7726, 311, 279, 547, 808, 13, 979, 566, 572, 3908, 13, 1260, 1083, 702, 1378, 20208, 323, 825, 12923, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,834 - vllm.entrypoints.logger - INFO - Received request cmpl-39a80587070143c08775d3e61ef36a32-0: prompt: "I need to move this table to make room for a new project I'm working on.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1184, 311, 3271, 419, 1965, 311, 1281, 3054, 369, 264, 501, 2390, 358, 2776, 3238, 389, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,836 - vllm.entrypoints.logger - INFO - Received request cmpl-9114eed14e7346a4b35ab4b91b982617-0: prompt: 'I hate when my little sister ruins my stuff. She always gets away with everything.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 12213, 979, 847, 2632, 12923, 45662, 847, 6259, 13, 2932, 2677, 5221, 3123, 448, 4297, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,842 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-463e606c77c64cbcac1e80c44be8f124-0.
2025-12-19 23:59:04,844 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3561870f3fc240a7bbaaa24d7e6516cc-0.
2025-12-19 23:59:04,845 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d90dbf994c24255a4fea0da16cd950c-0.
2025-12-19 23:59:04,846 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bb8c38d42e5f4b19a6748f6a0c835130-0.
2025-12-19 23:59:04,847 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7517772be93b4b53b021a927e229d37b-0.
2025-12-19 23:59:04,848 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e9deb8c1cfd9486f9b909e5616c739f4-0.
2025-12-19 23:59:04,849 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-39a80587070143c08775d3e61ef36a32-0.
2025-12-19 23:59:04,850 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9114eed14e7346a4b35ab4b91b982617-0.
2025-12-19 23:59:04,851 - vllm.core.scheduler - INFO - Pending queue size: (8)
2025-12-19 23:59:04,866 - vllm.entrypoints.logger - INFO - Received request cmpl-e68bc1dac978415eb94f49a19b3cbb7a-0: prompt: 'I was walking home from the store yesterday when I saw a police officer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 11435, 2114, 504, 279, 3553, 13671, 979, 358, 5485, 264, 4282, 9452, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,867 - vllm.entrypoints.logger - INFO - Received request cmpl-84af2c98b67e44f79fd4577638526ee4-0: prompt: ", do you think it's weird that I put on your gloves without asking?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 653, 498, 1744, 432, 594, 16283, 429, 358, 2182, 389, 697, 35416, 2041, 10161, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,871 - vllm.entrypoints.logger - INFO - Received request cmpl-0316aa752a6a4e8989085b6bcdc334c4-0: prompt: ' Ahh, this feels so good. I love lying on this wool mat.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [16366, 71, 11, 419, 11074, 773, 1661, 13, 358, 2948, 20446, 389, 419, 38540, 5517, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,873 - vllm.entrypoints.logger - INFO - Received request cmpl-6c6e030eecf9489ca52c8676a2844fb5-0: prompt: "I've been to many countries, including France, Italy, Spain, and Portugal. I've also been to Australia, New Zealand, and Japan.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 311, 1657, 5837, 11, 2670, 9625, 11, 15344, 11, 17689, 11, 323, 33311, 13, 358, 3003, 1083, 1012, 311, 8330, 11, 1532, 16911, 11, 323, 6323, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,876 - vllm.entrypoints.logger - INFO - Received request cmpl-e2bc0daf23e7448b8f864efa56f5a784-0: prompt: ' Why did you leave the door unlocked again? How many times do I have to remind you about this?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8429, 1521, 498, 5274, 279, 6006, 35618, 1549, 30, 2585, 1657, 3039, 653, 358, 614, 311, 23974, 498, 911, 419, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,877 - vllm.entrypoints.logger - INFO - Received request cmpl-5702fc4753254c4db149671b7f4a8a15-0: prompt: 'Hey, check out my new shirt! I just bought it from the store.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1779, 700, 847, 501, 15478, 0, 358, 1101, 10788, 432, 504, 279, 3553, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,885 - vllm.entrypoints.logger - INFO - Received request cmpl-3d04180ca859461290808f2a6d757280-0: prompt: 'Hey, do you want a glass of wine?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 1366, 264, 8991, 315, 13078, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,898 - vllm.entrypoints.logger - INFO - Received request cmpl-0a3695600b16415eae8288e1174801e4-0: prompt: "I can't believe it, I won my first case as a prosecutor today!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 432, 11, 358, 2765, 847, 1156, 1142, 438, 264, 35051, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,906 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e68bc1dac978415eb94f49a19b3cbb7a-0.
2025-12-19 23:59:04,907 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-84af2c98b67e44f79fd4577638526ee4-0.
2025-12-19 23:59:04,908 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0316aa752a6a4e8989085b6bcdc334c4-0.
2025-12-19 23:59:04,908 - vllm.entrypoints.logger - INFO - Received request cmpl-ff6ab9c3e1dc40cea6ecc2bdefff6c23-0: prompt: "My mom said I couldn't leave the party until it was over and I really wanted to go outside and play with my friends.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5050, 3368, 1053, 358, 7691, 944, 5274, 279, 4614, 3080, 432, 572, 916, 323, 358, 2167, 4829, 311, 728, 4889, 323, 1486, 448, 847, 4780, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,911 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6c6e030eecf9489ca52c8676a2844fb5-0.
2025-12-19 23:59:04,912 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e2bc0daf23e7448b8f864efa56f5a784-0.
2025-12-19 23:59:04,912 - vllm.entrypoints.logger - INFO - Received request cmpl-66bcb3504a1b4787822ca1deddbef892-0: prompt: 'Hey! I have great news! I got a tryout for the soccer team.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 0, 358, 614, 2244, 3669, 0, 358, 2684, 264, 1430, 411, 369, 279, 22174, 2083, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,913 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5702fc4753254c4db149671b7f4a8a15-0.
2025-12-19 23:59:04,914 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3d04180ca859461290808f2a6d757280-0.
2025-12-19 23:59:04,915 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0a3695600b16415eae8288e1174801e4-0.
2025-12-19 23:59:04,915 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ff6ab9c3e1dc40cea6ecc2bdefff6c23-0.
2025-12-19 23:59:04,916 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-66bcb3504a1b4787822ca1deddbef892-0.
2025-12-19 23:59:04,918 - vllm.core.scheduler - INFO - Pending queue size: (10)
2025-12-19 23:59:04,947 - vllm.entrypoints.logger - INFO - Received request cmpl-951dc7d530d7465d983023afe252ce58-0: prompt: "Hey, I brought you some cupcakes today. I know you're trying to stick to your diet, but I thought you could indulge a little.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 7117, 498, 1045, 87893, 3351, 13, 358, 1414, 498, 2299, 4460, 311, 9214, 311, 697, 9968, 11, 714, 358, 3381, 498, 1410, 67090, 264, 2632, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,963 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-951dc7d530d7465d983023afe252ce58-0.
2025-12-19 23:59:04,964 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:04,969 - vllm.entrypoints.logger - INFO - Received request cmpl-f6c50a77efb9400a83a91d151f87b391-0: prompt: "Ugh, I just can't believe she moved away. I feel so alone.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 1101, 646, 944, 4411, 1340, 7726, 3123, 13, 358, 2666, 773, 7484, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:04,976 - vllm.entrypoints.logger - INFO - Received request cmpl-eb94721ee26148089781500458718ec4-0: prompt: "Hi Mentor, I was wondering if you could teach me about Yoda. I'm really interested in his unique form of communication and his wisdom.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 572, 20293, 421, 498, 1410, 4538, 752, 911, 809, 13993, 13, 358, 2776, 2167, 8014, 304, 806, 4911, 1352, 315, 10535, 323, 806, 23389, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,007 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f6c50a77efb9400a83a91d151f87b391-0.
2025-12-19 23:59:05,008 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eb94721ee26148089781500458718ec4-0.
2025-12-19 23:59:05,010 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,013 - vllm.entrypoints.logger - INFO - Received request cmpl-f62bfe8913f44d31b5f0bf47eeee1259-0: prompt: 'Wife said she had to leave.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [54, 1612, 1053, 1340, 1030, 311, 5274, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,036 - vllm.entrypoints.logger - INFO - Received request cmpl-bacb1ecbeba441babdb3410aa3f8cd31-0: prompt: "I'm glad I was able to help you with your inbox. It was getting quite cluttered.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 15713, 358, 572, 2952, 311, 1492, 498, 448, 697, 22883, 13, 1084, 572, 3709, 5008, 53816, 291, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,053 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f62bfe8913f44d31b5f0bf47eeee1259-0.
2025-12-19 23:59:05,054 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bacb1ecbeba441babdb3410aa3f8cd31-0.
2025-12-19 23:59:05,056 - vllm.entrypoints.logger - INFO - Received request cmpl-d7ede7110d854592903a92ba9bdf0664-0: prompt: 'Hi, can I talk to you about something?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 646, 358, 3061, 311, 498, 911, 2494, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,056 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,069 - vllm.entrypoints.logger - INFO - Received request cmpl-6ab9ab6b4b53455ea6a081f88a3bbf92-0: prompt: "I'm so grateful for my position as a minister. I finally feel like I have some kind of respect in this world.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 25195, 369, 847, 2309, 438, 264, 12725, 13, 358, 5499, 2666, 1075, 358, 614, 1045, 3093, 315, 5091, 304, 419, 1879, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,099 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d7ede7110d854592903a92ba9bdf0664-0.
2025-12-19 23:59:05,100 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6ab9ab6b4b53455ea6a081f88a3bbf92-0.
2025-12-19 23:59:05,102 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,112 - vllm.entrypoints.logger - INFO - Received request cmpl-231d71ef6d894fb09e5b7f442640ff9e-0: prompt: 'Ugh, I feel terrible. I just want to stay in bed all day and watch Netflix.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 2666, 17478, 13, 358, 1101, 1366, 311, 4717, 304, 4845, 678, 1899, 323, 3736, 22642, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,120 - vllm.entrypoints.logger - INFO - Received request cmpl-22cd3db7f41d42bcaa784ce0db106f1c-0: prompt: "I feel really sad. I was refused the loan because I couldn't provide proof of income. Now I have to find another way to pay for my education.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 2167, 12421, 13, 358, 572, 16051, 279, 11679, 1576, 358, 7691, 944, 3410, 11064, 315, 7911, 13, 4695, 358, 614, 311, 1477, 2441, 1616, 311, 2291, 369, 847, 6731, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,123 - vllm.entrypoints.logger - INFO - Received request cmpl-41707950c6c248fab0b1b2afee2a1918-0: prompt: 'Hey, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,145 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-231d71ef6d894fb09e5b7f442640ff9e-0.
2025-12-19 23:59:05,147 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-22cd3db7f41d42bcaa784ce0db106f1c-0.
2025-12-19 23:59:05,148 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-41707950c6c248fab0b1b2afee2a1918-0.
2025-12-19 23:59:05,149 - vllm.entrypoints.logger - INFO - Received request cmpl-0a8c1b5d52c640b6a1349217a918d20a-0: prompt: 'I need to talk to you about something. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1184, 311, 3061, 311, 498, 911, 2494, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,149 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:05,157 - vllm.entrypoints.logger - INFO - Received request cmpl-e914587f05a64c0ca1b3977bb9338cb8-0: prompt: 'Hey, B, have you seen the latest sales report?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 11, 614, 498, 3884, 279, 5535, 6625, 1895, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,184 - vllm.entrypoints.logger - INFO - Received request cmpl-965c8cdee0264146b74f839d7920f7f2-0: prompt: 'Hey Mentor, I ran into my old high school classmate, Sarah.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 10613, 1119, 847, 2310, 1550, 2906, 536, 18052, 11, 20445, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,193 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0a8c1b5d52c640b6a1349217a918d20a-0.
2025-12-19 23:59:05,194 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e914587f05a64c0ca1b3977bb9338cb8-0.
2025-12-19 23:59:05,196 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-965c8cdee0264146b74f839d7920f7f2-0.
2025-12-19 23:59:05,197 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:05,234 - vllm.entrypoints.logger - INFO - Received request cmpl-39be04e2de3c428696f10b3be5da70f7-0: prompt: 'I was feeling really down yesterday. School is getting so stressful.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 8266, 2167, 1495, 13671, 13, 6022, 374, 3709, 773, 45783, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,240 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-39be04e2de3c428696f10b3be5da70f7-0.
2025-12-19 23:59:05,242 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:05,257 - vllm.entrypoints.logger - INFO - Received request cmpl-9398a1c1a7f74d2bb6cc035706037ee6-0: prompt: "I can't believe he would cheat on me like that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 566, 1035, 40768, 389, 752, 1075, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,265 - vllm.entrypoints.logger - INFO - Received request cmpl-202298a2f6bc4f2092cf00190446ac96-0: prompt: 'That was a good movie. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4792, 572, 264, 1661, 5700, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,275 - vllm.entrypoints.logger - INFO - Received request cmpl-ddd77e9cd7da422895773b9aea1a8f52-0: prompt: "This is ridiculous. I can't believe I'm being sued for greed.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 26775, 13, 358, 646, 944, 4411, 358, 2776, 1660, 41084, 369, 55826, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,286 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9398a1c1a7f74d2bb6cc035706037ee6-0.
2025-12-19 23:59:05,287 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-202298a2f6bc4f2092cf00190446ac96-0.
2025-12-19 23:59:05,288 - vllm.entrypoints.logger - INFO - Received request cmpl-543b8bf0554e4fb1b23e8bef5250c434-0: prompt: " I miss talking to Classmates B. It's not the same without him around.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 3116, 7404, 311, 3228, 16457, 425, 13, 1084, 594, 537, 279, 1852, 2041, 1435, 2163, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,288 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ddd77e9cd7da422895773b9aea1a8f52-0.
2025-12-19 23:59:05,289 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-543b8bf0554e4fb1b23e8bef5250c434-0.
2025-12-19 23:59:05,291 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:05,303 - vllm.entrypoints.logger - INFO - Received request cmpl-ab4ee2c0f57d4c15923069be462c5323-0: prompt: "Hey, sorry about what I said the other day. I didn't mean to bring you down like that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 14589, 911, 1128, 358, 1053, 279, 1008, 1899, 13, 358, 3207, 944, 3076, 311, 4446, 498, 1495, 1075, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,310 - vllm.entrypoints.logger - INFO - Received request cmpl-ce701393f0924a27ba03fc2124296a7a-0: prompt: 'Mentor, I joined a beginning running class today. I am excited to start training and be able to run for more than five minutes without getting winded.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 10859, 264, 7167, 4303, 536, 3351, 13, 358, 1079, 12035, 311, 1191, 4862, 323, 387, 2952, 311, 1598, 369, 803, 1091, 4236, 4420, 2041, 3709, 9956, 291, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,322 - vllm.entrypoints.logger - INFO - Received request cmpl-5a5bba21fa684c20abb972e7f0bc3973-0: prompt: 'Hi there, Neighbor B. How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 97163, 425, 13, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,324 - vllm.entrypoints.logger - INFO - Received request cmpl-918d7a6e29a84d2d8ed0a3cc9f475c74-0: prompt: "Coach, I'm really excited about this new project I'm working on at school.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 2776, 2167, 12035, 911, 419, 501, 2390, 358, 2776, 3238, 389, 518, 2906, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,334 - vllm.entrypoints.logger - INFO - Received request cmpl-2983a35fc7064746af4650487ba4e76e-0: prompt: 'I believe that talking to the person involved is the best way to resolve an issue.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 4411, 429, 7404, 311, 279, 1697, 6398, 374, 279, 1850, 1616, 311, 8830, 458, 4265, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,335 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ab4ee2c0f57d4c15923069be462c5323-0.
2025-12-19 23:59:05,336 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ce701393f0924a27ba03fc2124296a7a-0.
2025-12-19 23:59:05,337 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5a5bba21fa684c20abb972e7f0bc3973-0.
2025-12-19 23:59:05,338 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-918d7a6e29a84d2d8ed0a3cc9f475c74-0.
2025-12-19 23:59:05,339 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2983a35fc7064746af4650487ba4e76e-0.
2025-12-19 23:59:05,341 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:05,385 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:05,404 - vllm.entrypoints.logger - INFO - Received request cmpl-5536cc9fa3884677875c17076816b340-0: prompt: 'Hey, did I tell you about the girl I met in the park last weekend?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 911, 279, 3743, 358, 2270, 304, 279, 6118, 1537, 9001, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,413 - vllm.entrypoints.logger - INFO - Received request cmpl-1e9b019f5ea84de98db54ab5253a5ef5-0: prompt: "Hey Coach, I've got some news to share with you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 27176, 11, 358, 3003, 2684, 1045, 3669, 311, 4332, 448, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,431 - vllm.entrypoints.logger - INFO - Received request cmpl-4358e9e5cbaf43a8919fc64309a1fda7-0: prompt: "I've been practicing deep breathing exercises lately to manage my anxiety. I'm scared that one mistake could ruin everything for me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 35566, 5538, 25938, 22932, 30345, 311, 10091, 847, 18056, 13, 358, 2776, 26115, 429, 825, 16523, 1410, 37239, 4297, 369, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,433 - vllm.entrypoints.logger - INFO - Received request cmpl-08cee12bade34aea87659b3bb4e70339-0: prompt: 'Good morning, Neighbor B. How did you sleep last night?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 97163, 425, 13, 2585, 1521, 498, 6084, 1537, 3729, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,455 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5536cc9fa3884677875c17076816b340-0.
2025-12-19 23:59:05,457 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1e9b019f5ea84de98db54ab5253a5ef5-0.
2025-12-19 23:59:05,460 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4358e9e5cbaf43a8919fc64309a1fda7-0.
2025-12-19 23:59:05,462 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-08cee12bade34aea87659b3bb4e70339-0.
2025-12-19 23:59:05,463 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:05,468 - vllm.entrypoints.logger - INFO - Received request cmpl-0511dd1b22594d759810047911348395-0: prompt: ", I'm so excited! I've been asked to sing at my friend's birthday party this weekend.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 773, 12035, 0, 358, 3003, 1012, 4588, 311, 7780, 518, 847, 4238, 594, 15198, 4614, 419, 9001, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,495 - vllm.entrypoints.logger - INFO - Received request cmpl-b373daa2712e4e078a9dcf7095596f91-0: prompt: 'Hi there, how are you today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1246, 525, 498, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,508 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0511dd1b22594d759810047911348395-0.
2025-12-19 23:59:05,509 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b373daa2712e4e078a9dcf7095596f91-0.
2025-12-19 23:59:05,510 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,522 - vllm.entrypoints.logger - INFO - Received request cmpl-a3c79f9417d242bfa26a8e80068d764d-0: prompt: 'Mentor, I have been thinking a lot about practicing safe sex. I want to protect myself from STIs and also avoid unplanned pregnancies.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 1012, 7274, 264, 2696, 911, 35566, 6092, 1839, 13, 358, 1366, 311, 6016, 7037, 504, 3928, 3872, 323, 1083, 5648, 83845, 7295, 81208, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,538 - vllm.entrypoints.logger - INFO - Received request cmpl-7907b0a9affe414fa053a2501b7553e6-0: prompt: "I always make sure to keep my room clean. It's important to have a tidy space.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 1281, 2704, 311, 2506, 847, 3054, 4240, 13, 1084, 594, 2989, 311, 614, 264, 57805, 3550, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,541 - vllm.entrypoints.logger - INFO - Received request cmpl-0eac2a60480a4dcc8ee238b3d567666a-0: prompt: "I really struggle with keeping secrets. I'm always afraid that if I tell someone something important, it'll somehow get out and I'll get in trouble.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 14651, 448, 10282, 23594, 13, 358, 2776, 2677, 16575, 429, 421, 358, 3291, 4325, 2494, 2989, 11, 432, 3278, 16925, 633, 700, 323, 358, 3278, 633, 304, 12264, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,554 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a3c79f9417d242bfa26a8e80068d764d-0.
2025-12-19 23:59:05,555 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7907b0a9affe414fa053a2501b7553e6-0.
2025-12-19 23:59:05,556 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0eac2a60480a4dcc8ee238b3d567666a-0.
2025-12-19 23:59:05,557 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:05,570 - vllm.entrypoints.logger - INFO - Received request cmpl-6b86a4a4ba3d4f99975cecd83c6831c0-0: prompt: 'Thanks for saying that, it means a lot to me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12658, 369, 5488, 429, 11, 432, 3363, 264, 2696, 311, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,588 - vllm.entrypoints.logger - INFO - Received request cmpl-5f8392083a0b40e887ec52644329de3f-0: prompt: "I still can't believe that you killed my goat. What were you thinking?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2058, 646, 944, 4411, 429, 498, 7425, 847, 53292, 13, 3555, 1033, 498, 7274, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,601 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6b86a4a4ba3d4f99975cecd83c6831c0-0.
2025-12-19 23:59:05,603 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5f8392083a0b40e887ec52644329de3f-0.
2025-12-19 23:59:05,604 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,616 - vllm.entrypoints.logger - INFO - Received request cmpl-ec7b4ceb8bd4446ebbf77d6e0bad266e-0: prompt: "Good morning, Boss. I've been reviewing some of my old stories and I think I can improve my writing skills. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 31569, 13, 358, 3003, 1012, 33888, 1045, 315, 847, 2310, 7343, 323, 358, 1744, 358, 646, 7269, 847, 4378, 7361, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,619 - vllm.entrypoints.logger - INFO - Received request cmpl-4c75418c2d8e40929962899edd96346b-0: prompt: 'Hi there, do you need help carrying your backpack?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 653, 498, 1184, 1492, 15331, 697, 33136, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,652 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ec7b4ceb8bd4446ebbf77d6e0bad266e-0.
2025-12-19 23:59:05,653 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4c75418c2d8e40929962899edd96346b-0.
2025-12-19 23:59:05,654 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,659 - vllm.entrypoints.logger - INFO - Received request cmpl-3b712d9bf35c4d00a0aa0253b8cb5fea-0: prompt: "I can't believe personY thinks they know everything. They're just so arrogant and narrow-minded.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1697, 56, 15482, 807, 1414, 4297, 13, 2379, 2299, 1101, 773, 65368, 323, 15026, 33323, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,660 - vllm.entrypoints.logger - INFO - Received request cmpl-eb2ee504bcd14c8ca369027a53ad9ab4-0: prompt: 'Hi there! Fancy seeing you here.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 0, 81799, 9120, 498, 1588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,696 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3b712d9bf35c4d00a0aa0253b8cb5fea-0.
2025-12-19 23:59:05,698 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eb2ee504bcd14c8ca369027a53ad9ab4-0.
2025-12-19 23:59:05,699 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,740 - vllm.entrypoints.logger - INFO - Received request cmpl-4a68ae0d74c141af9aa954135e085a2b-0: prompt: ', here are your glasses.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 1588, 525, 697, 28147, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,742 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:05,743 - vllm.entrypoints.logger - INFO - Received request cmpl-01b8f7a0b2cc4b39b3af8b3b4e80a2b8-0: prompt: "Hey, sorry to interrupt, but I'm not feeling well. I think I need to leave the meeting early.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 14589, 311, 12667, 11, 714, 358, 2776, 537, 8266, 1632, 13, 358, 1744, 358, 1184, 311, 5274, 279, 6438, 4124, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,745 - vllm.entrypoints.logger - INFO - Received request cmpl-430f085d1f6d449e9d1272bbf394ef42-0: prompt: 'Hey, are you excited for the game later?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 525, 498, 12035, 369, 279, 1809, 2937, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,753 - vllm.entrypoints.logger - INFO - Received request cmpl-d3526d8513364b13b3f68706e80ebb4d-0: prompt: 'Neighbors B, I noticed that you seem to be struggling lately. Is everything alright?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [58086, 425, 11, 358, 13686, 429, 498, 2803, 311, 387, 19962, 30345, 13, 2160, 4297, 50117, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,757 - vllm.entrypoints.logger - INFO - Received request cmpl-a7aa683053824ec8a2a89bd5f8d23304-0: prompt: "Have you ever noticed how people always seem to be curious about what I'm up to?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 3512, 13686, 1246, 1251, 2677, 2803, 311, 387, 22208, 911, 1128, 358, 2776, 705, 311, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,762 - vllm.entrypoints.logger - INFO - Received request cmpl-5029bed5ef8e4ff38086392990fdbf50-0: prompt: "I hope you won't judge me for what I'm about to say.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3900, 498, 2765, 944, 11651, 752, 369, 1128, 358, 2776, 911, 311, 1977, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,784 - vllm.entrypoints.logger - INFO - Received request cmpl-d3ed09b1364442efa75aa4d72f546d43-0: prompt: "Hey, I notice you're smiling a lot today. Did something good happen?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 5293, 498, 2299, 36063, 264, 2696, 3351, 13, 14568, 2494, 1661, 3537, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,797 - vllm.entrypoints.logger - INFO - Received request cmpl-917d4abbb3ca4bdf9b5b45c3f8e8d413-0: prompt: "I protested yesterday while wearing a mask. I don't think the government is doing enough to protect people from the virus.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 69405, 13671, 1393, 12233, 264, 6911, 13, 358, 1513, 944, 1744, 279, 3033, 374, 3730, 3322, 311, 6016, 1251, 504, 279, 16770, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,802 - vllm.entrypoints.logger - INFO - Received request cmpl-1084fc5297e64155a1ef092c1901fab3-0: prompt: ' My children are growing up so fast. My son is 8 years old and my daughter is 6 years old.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3017, 2841, 525, 7826, 705, 773, 4937, 13, 3017, 4438, 374, 220, 23, 1635, 2310, 323, 847, 9803, 374, 220, 21, 1635, 2310, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,804 - vllm.entrypoints.logger - INFO - Received request cmpl-65055a0e561047658209ac186725b3eb-0: prompt: "Mentor, I have some exciting news! I've decided to start a catering service.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 1045, 13245, 3669, 0, 358, 3003, 6635, 311, 1191, 264, 53829, 2473, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,815 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4a68ae0d74c141af9aa954135e085a2b-0.
2025-12-19 23:59:05,816 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-01b8f7a0b2cc4b39b3af8b3b4e80a2b8-0.
2025-12-19 23:59:05,817 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-430f085d1f6d449e9d1272bbf394ef42-0.
2025-12-19 23:59:05,818 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d3526d8513364b13b3f68706e80ebb4d-0.
2025-12-19 23:59:05,819 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a7aa683053824ec8a2a89bd5f8d23304-0.
2025-12-19 23:59:05,820 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5029bed5ef8e4ff38086392990fdbf50-0.
2025-12-19 23:59:05,821 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d3ed09b1364442efa75aa4d72f546d43-0.
2025-12-19 23:59:05,822 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-917d4abbb3ca4bdf9b5b45c3f8e8d413-0.
2025-12-19 23:59:05,823 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1084fc5297e64155a1ef092c1901fab3-0.
2025-12-19 23:59:05,824 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-65055a0e561047658209ac186725b3eb-0.
2025-12-19 23:59:05,825 - vllm.core.scheduler - INFO - Pending queue size: (10)
2025-12-19 23:59:05,828 - vllm.entrypoints.logger - INFO - Received request cmpl-a4a32020027b41c9909485de7e7ad77f-0: prompt: 'So, I heard about this guy who stood up to his boss and got fired.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 358, 6617, 911, 419, 7412, 879, 14638, 705, 311, 806, 13392, 323, 2684, 13895, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,865 - vllm.entrypoints.logger - INFO - Received request cmpl-a161d827a9344e8daaa96c893e8e6319-0: prompt: 'I went bungee jumping over the weekend, it was so much fun!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3937, 293, 13884, 68, 29002, 916, 279, 9001, 11, 432, 572, 773, 1753, 2464, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,869 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a4a32020027b41c9909485de7e7ad77f-0.
2025-12-19 23:59:05,871 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a161d827a9344e8daaa96c893e8e6319-0.
2025-12-19 23:59:05,872 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,888 - vllm.entrypoints.logger - INFO - Received request cmpl-a1894687adf74de6a12fd167625ee0ff-0: prompt: "Hi Doctor, it's been a while since my last appointment.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 18635, 11, 432, 594, 1012, 264, 1393, 2474, 847, 1537, 17635, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,903 - vllm.entrypoints.logger - INFO - Received request cmpl-6fdd39ece9a549348a3e70d0983c0e9d-0: prompt: 'Hey, sorry I forgot to tell you about the surprise party last night.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 14589, 358, 28595, 311, 3291, 498, 911, 279, 12761, 4614, 1537, 3729, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,916 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a1894687adf74de6a12fd167625ee0ff-0.
2025-12-19 23:59:05,918 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6fdd39ece9a549348a3e70d0983c0e9d-0.
2025-12-19 23:59:05,920 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:05,921 - vllm.entrypoints.logger - INFO - Received request cmpl-b78c6e090edc4f99bd078bf0ed06c411-0: prompt: 'What happened, Mentee? You dropped your rifle and it fired?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=34, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3838, 6932, 11, 48593, 2127, 30, 1446, 12226, 697, 28132, 323, 432, 13895, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,924 - vllm.entrypoints.logger - INFO - Received request cmpl-3bcc06c5e5214ffcadddd21839265f1f-0: prompt: 'You know, every day when I wake up, I feel grateful for having you as my wife.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 1449, 1899, 979, 358, 15156, 705, 11, 358, 2666, 25195, 369, 3432, 498, 438, 847, 7403, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,943 - vllm.entrypoints.logger - INFO - Received request cmpl-a35cd1fbc79646a0a269d0dbe7f58d53-0: prompt: 'Hi Mentor, I just got back from my trip and wanted to tell you how smoothly everything went with packing my suitcase.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 1101, 2684, 1182, 504, 847, 8411, 323, 4829, 311, 3291, 498, 1246, 38411, 4297, 3937, 448, 35713, 847, 87849, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,964 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b78c6e090edc4f99bd078bf0ed06c411-0.
2025-12-19 23:59:05,965 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3bcc06c5e5214ffcadddd21839265f1f-0.
2025-12-19 23:59:05,966 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a35cd1fbc79646a0a269d0dbe7f58d53-0.
2025-12-19 23:59:05,967 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:05,976 - vllm.entrypoints.logger - INFO - Received request cmpl-85d48e32cbb2449ba81d9bab721498a5-0: prompt: " I'm just so frustrated. I feel like I'm never listened to in our group discussions.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 1101, 773, 32530, 13, 358, 2666, 1075, 358, 2776, 2581, 33693, 311, 304, 1039, 1874, 20333, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:05,983 - vllm.entrypoints.logger - INFO - Received request cmpl-9b0e8cb5039f4119943c0cea13573a83-0: prompt: "Hey, can I talk to you about something that's been bothering me?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 358, 3061, 311, 498, 911, 2494, 429, 594, 1012, 90159, 752, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,003 - vllm.entrypoints.logger - INFO - Received request cmpl-8f20397a951e41ed88a486d88a24522d-0: prompt: 'Today we need to discuss the preparations for the school festival. We have to make sure everything is organized and everyone knows their role.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 582, 1184, 311, 4263, 279, 46879, 369, 279, 2906, 18780, 13, 1205, 614, 311, 1281, 2704, 4297, 374, 16645, 323, 5019, 8788, 862, 3476, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,011 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-85d48e32cbb2449ba81d9bab721498a5-0.
2025-12-19 23:59:06,014 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9b0e8cb5039f4119943c0cea13573a83-0.
2025-12-19 23:59:06,015 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8f20397a951e41ed88a486d88a24522d-0.
2025-12-19 23:59:06,015 - vllm.entrypoints.logger - INFO - Received request cmpl-07e436ef88634ba29c3c61941c7c2bad-0: prompt: 'Hi, Doctor. I wanted to talk to you about something that has been bothering me for a while.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 18635, 13, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 702, 1012, 90159, 752, 369, 264, 1393, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,016 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:06,017 - vllm.entrypoints.logger - INFO - Received request cmpl-6f75c5b90c2c4658a668849c083b46c0-0: prompt: 'Hi, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,042 - vllm.entrypoints.logger - INFO - Received request cmpl-d7ce3c0a9aae4d7b8d9a1068dd80278d-0: prompt: 'Here, take this cigarette. You always look so happy when you smoke.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8420, 11, 1896, 419, 35113, 13, 1446, 2677, 1401, 773, 6247, 979, 498, 16205, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,060 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-07e436ef88634ba29c3c61941c7c2bad-0.
2025-12-19 23:59:06,060 - vllm.entrypoints.logger - INFO - Received request cmpl-04a8b1771441419ab8f2103ec645eee9-0: prompt: 'I really wish I could be more confident in social situations. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 6426, 358, 1410, 387, 803, 16506, 304, 3590, 14740, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,062 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f75c5b90c2c4658a668849c083b46c0-0.
2025-12-19 23:59:06,063 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d7ce3c0a9aae4d7b8d9a1068dd80278d-0.
2025-12-19 23:59:06,064 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-04a8b1771441419ab8f2103ec645eee9-0.
2025-12-19 23:59:06,065 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:06,084 - vllm.entrypoints.logger - INFO - Received request cmpl-2dded4793f1143bab309c6d5ced53f29-0: prompt: 'Hey Mentor, I just printed out a document for my meeting with my boss later today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 1101, 16709, 700, 264, 2197, 369, 847, 6438, 448, 847, 13392, 2937, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,100 - vllm.entrypoints.logger - INFO - Received request cmpl-73f7e41ad2c14fa69692f6ed4c72e1e8-0: prompt: 'I love the winter season so much! The snow just makes everything look so beautiful.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 279, 12406, 3200, 773, 1753, 0, 576, 11794, 1101, 3643, 4297, 1401, 773, 6233, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,105 - vllm.entrypoints.logger - INFO - Received request cmpl-49ef8d552b7747cbbf5e969a96b081f3-0: prompt: ", I can't believe it. My best friend has been cheating on me! I don't know what to do.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 646, 944, 4411, 432, 13, 3017, 1850, 4238, 702, 1012, 41723, 389, 752, 0, 358, 1513, 944, 1414, 1128, 311, 653, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,110 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2dded4793f1143bab309c6d5ced53f29-0.
2025-12-19 23:59:06,111 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-73f7e41ad2c14fa69692f6ed4c72e1e8-0.
2025-12-19 23:59:06,112 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-49ef8d552b7747cbbf5e969a96b081f3-0.
2025-12-19 23:59:06,114 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:06,118 - vllm.entrypoints.logger - INFO - Received request cmpl-df066aaccb054cb1bd61901ce23efd79-0: prompt: 'Can you believe it? A tree just fell on my car!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 362, 4916, 1101, 11052, 389, 847, 1803, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,157 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-df066aaccb054cb1bd61901ce23efd79-0.
2025-12-19 23:59:06,159 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:06,164 - vllm.entrypoints.logger - INFO - Received request cmpl-0af1ab7a69754a948dc8db8cbf14564b-0: prompt: ', look at my new clothes! I went to the store and got a whole new wardrobe.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 1401, 518, 847, 501, 15097, 0, 358, 3937, 311, 279, 3553, 323, 2684, 264, 4361, 501, 45746, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,173 - vllm.entrypoints.logger - INFO - Received request cmpl-ce71714b5f824920b937bdce1555f8d3-0: prompt: ", I can't believe how much progress we've made at the gym. I feel so much better already.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 646, 944, 4411, 1246, 1753, 5098, 582, 3003, 1865, 518, 279, 18813, 13, 358, 2666, 773, 1753, 2664, 2669, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,202 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0af1ab7a69754a948dc8db8cbf14564b-0.
2025-12-19 23:59:06,204 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ce71714b5f824920b937bdce1555f8d3-0.
2025-12-19 23:59:06,205 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:06,218 - vllm.entrypoints.logger - INFO - Received request cmpl-92a00f6760744460a55ec809a1a1b10d-0: prompt: 'Good morning Boss. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 31569, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,249 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-92a00f6760744460a55ec809a1a1b10d-0.
2025-12-19 23:59:06,250 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:06,284 - vllm.entrypoints.logger - INFO - Received request cmpl-b8d60af3b53e4f09a7abb1c0d901dcf4-0: prompt: "I really love playing Bingo. I've been playing it for years now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 2948, 5619, 92048, 13, 358, 3003, 1012, 5619, 432, 369, 1635, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,293 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b8d60af3b53e4f09a7abb1c0d901dcf4-0.
2025-12-19 23:59:06,295 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:06,298 - vllm.entrypoints.logger - INFO - Received request cmpl-f547e1d1aa5340a891270bd173fae0f4-0: prompt: 'Hey, give me your pen!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 2968, 752, 697, 5750, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,302 - vllm.entrypoints.logger - INFO - Received request cmpl-ed22b11e094a47ebaba180ca53806b50-0: prompt: ", you won't believe what I found in my grandparents' attic! ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 498, 2765, 944, 4411, 1128, 358, 1730, 304, 847, 55335, 6, 73621, 0, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,312 - vllm.entrypoints.logger - INFO - Received request cmpl-09c4014df6da44c5acdb587de78170df-0: prompt: "I always make sure to be polite and careful around servers. It's just common courtesy, you know?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 1281, 2704, 311, 387, 47787, 323, 16585, 2163, 16293, 13, 1084, 594, 1101, 4185, 26013, 11, 498, 1414, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,323 - vllm.entrypoints.logger - INFO - Received request cmpl-04cc18ed337a44e8b75b86f7db87819c-0: prompt: 'Mom, I know you have reservations about me going to the party, but I really think I should be able to go.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [58783, 11, 358, 1414, 498, 614, 40062, 911, 752, 2087, 311, 279, 4614, 11, 714, 358, 2167, 1744, 358, 1265, 387, 2952, 311, 728, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,339 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f547e1d1aa5340a891270bd173fae0f4-0.
2025-12-19 23:59:06,341 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ed22b11e094a47ebaba180ca53806b50-0.
2025-12-19 23:59:06,342 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-09c4014df6da44c5acdb587de78170df-0.
2025-12-19 23:59:06,345 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-04cc18ed337a44e8b75b86f7db87819c-0.
2025-12-19 23:59:06,347 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:06,347 - vllm.entrypoints.logger - INFO - Received request cmpl-0910ce720a054ba0884c7bb0e7ef92d2-0: prompt: "I can't believe he did that! How could he be so thoughtless?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 566, 1521, 429, 0, 2585, 1410, 566, 387, 773, 3381, 1717, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,358 - vllm.entrypoints.logger - INFO - Received request cmpl-e78c616de5ac483db12f3269e604b557-0: prompt: " I can't believe I've been studying for over six hours straight. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 646, 944, 4411, 358, 3003, 1012, 20956, 369, 916, 4743, 4115, 7678, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,374 - vllm.entrypoints.logger - INFO - Received request cmpl-b846dcd442a8488fbdd69c31bffa2cda-0: prompt: " I'm really enjoying cooking healthy meals lately. It makes me feel good knowing I'm nourishing my body with the right foods.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 2167, 21413, 17233, 9314, 20969, 30345, 13, 1084, 3643, 752, 2666, 1661, 14063, 358, 2776, 45698, 10976, 847, 2487, 448, 279, 1290, 15298, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,377 - vllm.entrypoints.logger - INFO - Received request cmpl-e9f20ddff34347a48c89b06f73cf095a-0: prompt: "Hey! How's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 0, 2585, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,391 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0910ce720a054ba0884c7bb0e7ef92d2-0.
2025-12-19 23:59:06,392 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e78c616de5ac483db12f3269e604b557-0.
2025-12-19 23:59:06,393 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b846dcd442a8488fbdd69c31bffa2cda-0.
2025-12-19 23:59:06,395 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e9f20ddff34347a48c89b06f73cf095a-0.
2025-12-19 23:59:06,396 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:06,408 - vllm.entrypoints.logger - INFO - Received request cmpl-4cc5daf97b144a628252418e003fc034-0: prompt: '*sighs contentedly*', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [33454, 1090, 82, 2213, 52323, 9], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,421 - vllm.entrypoints.logger - INFO - Received request cmpl-410920742824486d9678e0b99770c8ac-0: prompt: 'Look at this photo of my family from when I was a kid. We were so happy back then.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10380, 518, 419, 6548, 315, 847, 2997, 504, 979, 358, 572, 264, 10369, 13, 1205, 1033, 773, 6247, 1182, 1221, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,424 - vllm.entrypoints.logger - INFO - Received request cmpl-22a6eb5f8e3643579d495fc23ccd7209-0: prompt: 'Want to play some chess with me?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [28823, 311, 1486, 1045, 32719, 448, 752, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,431 - vllm.entrypoints.logger - INFO - Received request cmpl-a294f8590a304d468ed0ca8f77a61f43-0: prompt: 'Hey, have you ever read anything by this author before?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 1349, 4113, 553, 419, 3150, 1573, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,440 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4cc5daf97b144a628252418e003fc034-0.
2025-12-19 23:59:06,441 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-410920742824486d9678e0b99770c8ac-0.
2025-12-19 23:59:06,442 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-22a6eb5f8e3643579d495fc23ccd7209-0.
2025-12-19 23:59:06,443 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a294f8590a304d468ed0ca8f77a61f43-0.
2025-12-19 23:59:06,445 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:06,457 - vllm.entrypoints.logger - INFO - Received request cmpl-bdfbdaa80b2141a89c6426b0c0f077fb-0: prompt: 'I love spending time with you, Wife. You know that, right?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 10164, 882, 448, 498, 11, 42408, 13, 1446, 1414, 429, 11, 1290, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,472 - vllm.entrypoints.logger - INFO - Received request cmpl-81ed7a34bf214d3a945097f3c3e3c406-0: prompt: 'Good morning, neighbor! How are you today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 9565, 0, 2585, 525, 498, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,485 - vllm.entrypoints.logger - INFO - Received request cmpl-f6f72f39a19a43ff812a0b2885f2f974-0: prompt: 'When I was a child, I always loved taking things apart to see how they worked. It was fascinating to me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4498, 358, 572, 264, 1682, 11, 358, 2677, 10245, 4633, 2513, 10747, 311, 1490, 1246, 807, 6439, 13, 1084, 572, 26291, 311, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,488 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bdfbdaa80b2141a89c6426b0c0f077fb-0.
2025-12-19 23:59:06,490 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-81ed7a34bf214d3a945097f3c3e3c406-0.
2025-12-19 23:59:06,491 - vllm.entrypoints.logger - INFO - Received request cmpl-2399a85146bd4380b5f3af721ef93c90-0: prompt: 'Hey, it was so much fun hanging out at the bar with you yesterday!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 432, 572, 773, 1753, 2464, 20704, 700, 518, 279, 3619, 448, 498, 13671, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,491 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f6f72f39a19a43ff812a0b2885f2f974-0.
2025-12-19 23:59:06,492 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:06,494 - vllm.entrypoints.logger - INFO - Received request cmpl-25db09b28b18415abe4bcca104ecb7a5-0: prompt: 'I had a really great day at the mall today. I found a new shirt that fits me perfectly.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 264, 2167, 2244, 1899, 518, 279, 33253, 3351, 13, 358, 1730, 264, 501, 15478, 429, 18304, 752, 13942, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,503 - vllm.entrypoints.logger - INFO - Received request cmpl-b3fdf1eb23174729baadf671c2ca6537-0: prompt: "Hey there, Neighbors B! I just wanted to say that I'm really proud of you for organizing that community event last weekend. You did a really good job.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 4182, 24101, 425, 0, 358, 1101, 4829, 311, 1977, 429, 358, 2776, 2167, 12409, 315, 498, 369, 34721, 429, 3942, 1538, 1537, 9001, 13, 1446, 1521, 264, 2167, 1661, 2618, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,511 - vllm.entrypoints.logger - INFO - Received request cmpl-8e73bf8329e64203bc9c936f202fae15-0: prompt: 'This year, Im going to start working out three times a week and eating healthier. I want to be able to fit into my skinny jeans again and have more energy throughout the day.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 1042, 11, 358, 4249, 2087, 311, 1191, 3238, 700, 2326, 3039, 264, 2003, 323, 12182, 38245, 13, 358, 1366, 311, 387, 2952, 311, 4946, 1119, 847, 47677, 33289, 1549, 323, 614, 803, 4802, 6814, 279, 1899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,512 - vllm.entrypoints.logger - INFO - Received request cmpl-90a4fd6243b245b8ab21c55112253621-0: prompt: 'You know, I was facing a tough situation yesterday but I talked to Person Y about it. They gave me some really good advice and it helped a lot.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 572, 12880, 264, 11045, 6534, 13671, 714, 358, 14897, 311, 7357, 809, 911, 432, 13, 2379, 6551, 752, 1045, 2167, 1661, 9462, 323, 432, 8910, 264, 2696, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,531 - vllm.entrypoints.logger - INFO - Received request cmpl-692d2859b25c414fba497870521edb90-0: prompt: "I can't believe what happened today. I was walking home from the store and Neighbors B approached me and demanded my money.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1128, 6932, 3351, 13, 358, 572, 11435, 2114, 504, 279, 3553, 323, 4182, 24101, 425, 24706, 752, 323, 29426, 847, 3220, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,536 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2399a85146bd4380b5f3af721ef93c90-0.
2025-12-19 23:59:06,538 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-25db09b28b18415abe4bcca104ecb7a5-0.
2025-12-19 23:59:06,538 - vllm.entrypoints.logger - INFO - Received request cmpl-517f62225de84a758c905648cf53fef3-0: prompt: 'Hey, can I talk to you about something?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 358, 3061, 311, 498, 911, 2494, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,539 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b3fdf1eb23174729baadf671c2ca6537-0.
2025-12-19 23:59:06,540 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8e73bf8329e64203bc9c936f202fae15-0.
2025-12-19 23:59:06,540 - vllm.entrypoints.logger - INFO - Received request cmpl-4fd40112fb6745bb8374c23bd5085a89-0: prompt: "I can't believe the deal fell through. I was so excited about it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 279, 3484, 11052, 1526, 13, 358, 572, 773, 12035, 911, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,541 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-90a4fd6243b245b8ab21c55112253621-0.
2025-12-19 23:59:06,542 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-692d2859b25c414fba497870521edb90-0.
2025-12-19 23:59:06,543 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-517f62225de84a758c905648cf53fef3-0.
2025-12-19 23:59:06,544 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4fd40112fb6745bb8374c23bd5085a89-0.
2025-12-19 23:59:06,545 - vllm.core.scheduler - INFO - Pending queue size: (8)
2025-12-19 23:59:06,547 - vllm.entrypoints.logger - INFO - Received request cmpl-20ef1878edc34b8b994723553fcbc337-0: prompt: "I can't wait for the next sailing competition.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 369, 279, 1790, 50029, 10707, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,551 - vllm.entrypoints.logger - INFO - Received request cmpl-f2991aaf8d6c4923ab759d2cc59c654d-0: prompt: 'Hey, can I talk to you for a second?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 358, 3061, 311, 498, 369, 264, 2086, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,568 - vllm.entrypoints.logger - INFO - Received request cmpl-1af8e447f6d64d689ce29b40385fb2ea-0: prompt: 'Can you believe it? I saw the future, and a big storm was coming to our town. I warned everyone to evacuate, and thankfully they all got out in time.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 5485, 279, 3853, 11, 323, 264, 2409, 13458, 572, 5001, 311, 1039, 6290, 13, 358, 18683, 5019, 311, 88989, 11, 323, 75985, 807, 678, 2684, 700, 304, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,579 - vllm.entrypoints.logger - INFO - Received request cmpl-156126649d064730b440b6afe84a1239-0: prompt: 'Hey, can I talk to you about something?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 358, 3061, 311, 498, 911, 2494, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,590 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-20ef1878edc34b8b994723553fcbc337-0.
2025-12-19 23:59:06,592 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f2991aaf8d6c4923ab759d2cc59c654d-0.
2025-12-19 23:59:06,593 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1af8e447f6d64d689ce29b40385fb2ea-0.
2025-12-19 23:59:06,594 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-156126649d064730b440b6afe84a1239-0.
2025-12-19 23:59:06,595 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:06,607 - vllm.entrypoints.logger - INFO - Received request cmpl-9a41e74884e844d1a902738a664c2829-0: prompt: "I'm really passionate about learning new things and expanding my knowledge.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 24425, 911, 6832, 501, 2513, 323, 23175, 847, 6540, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,617 - vllm.entrypoints.logger - INFO - Received request cmpl-0c9146d80997493b821beaf6f2dc44c5-0: prompt: "Hi Mentor, I'm not feeling well. I have a bad cold and I just feel awful.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 2776, 537, 8266, 1632, 13, 358, 614, 264, 3873, 9255, 323, 358, 1101, 2666, 24607, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,620 - vllm.entrypoints.logger - INFO - Received request cmpl-80e130b4fe3848d5a3eae1f404e57a57-0: prompt: 'Hey, did you get the membership to the country club that I sent you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 633, 279, 15871, 311, 279, 3146, 6335, 429, 358, 3208, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,622 - vllm.entrypoints.logger - INFO - Received request cmpl-8cae22dbe4ba4aa8b00837f544b2b127-0: prompt: "I'm just feeling really down today, you know?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 1101, 8266, 2167, 1495, 3351, 11, 498, 1414, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,637 - vllm.entrypoints.logger - INFO - Received request cmpl-0ba751d5a1b64e2e9a95ba9d0259334b-0: prompt: 'Hey, have you finished writing that story we were working on last week?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 8060, 4378, 429, 3364, 582, 1033, 3238, 389, 1537, 2003, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,640 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9a41e74884e844d1a902738a664c2829-0.
2025-12-19 23:59:06,641 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0c9146d80997493b821beaf6f2dc44c5-0.
2025-12-19 23:59:06,642 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-80e130b4fe3848d5a3eae1f404e57a57-0.
2025-12-19 23:59:06,643 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8cae22dbe4ba4aa8b00837f544b2b127-0.
2025-12-19 23:59:06,644 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0ba751d5a1b64e2e9a95ba9d0259334b-0.
2025-12-19 23:59:06,645 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:06,653 - vllm.entrypoints.logger - INFO - Received request cmpl-17a6f0234aec4c0a8b5238b267c93e19-0: prompt: "I've been walking for hours and I am so tired. My feet are killing me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 11435, 369, 4115, 323, 358, 1079, 773, 19227, 13, 3017, 7541, 525, 13118, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,676 - vllm.entrypoints.logger - INFO - Received request cmpl-c63c44b7041f4a2b9793102af9559f40-0: prompt: "B, there's something that I need to tell you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [33, 11, 1052, 594, 2494, 429, 358, 1184, 311, 3291, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,690 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-17a6f0234aec4c0a8b5238b267c93e19-0.
2025-12-19 23:59:06,691 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c63c44b7041f4a2b9793102af9559f40-0.
2025-12-19 23:59:06,693 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:06,704 - vllm.entrypoints.logger - INFO - Received request cmpl-9b0f5536c6ae4fd5afdbde66482e1d77-0: prompt: " Thanks for coming to dinner, it's great to hang out and catch up with you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11114, 369, 5001, 311, 13856, 11, 432, 594, 2244, 311, 14678, 700, 323, 2287, 705, 448, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,706 - vllm.entrypoints.logger - INFO - Received request cmpl-56e3f3a308d0444b9e7175fd15946cc2-0: prompt: "Hey, how's it going, Neighbors B?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 432, 2087, 11, 4182, 24101, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,711 - vllm.entrypoints.logger - INFO - Received request cmpl-84fb41d8f1dc4592a8ac8ee646e980ff-0: prompt: 'I had so much fun watching the event yesterday. The participants were amazing!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 773, 1753, 2464, 10099, 279, 1538, 13671, 13, 576, 13026, 1033, 7897, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,720 - vllm.entrypoints.logger - INFO - Received request cmpl-f777900586544bc1a041b2737de08d03-0: prompt: 'Hey there, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,723 - vllm.entrypoints.logger - INFO - Received request cmpl-9b11f6d9532a40078cc93d7ccc43eeb5-0: prompt: 'Hey Mentor, guess what? I was given tickets to a concert last weekend!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 7942, 1128, 30, 358, 572, 2661, 14403, 311, 264, 20830, 1537, 9001, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,736 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9b0f5536c6ae4fd5afdbde66482e1d77-0.
2025-12-19 23:59:06,737 - vllm.entrypoints.logger - INFO - Received request cmpl-18c9cee9c6bd4551b1536994edef5b70-0: prompt: "Mentor, I need to talk to you about something that's been weighing on my mind for a while now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 46726, 389, 847, 3971, 369, 264, 1393, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,737 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-56e3f3a308d0444b9e7175fd15946cc2-0.
2025-12-19 23:59:06,739 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-84fb41d8f1dc4592a8ac8ee646e980ff-0.
2025-12-19 23:59:06,740 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f777900586544bc1a041b2737de08d03-0.
2025-12-19 23:59:06,741 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9b11f6d9532a40078cc93d7ccc43eeb5-0.
2025-12-19 23:59:06,742 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-18c9cee9c6bd4551b1536994edef5b70-0.
2025-12-19 23:59:06,743 - vllm.core.scheduler - INFO - Pending queue size: (6)
2025-12-19 23:59:06,744 - vllm.entrypoints.logger - INFO - Received request cmpl-6fe21713056e437d9f176f391d44c2d5-0: prompt: "Mentor, I found a new way to make money! I started recycling old materials and it's been paying off.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1730, 264, 501, 1616, 311, 1281, 3220, 0, 358, 3855, 33878, 2310, 7236, 323, 432, 594, 1012, 12515, 1007, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,764 - vllm.entrypoints.logger - INFO - Received request cmpl-de20aad3a61145b887c2195108eaa66a-0: prompt: "Hey, have you heard about TopicZ? I've been reading some articles on it and I find it really interesting.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 911, 32911, 57, 30, 358, 3003, 1012, 5290, 1045, 9709, 389, 432, 323, 358, 1477, 432, 2167, 7040, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,788 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6fe21713056e437d9f176f391d44c2d5-0.
2025-12-19 23:59:06,790 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-de20aad3a61145b887c2195108eaa66a-0.
2025-12-19 23:59:06,791 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:06,796 - vllm.entrypoints.logger - INFO - Received request cmpl-10fb98e8e7a14204a47b4fdcbdf94533-0: prompt: "Hey, what's up? I'm so bored at home, I need to go out and do something.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1128, 594, 705, 30, 358, 2776, 773, 33286, 518, 2114, 11, 358, 1184, 311, 728, 700, 323, 653, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,802 - vllm.entrypoints.logger - INFO - Received request cmpl-65938dc18efd4964af0995e55f7a8065-0: prompt: 'I really want to excel in my career and I am determined to work hard for it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 1366, 311, 24538, 304, 847, 6931, 323, 358, 1079, 10838, 311, 975, 2588, 369, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,825 - vllm.entrypoints.logger - INFO - Received request cmpl-be2cbc2f9b3b4d31a0a27ab3df6b952b-0: prompt: 'Hi, Doctor. I wanted to talk to you about something different today. I recently set up my own business and I had to take a loan from the bank to help me get started.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 18635, 13, 358, 4829, 311, 3061, 311, 498, 911, 2494, 2155, 3351, 13, 358, 5926, 738, 705, 847, 1828, 2562, 323, 358, 1030, 311, 1896, 264, 11679, 504, 279, 6073, 311, 1492, 752, 633, 3855, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,831 - vllm.entrypoints.logger - INFO - Received request cmpl-1cf16fb3da52411b869698c4d5b05b17-0: prompt: 'You know, I was thinking about how open and honest you are with your friends and family. I really admire that about you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 572, 7274, 911, 1246, 1787, 323, 10745, 498, 525, 448, 697, 4780, 323, 2997, 13, 358, 2167, 49763, 429, 911, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,832 - vllm.entrypoints.logger - INFO - Received request cmpl-43a3874cda5d48c691eb2c49e8245724-0: prompt: "Hey there! How's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 0, 2585, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,836 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-10fb98e8e7a14204a47b4fdcbdf94533-0.
2025-12-19 23:59:06,838 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-65938dc18efd4964af0995e55f7a8065-0.
2025-12-19 23:59:06,839 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-be2cbc2f9b3b4d31a0a27ab3df6b952b-0.
2025-12-19 23:59:06,840 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1cf16fb3da52411b869698c4d5b05b17-0.
2025-12-19 23:59:06,841 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-43a3874cda5d48c691eb2c49e8245724-0.
2025-12-19 23:59:06,843 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:06,849 - vllm.entrypoints.logger - INFO - Received request cmpl-0c64ddf3a4494de99a4b15bc8724da27-0: prompt: 'Did you know that Neighbors A has been ranked as one of the most livable cities in the country?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 1414, 429, 4182, 24101, 362, 702, 1012, 21006, 438, 825, 315, 279, 1429, 15120, 480, 9720, 304, 279, 3146, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,881 - vllm.entrypoints.logger - INFO - Received request cmpl-fccd4fe4f059485bb06d6f4213b6a4eb-0: prompt: 'I visited another city last week, and I was really impressed by how clean it is. There were hardly any litter on the streets.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 11994, 2441, 3283, 1537, 2003, 11, 323, 358, 572, 2167, 24404, 553, 1246, 4240, 432, 374, 13, 2619, 1033, 20171, 894, 38582, 389, 279, 14371, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,886 - vllm.entrypoints.logger - INFO - Received request cmpl-bb68f31718c44d228bb66e3d0ffe4bbf-0: prompt: "I'm so scared about the murder that happened in our neighborhood.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 26115, 911, 279, 9901, 429, 6932, 304, 1039, 12534, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,889 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0c64ddf3a4494de99a4b15bc8724da27-0.
2025-12-19 23:59:06,891 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fccd4fe4f059485bb06d6f4213b6a4eb-0.
2025-12-19 23:59:06,892 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bb68f31718c44d228bb66e3d0ffe4bbf-0.
2025-12-19 23:59:06,893 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:06,894 - vllm.entrypoints.logger - INFO - Received request cmpl-d821f4cc986f4cdfac091b2c229b8f78-0: prompt: "I can't take it anymore, teacher. The bully has been picking on me all day. I want to hit him.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 1896, 432, 14584, 11, 11079, 13, 576, 63076, 702, 1012, 21132, 389, 752, 678, 1899, 13, 358, 1366, 311, 4201, 1435, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,896 - vllm.entrypoints.logger - INFO - Received request cmpl-e1ed64386066458ea6813c98fb4ef08d-0: prompt: 'Hey, Neighbor B! How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 97163, 425, 0, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,916 - vllm.entrypoints.logger - INFO - Received request cmpl-ff7546cb893e410e88207fc98014705b-0: prompt: "I really like living in the United States. It's a beautiful country.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 1075, 5382, 304, 279, 3639, 4180, 13, 1084, 594, 264, 6233, 3146, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,922 - vllm.entrypoints.logger - INFO - Received request cmpl-ec59e5ba491a48bc8a3e22a367d39f40-0: prompt: 'Hey, Neighbors B! Did you find my little surprise in your office this morning?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 0, 14568, 498, 1477, 847, 2632, 12761, 304, 697, 5163, 419, 6556, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,937 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d821f4cc986f4cdfac091b2c229b8f78-0.
2025-12-19 23:59:06,938 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e1ed64386066458ea6813c98fb4ef08d-0.
2025-12-19 23:59:06,939 - vllm.entrypoints.logger - INFO - Received request cmpl-3aeb6a013d2b4465bedae3dd4165255d-0: prompt: "Hey, have you played this new racing game? It's so exciting!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6342, 419, 501, 21313, 1809, 30, 1084, 594, 773, 13245, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,939 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ff7546cb893e410e88207fc98014705b-0.
2025-12-19 23:59:06,941 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ec59e5ba491a48bc8a3e22a367d39f40-0.
2025-12-19 23:59:06,942 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3aeb6a013d2b4465bedae3dd4165255d-0.
2025-12-19 23:59:06,943 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:06,977 - vllm.entrypoints.logger - INFO - Received request cmpl-a9103c19dfe84c4b833890c596e332fd-0: prompt: 'Good morning! I just got back from running some errands.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 0, 358, 1101, 2684, 1182, 504, 4303, 1045, 1848, 2844, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,979 - vllm.entrypoints.logger - INFO - Received request cmpl-a9bce82c8bb4428991bb5d9d1db5144f-0: prompt: 'Hey, I wanted to tell you about something, but I forgot.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 3291, 498, 911, 2494, 11, 714, 358, 28595, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,981 - vllm.entrypoints.logger - INFO - Received request cmpl-3d7f748dd69b419392d7db273e78eab4-0: prompt: 'Hey Mentor, are you feeling cold? I noticed you shivering a little.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 525, 498, 8266, 9255, 30, 358, 13686, 498, 557, 83940, 264, 2632, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:06,988 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a9103c19dfe84c4b833890c596e332fd-0.
2025-12-19 23:59:06,989 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a9bce82c8bb4428991bb5d9d1db5144f-0.
2025-12-19 23:59:06,990 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3d7f748dd69b419392d7db273e78eab4-0.
2025-12-19 23:59:06,992 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:07,022 - vllm.entrypoints.logger - INFO - Received request cmpl-ed82ba17bc924580b2359eb06d8dd8c8-0: prompt: 'I have to say, your beauty parlor is always so clean. I really appreciate that.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 311, 1977, 11, 697, 13143, 1346, 9566, 374, 2677, 773, 4240, 13, 358, 2167, 15401, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,036 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ed82ba17bc924580b2359eb06d8dd8c8-0.
2025-12-19 23:59:07,037 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:07,037 - vllm.entrypoints.logger - INFO - Received request cmpl-07db150f94a0475a99599931c368e8e2-0: prompt: "I'm really excited about all the opportunities that are coming up for me. I've been applying to a lot of internships and I'm hoping to get one soon.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 12035, 911, 678, 279, 10488, 429, 525, 5001, 705, 369, 752, 13, 358, 3003, 1012, 18950, 311, 264, 2696, 315, 2590, 17675, 323, 358, 2776, 15652, 311, 633, 825, 5135, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,040 - vllm.entrypoints.logger - INFO - Received request cmpl-c0b5a83f026e440087b26646df687d83-0: prompt: "I can't believe I saved you from that burning building! I was so scared for a moment there.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 6781, 498, 504, 429, 19675, 4752, 0, 358, 572, 773, 26115, 369, 264, 4445, 1052, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,055 - vllm.entrypoints.logger - INFO - Received request cmpl-63bd876cf9fe4e15a87d7bc99dbc6948-0: prompt: 'Hey, have you heard the news?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 279, 3669, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,057 - vllm.entrypoints.logger - INFO - Received request cmpl-97c05d7ead2a4ce7a30ecc336c233dcf-0: prompt: "I'm really happy with how well I'm doing in school. I get good grades and I feel really smart.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 6247, 448, 1246, 1632, 358, 2776, 3730, 304, 2906, 13, 358, 633, 1661, 27611, 323, 358, 2666, 2167, 7785, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,079 - vllm.entrypoints.logger - INFO - Received request cmpl-d6a71624949f400d9633ce7fc387727a-0: prompt: "Hey, can we talk for a minute? I wanted to speak to you about something that's been bothering me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 582, 3061, 369, 264, 9383, 30, 358, 4829, 311, 6468, 311, 498, 911, 2494, 429, 594, 1012, 90159, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,081 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-07db150f94a0475a99599931c368e8e2-0.
2025-12-19 23:59:07,082 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c0b5a83f026e440087b26646df687d83-0.
2025-12-19 23:59:07,083 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-63bd876cf9fe4e15a87d7bc99dbc6948-0.
2025-12-19 23:59:07,084 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-97c05d7ead2a4ce7a30ecc336c233dcf-0.
2025-12-19 23:59:07,085 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d6a71624949f400d9633ce7fc387727a-0.
2025-12-19 23:59:07,087 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:07,115 - vllm.entrypoints.logger - INFO - Received request cmpl-ac0e2a3a67c44e46bc0578ea9ea45e30-0: prompt: "I've been doing a lot of thinking lately, and I've come to the realization that I haven't been very truthful in my life.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 3730, 264, 2696, 315, 7274, 30345, 11, 323, 358, 3003, 2525, 311, 279, 48703, 429, 358, 8990, 944, 1012, 1602, 89867, 304, 847, 2272, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,130 - vllm.entrypoints.logger - INFO - Received request cmpl-a756fdd3d61a4a20bafccb9249d1b77a-0: prompt: 'I changed my identity because I was tired of being myself.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 5497, 847, 9569, 1576, 358, 572, 19227, 315, 1660, 7037, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,133 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ac0e2a3a67c44e46bc0578ea9ea45e30-0.
2025-12-19 23:59:07,134 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a756fdd3d61a4a20bafccb9249d1b77a-0.
2025-12-19 23:59:07,136 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:07,150 - vllm.entrypoints.logger - INFO - Received request cmpl-279b5a8716154a37a41619e01b32ad26-0: prompt: "Doctor, I've been having a bit of an issue lately with my dog, Max. Every time he eats, he makes a huge mess and I constantly have to clean up after him. It's getting frustrating.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 3003, 1012, 3432, 264, 2699, 315, 458, 4265, 30345, 448, 847, 5562, 11, 7487, 13, 7209, 882, 566, 49677, 11, 566, 3643, 264, 6765, 9435, 323, 358, 14971, 614, 311, 4240, 705, 1283, 1435, 13, 1084, 594, 3709, 34611, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,154 - vllm.entrypoints.logger - INFO - Received request cmpl-381906efa8c84170bd16f0a823c408d9-0: prompt: "I can't wait to start selling my homemade jewelry. I really need to earn enough money to buy a new car.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 311, 1191, 11236, 847, 35333, 30717, 13, 358, 2167, 1184, 311, 7232, 3322, 3220, 311, 3695, 264, 501, 1803, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,179 - vllm.entrypoints.logger - INFO - Received request cmpl-100c992dd2e44feab9b59571c4ee46c8-0: prompt: "I can't believe it, I'm officially the first person in my family to finish high school!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 432, 11, 358, 2776, 18562, 279, 1156, 1697, 304, 847, 2997, 311, 6248, 1550, 2906, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,180 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-279b5a8716154a37a41619e01b32ad26-0.
2025-12-19 23:59:07,181 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-381906efa8c84170bd16f0a823c408d9-0.
2025-12-19 23:59:07,182 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-100c992dd2e44feab9b59571c4ee46c8-0.
2025-12-19 23:59:07,184 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:07,191 - vllm.entrypoints.logger - INFO - Received request cmpl-43664ccfe97e4aaead0fb0bd4ff5fbcd-0: prompt: "Mentor, I found this rock that I think is really interesting. I dug it up and it turns out it's a fossil! ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1730, 419, 6946, 429, 358, 1744, 374, 2167, 7040, 13, 358, 43020, 432, 705, 323, 432, 10577, 700, 432, 594, 264, 30276, 0, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,214 - vllm.entrypoints.logger - INFO - Received request cmpl-bc6fe9991c1545f18e623eed8145681f-0: prompt: " I can't believe my parents would threaten to cut me off financially if I don't get my grades up.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 646, 944, 4411, 847, 6562, 1035, 39150, 311, 3931, 752, 1007, 37975, 421, 358, 1513, 944, 633, 847, 27611, 705, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,215 - vllm.entrypoints.logger - INFO - Received request cmpl-4de63a4c936d4f7ca243077adbb20000-0: prompt: "I can't believe how scared I was when I had to run away from you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1246, 26115, 358, 572, 979, 358, 1030, 311, 1598, 3123, 504, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,229 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-43664ccfe97e4aaead0fb0bd4ff5fbcd-0.
2025-12-19 23:59:07,230 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc6fe9991c1545f18e623eed8145681f-0.
2025-12-19 23:59:07,231 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4de63a4c936d4f7ca243077adbb20000-0.
2025-12-19 23:59:07,233 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:07,239 - vllm.entrypoints.logger - INFO - Received request cmpl-56543f731d694687bf8a4b0ec935f731-0: prompt: 'Good morning, Mentor. Today, I woke up early and feeling really energized. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 91191, 13, 11201, 11, 358, 38726, 705, 4124, 323, 8266, 2167, 4501, 1506, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,276 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-56543f731d694687bf8a4b0ec935f731-0.
2025-12-19 23:59:07,278 - vllm.core.scheduler - INFO - Pending queue size: (1)
2025-12-19 23:59:07,299 - vllm.entrypoints.logger - INFO - Received request cmpl-bc839d07a0004cfc8e85ce85ee0ec860-0: prompt: "Ugh, I can't believe we're out of ice cream.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 646, 944, 4411, 582, 2299, 700, 315, 9853, 12644, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,302 - vllm.entrypoints.logger - INFO - Received request cmpl-d05f48512aa14b32a2dd1ab0a9993477-0: prompt: "I can't wait to get home and relax tonight.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 311, 633, 2114, 323, 11967, 17913, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,322 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc839d07a0004cfc8e85ce85ee0ec860-0.
2025-12-19 23:59:07,324 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d05f48512aa14b32a2dd1ab0a9993477-0.
2025-12-19 23:59:07,325 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:07,335 - vllm.entrypoints.logger - INFO - Received request cmpl-7996d1a3951b427badde567bf8c2d318-0: prompt: "Hi Boss, I wanted to talk to you about something that's been on my mind.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 31569, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 389, 847, 3971, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,354 - vllm.entrypoints.logger - INFO - Received request cmpl-b353b735fe964cd1876213a6ef3c0c2b-0: prompt: "I'm just so grateful that my business has been successful and I've been able to achieve my goal of becoming wealthy.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 1101, 773, 25195, 429, 847, 2562, 702, 1012, 6849, 323, 358, 3003, 1012, 2952, 311, 11075, 847, 5795, 315, 10454, 27894, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,360 - vllm.entrypoints.logger - INFO - Received request cmpl-acc231884eaa4b13924eb4eae6931891-0: prompt: 'I visited the taxidermist in town yesterday.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 11994, 279, 3742, 1776, 78924, 304, 6290, 13671, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,369 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7996d1a3951b427badde567bf8c2d318-0.
2025-12-19 23:59:07,370 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b353b735fe964cd1876213a6ef3c0c2b-0.
2025-12-19 23:59:07,372 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-acc231884eaa4b13924eb4eae6931891-0.
2025-12-19 23:59:07,373 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:07,387 - vllm.entrypoints.logger - INFO - Received request cmpl-ea65990451454513b3c7cbfcb0ce2d3b-0: prompt: ", I can't believe what I did.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 646, 944, 4411, 1128, 358, 1521, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,410 - vllm.entrypoints.logger - INFO - Received request cmpl-fc04bf8bc810424abf0d8369edb9fbd8-0: prompt: 'Wow, that PB&J sandwich was actually pretty good.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 429, 30934, 5, 41, 27874, 572, 3520, 5020, 1661, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,417 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ea65990451454513b3c7cbfcb0ce2d3b-0.
2025-12-19 23:59:07,418 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fc04bf8bc810424abf0d8369edb9fbd8-0.
2025-12-19 23:59:07,420 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:07,430 - vllm.entrypoints.logger - INFO - Received request cmpl-8c9dbfc9cdd8403b9dd37f0b2f044604-0: prompt: 'Neighbors B, can we talk about something serious?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [58086, 425, 11, 646, 582, 3061, 911, 2494, 6001, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,444 - vllm.entrypoints.logger - INFO - Received request cmpl-03e0e27a36cd45e3abd8780280ea8522-0: prompt: "I really don't like sitting on the right-hand side of the room.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 1513, 944, 1075, 11699, 389, 279, 1290, 24413, 3108, 315, 279, 3054, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,454 - vllm.entrypoints.logger - INFO - Received request cmpl-d32957807e324747bfdfa65f87f91bd7-0: prompt: "Hi Neighbors B, it's been a while since we've hung out. I was wondering if you'd like to grab lunch with me next week?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 4182, 24101, 425, 11, 432, 594, 1012, 264, 1393, 2474, 582, 3003, 18295, 700, 13, 358, 572, 20293, 421, 498, 4172, 1075, 311, 11633, 15786, 448, 752, 1790, 2003, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,457 - vllm.entrypoints.logger - INFO - Received request cmpl-74d4e71bc8e24a588ce9c924c2e04286-0: prompt: "I just wanted to let you know that I really appreciate your beauty parlor. It's always so clean and the staff is so friendly.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 4829, 311, 1077, 498, 1414, 429, 358, 2167, 15401, 697, 13143, 1346, 9566, 13, 1084, 594, 2677, 773, 4240, 323, 279, 5570, 374, 773, 11657, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,463 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8c9dbfc9cdd8403b9dd37f0b2f044604-0.
2025-12-19 23:59:07,465 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-03e0e27a36cd45e3abd8780280ea8522-0.
2025-12-19 23:59:07,466 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d32957807e324747bfdfa65f87f91bd7-0.
2025-12-19 23:59:07,467 - vllm.entrypoints.logger - INFO - Received request cmpl-8f7ffecc6fe34445ad20edabbef6dda2-0: prompt: 'Today, I spent some time sorting items in the recycling bin.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 7391, 1045, 882, 28273, 3589, 304, 279, 33878, 9544, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,467 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-74d4e71bc8e24a588ce9c924c2e04286-0.
2025-12-19 23:59:07,468 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8f7ffecc6fe34445ad20edabbef6dda2-0.
2025-12-19 23:59:07,470 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:07,473 - vllm.entrypoints.logger - INFO - Received request cmpl-e3f79a3963c4419c88a63d28d433e1e9-0: prompt: 'I always make sure to say "thank you" to my mom whenever she does something for me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 1281, 2704, 311, 1977, 330, 57417, 498, 1, 311, 847, 3368, 15356, 1340, 1558, 2494, 369, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,503 - vllm.entrypoints.logger - INFO - Received request cmpl-9cb52da5bea44a0b8e25d2877fdacfa1-0: prompt: 'Hey there, Neighbor B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 97163, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,514 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e3f79a3963c4419c88a63d28d433e1e9-0.
2025-12-19 23:59:07,518 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9cb52da5bea44a0b8e25d2877fdacfa1-0.
2025-12-19 23:59:07,519 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:07,524 - vllm.entrypoints.logger - INFO - Received request cmpl-1a153bc52acc4e18bb658dd6941fe254-0: prompt: "You know, I've realized something about myself. I think I might be jealous of other people's happiness.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 3003, 15043, 2494, 911, 7037, 13, 358, 1744, 358, 2578, 387, 40590, 315, 1008, 1251, 594, 23009, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,526 - vllm.entrypoints.logger - INFO - Received request cmpl-40b043bd5b284206977ff541387c9c63-0: prompt: 'So, remember when we used to talk about music all the time?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 6099, 979, 582, 1483, 311, 3061, 911, 4627, 678, 279, 882, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,527 - vllm.entrypoints.logger - INFO - Received request cmpl-d5469c02c5aa434e8a12c59be05a1e54-0: prompt: 'That was such a fun day! Building a snowman with you was great.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4792, 572, 1741, 264, 2464, 1899, 0, 16858, 264, 11794, 1515, 448, 498, 572, 2244, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,531 - vllm.entrypoints.logger - INFO - Received request cmpl-b5b41d3642044ced9a98abfac95043bd-0: prompt: "You know, I really love keeping you on your toes. It's fun to be mysterious.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2167, 2948, 10282, 498, 389, 697, 44613, 13, 1084, 594, 2464, 311, 387, 25382, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,536 - vllm.entrypoints.logger - INFO - Received request cmpl-15930488b9a64a44964159c7e92ed6d9-0: prompt: "I love the rain. It's so calming to watch it fall and wash away all the dirt and grime of the city.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 279, 11174, 13, 1084, 594, 773, 77629, 311, 3736, 432, 4399, 323, 11369, 3123, 678, 279, 25284, 323, 1081, 545, 315, 279, 3283, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,555 - vllm.entrypoints.logger - INFO - Received request cmpl-d78b6dd0ac634912a9ac3e93ac34e6a4-0: prompt: "Hey Mentor, I wanted to thank you for all the guidance and advice you've given me over the years.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 4829, 311, 9702, 498, 369, 678, 279, 18821, 323, 9462, 498, 3003, 2661, 752, 916, 279, 1635, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,564 - vllm.entrypoints.logger - INFO - Received request cmpl-8c9b981db3ef4a71b917dffa0e140208-0: prompt: "Boss, I wanted to share something with you that's been on my mind lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 358, 4829, 311, 4332, 2494, 448, 498, 429, 594, 1012, 389, 847, 3971, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,565 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1a153bc52acc4e18bb658dd6941fe254-0.
2025-12-19 23:59:07,567 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-40b043bd5b284206977ff541387c9c63-0.
2025-12-19 23:59:07,568 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d5469c02c5aa434e8a12c59be05a1e54-0.
2025-12-19 23:59:07,569 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b5b41d3642044ced9a98abfac95043bd-0.
2025-12-19 23:59:07,570 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-15930488b9a64a44964159c7e92ed6d9-0.
2025-12-19 23:59:07,571 - vllm.entrypoints.logger - INFO - Received request cmpl-adf513c8cce54799ba64c7f2a1ae1673-0: prompt: "Hey neighbor, I'm finally moved into my new apartment!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 358, 2776, 5499, 7726, 1119, 847, 501, 13154, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,571 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d78b6dd0ac634912a9ac3e93ac34e6a4-0.
2025-12-19 23:59:07,572 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8c9b981db3ef4a71b917dffa0e140208-0.
2025-12-19 23:59:07,573 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-adf513c8cce54799ba64c7f2a1ae1673-0.
2025-12-19 23:59:07,575 - vllm.core.scheduler - INFO - Pending queue size: (8)
2025-12-19 23:59:07,593 - vllm.entrypoints.logger - INFO - Received request cmpl-747c64a92eff4fa1950e27d385e489ff-0: prompt: "Hi, Teacher! It's great to see you again.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 29069, 0, 1084, 594, 2244, 311, 1490, 498, 1549, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,607 - vllm.entrypoints.logger - INFO - Received request cmpl-044ff419f71b490a977e8d5e487fb2c9-0: prompt: " It's been a year since Aunt Betty passed away, and I still miss her so much.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1084, 594, 1012, 264, 1042, 2474, 70222, 55011, 5823, 3123, 11, 323, 358, 2058, 3116, 1059, 773, 1753, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,619 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-747c64a92eff4fa1950e27d385e489ff-0.
2025-12-19 23:59:07,620 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-044ff419f71b490a977e8d5e487fb2c9-0.
2025-12-19 23:59:07,622 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:07,636 - vllm.entrypoints.logger - INFO - Received request cmpl-7bb1bb57b6af4067ad7c827c8047255f-0: prompt: "Coach, I've noticed that my arms and legs are looking more defined after exercising with dumbbells and kettlebells three times a week.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 3003, 13686, 429, 847, 11715, 323, 14201, 525, 3330, 803, 4512, 1283, 50482, 448, 29255, 65, 6436, 323, 72318, 65, 6436, 2326, 3039, 264, 2003, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,658 - vllm.entrypoints.logger - INFO - Received request cmpl-5332710743d840389382b2d01a3c32b8-0: prompt: "I can't believe I lost the role. It's been a tough few days.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 5558, 279, 3476, 13, 1084, 594, 1012, 264, 11045, 2421, 2849, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,667 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7bb1bb57b6af4067ad7c827c8047255f-0.
2025-12-19 23:59:07,668 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5332710743d840389382b2d01a3c32b8-0.
2025-12-19 23:59:07,670 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:07,674 - vllm.entrypoints.logger - INFO - Received request cmpl-3603461a9a26405e88b2d7889d0758b8-0: prompt: 'Hey, did I tell you about the puppy my friend gave me?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 911, 279, 41189, 847, 4238, 6551, 752, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,712 - vllm.entrypoints.logger - INFO - Received request cmpl-1683ef4e62d54acaa81758e81694d9b1-0: prompt: 'I got the part on that TV show I auditioned for!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2684, 279, 949, 389, 429, 5883, 1473, 358, 61888, 291, 369, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,713 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3603461a9a26405e88b2d7889d0758b8-0.
2025-12-19 23:59:07,714 - vllm.entrypoints.logger - INFO - Received request cmpl-31c38207fc424c1cba79a73c72ce123d-0: prompt: 'Hey, I wanted to talk to you about something serious today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 6001, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,715 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1683ef4e62d54acaa81758e81694d9b1-0.
2025-12-19 23:59:07,716 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-31c38207fc424c1cba79a73c72ce123d-0.
2025-12-19 23:59:07,718 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:07,728 - vllm.entrypoints.logger - INFO - Received request cmpl-6d83a7c6746c4ffabeae6631377a8787-0: prompt: 'Hey, have you ever heard of cryptography?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 6617, 315, 86837, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,747 - vllm.entrypoints.logger - INFO - Received request cmpl-53536768eae3461b90ee796357581580-0: prompt: "I can't wait to explore Paris! This is my dream trip.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 311, 13186, 12095, 0, 1096, 374, 847, 7904, 8411, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,753 - vllm.entrypoints.logger - INFO - Received request cmpl-20d47b0469674575b1f66e5013a9a7b9-0: prompt: "Doctor, I couldn't resist my favorite candy today. I begged my mom until she finally gave me a giant bag of Skittles.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 7691, 944, 22106, 847, 6930, 31556, 3351, 13, 358, 79292, 847, 3368, 3080, 1340, 5499, 6551, 752, 264, 14538, 8968, 315, 4818, 1442, 642, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,758 - vllm.entrypoints.logger - INFO - Received request cmpl-d9a2fdd66b9c436aac8642ce13879d2a-0: prompt: "I'm so sorry for what happened in your room. I know I messed up.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 14589, 369, 1128, 6932, 304, 697, 3054, 13, 358, 1414, 358, 64202, 705, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,764 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6d83a7c6746c4ffabeae6631377a8787-0.
2025-12-19 23:59:07,765 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-53536768eae3461b90ee796357581580-0.
2025-12-19 23:59:07,766 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-20d47b0469674575b1f66e5013a9a7b9-0.
2025-12-19 23:59:07,767 - vllm.entrypoints.logger - INFO - Received request cmpl-8be5d16d04234189bea27af4de19ec72-0: prompt: 'Hi there, are you okay? I noticed you were crying earlier.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 525, 498, 16910, 30, 358, 13686, 498, 1033, 30199, 6788, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,767 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d9a2fdd66b9c436aac8642ce13879d2a-0.
2025-12-19 23:59:07,769 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:07,781 - vllm.entrypoints.logger - INFO - Received request cmpl-d6feb783b2674f49b64799c01c792c16-0: prompt: 'Boss, I have something important to tell you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 358, 614, 2494, 2989, 311, 3291, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,783 - vllm.entrypoints.logger - INFO - Received request cmpl-075acf9898444815bff0621ddef2c80f-0: prompt: "It feels good to help someone, doesn't it?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 11074, 1661, 311, 1492, 4325, 11, 3171, 944, 432, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,786 - vllm.entrypoints.logger - INFO - Received request cmpl-e273e40b0f234256a152c2b537df1cc5-0: prompt: "I just don't understand why everything can't be done quickly. It's so frustrating!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 1513, 944, 3535, 3170, 4297, 646, 944, 387, 2814, 6157, 13, 1084, 594, 773, 34611, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,803 - vllm.entrypoints.logger - INFO - Received request cmpl-0f14d8d878114adbb0f2c93afeae5771-0: prompt: "You know, I've been thinking about your business a lot lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 3003, 1012, 7274, 911, 697, 2562, 264, 2696, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,807 - vllm.entrypoints.logger - INFO - Received request cmpl-171ccae333cb4dae8bbedab414183419-0: prompt: 'Today I had a really interesting conversation with someone who knows a lot about your favorite subject.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 358, 1030, 264, 2167, 7040, 10435, 448, 4325, 879, 8788, 264, 2696, 911, 697, 6930, 3832, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,814 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8be5d16d04234189bea27af4de19ec72-0.
2025-12-19 23:59:07,815 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d6feb783b2674f49b64799c01c792c16-0.
2025-12-19 23:59:07,816 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-075acf9898444815bff0621ddef2c80f-0.
2025-12-19 23:59:07,817 - vllm.entrypoints.logger - INFO - Received request cmpl-d42c504399c64ba79e85827b89dd98e9-0: prompt: "Boss, I wanted to share with you that I'm feeling really proud of myself lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 358, 4829, 311, 4332, 448, 498, 429, 358, 2776, 8266, 2167, 12409, 315, 7037, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,817 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e273e40b0f234256a152c2b537df1cc5-0.
2025-12-19 23:59:07,818 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0f14d8d878114adbb0f2c93afeae5771-0.
2025-12-19 23:59:07,819 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-171ccae333cb4dae8bbedab414183419-0.
2025-12-19 23:59:07,821 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d42c504399c64ba79e85827b89dd98e9-0.
2025-12-19 23:59:07,822 - vllm.core.scheduler - INFO - Pending queue size: (7)
2025-12-19 23:59:07,825 - vllm.entrypoints.logger - INFO - Received request cmpl-24531f91a991462dbb98d586aec347ee-0: prompt: "Ah, it's good to finally be off work. I'm so tired.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 432, 594, 1661, 311, 5499, 387, 1007, 975, 13, 358, 2776, 773, 19227, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,830 - vllm.entrypoints.logger - INFO - Received request cmpl-4d4bc01bc7b044eb89edd4b2a8cf337a-0: prompt: 'Hey, remember that juicy burger I had a couple of years back? I still remember how pleased it made me feel.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 6099, 429, 55053, 44623, 358, 1030, 264, 5625, 315, 1635, 1182, 30, 358, 2058, 6099, 1246, 18442, 432, 1865, 752, 2666, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,831 - vllm.entrypoints.logger - INFO - Received request cmpl-a9e71ce1d7c148c7b5419e28ece577e1-0: prompt: 'Hey, I saw you walking earlier. Where were you headed?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 5485, 498, 11435, 6788, 13, 10967, 1033, 498, 19383, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,849 - vllm.entrypoints.logger - INFO - Received request cmpl-942f18b649974a559f25baf66299899f-0: prompt: "Hey, I'm so excited to tell you that I booked my flight to Europe!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 2776, 773, 12035, 311, 3291, 498, 429, 358, 32970, 847, 10971, 311, 4505, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,853 - vllm.entrypoints.logger - INFO - Received request cmpl-28661645982549d79e03765cf2b0d71d-0: prompt: "You know, I've been thinking a lot lately about new ways to get things done.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 3003, 1012, 7274, 264, 2696, 30345, 911, 501, 5510, 311, 633, 2513, 2814, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,858 - vllm.entrypoints.logger - INFO - Received request cmpl-84689fa60f28434e887c831d235c2de3-0: prompt: "It's pouring out there today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 50413, 700, 1052, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,866 - vllm.entrypoints.logger - INFO - Received request cmpl-14f56edabb1f4c68ab76860585b76258-0: prompt: 'I noticed that the data shows an increasing trend in the number of people using our service each month.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 13686, 429, 279, 821, 4933, 458, 7703, 9149, 304, 279, 1372, 315, 1251, 1667, 1039, 2473, 1817, 2254, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,871 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-24531f91a991462dbb98d586aec347ee-0.
2025-12-19 23:59:07,872 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d4bc01bc7b044eb89edd4b2a8cf337a-0.
2025-12-19 23:59:07,873 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a9e71ce1d7c148c7b5419e28ece577e1-0.
2025-12-19 23:59:07,874 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-942f18b649974a559f25baf66299899f-0.
2025-12-19 23:59:07,876 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-28661645982549d79e03765cf2b0d71d-0.
2025-12-19 23:59:07,877 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-84689fa60f28434e887c831d235c2de3-0.
2025-12-19 23:59:07,878 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-14f56edabb1f4c68ab76860585b76258-0.
2025-12-19 23:59:07,879 - vllm.core.scheduler - INFO - Pending queue size: (7)
2025-12-19 23:59:07,898 - vllm.entrypoints.logger - INFO - Received request cmpl-c829a2da9e404fa7aa48def905b4b1df-0: prompt: "I didn't do my homework because my dog ate it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3207, 944, 653, 847, 28459, 1576, 847, 5562, 29812, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,915 - vllm.entrypoints.logger - INFO - Received request cmpl-ac6eb419ad8f479a81d44996805bc137-0: prompt: 'Good morning, Teacher!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 29069, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,918 - vllm.entrypoints.logger - INFO - Received request cmpl-6e5b3bab975d4541b261055447a261e9-0: prompt: 'Wow, that was unexpected. Is everything okay?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 429, 572, 16500, 13, 2160, 4297, 16910, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,925 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c829a2da9e404fa7aa48def905b4b1df-0.
2025-12-19 23:59:07,926 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ac6eb419ad8f479a81d44996805bc137-0.
2025-12-19 23:59:07,928 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6e5b3bab975d4541b261055447a261e9-0.
2025-12-19 23:59:07,929 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:07,934 - vllm.entrypoints.logger - INFO - Received request cmpl-6b1a8933c7df4c51b60df5964220d6df-0: prompt: 'So, what are you planning to buy at the grocery store today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 1128, 525, 498, 9115, 311, 3695, 518, 279, 29587, 3553, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,938 - vllm.entrypoints.logger - INFO - Received request cmpl-b2b7ae7001154264891c51c88b90e1ac-0: prompt: "Mentor, I've been thinking about what we discussed last time. I want happiness in my life, but I don't know how to achieve it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 3003, 1012, 7274, 911, 1128, 582, 14078, 1537, 882, 13, 358, 1366, 23009, 304, 847, 2272, 11, 714, 358, 1513, 944, 1414, 1246, 311, 11075, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,948 - vllm.entrypoints.logger - INFO - Received request cmpl-eb7185613f034b16a07192e5a994e30b-0: prompt: "I can't believe I got yelled at by my parents. I know I shouldn't have exploded like that, but Co-workers B really pushed my buttons.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 2684, 55077, 518, 553, 847, 6562, 13, 358, 1414, 358, 13133, 944, 614, 43813, 1075, 429, 11, 714, 3539, 62284, 425, 2167, 15391, 847, 12424, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,950 - vllm.entrypoints.logger - INFO - Received request cmpl-8307d390303c48e8bc9d6c4fffd25fa0-0: prompt: "Hey, how's your brother doing?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 697, 10641, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,973 - vllm.entrypoints.logger - INFO - Received request cmpl-f481ace219ff4f0daa0d4c31d01403b5-0: prompt: " I just can't believe how difficult it was to get everyone to follow my instructions at work today. It's like no one understands the importance of doing things right.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1101, 646, 944, 4411, 1246, 5000, 432, 572, 311, 633, 5019, 311, 1795, 847, 11221, 518, 975, 3351, 13, 1084, 594, 1075, 902, 825, 30769, 279, 12650, 315, 3730, 2513, 1290, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,973 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6b1a8933c7df4c51b60df5964220d6df-0.
2025-12-19 23:59:07,975 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b2b7ae7001154264891c51c88b90e1ac-0.
2025-12-19 23:59:07,976 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eb7185613f034b16a07192e5a994e30b-0.
2025-12-19 23:59:07,977 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8307d390303c48e8bc9d6c4fffd25fa0-0.
2025-12-19 23:59:07,977 - vllm.entrypoints.logger - INFO - Received request cmpl-9d78c20a329c484f93137499d2133c83-0: prompt: 'Hey, have you started your clinical rotations yet?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3855, 697, 14490, 69201, 3602, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,978 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f481ace219ff4f0daa0d4c31d01403b5-0.
2025-12-19 23:59:07,979 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9d78c20a329c484f93137499d2133c83-0.
2025-12-19 23:59:07,981 - vllm.core.scheduler - INFO - Pending queue size: (6)
2025-12-19 23:59:07,982 - vllm.entrypoints.logger - INFO - Received request cmpl-861c9dac70474586a2e8881f2fe9d863-0: prompt: 'Hey, I remember when we first became friends. Do you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 6099, 979, 582, 1156, 6116, 4780, 13, 3155, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,987 - vllm.entrypoints.logger - INFO - Received request cmpl-6ebbf941d0d842149058d4616cfecb44-0: prompt: 'Whew, that was a good workout.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1639, 365, 11, 429, 572, 264, 1661, 25242, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:07,998 - vllm.entrypoints.logger - INFO - Received request cmpl-290ffdf079a347858f5b7f8ea1a20fc0-0: prompt: 'Hey, did you know I looked up that information you were talking about last week?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 1414, 358, 6966, 705, 429, 1995, 498, 1033, 7404, 911, 1537, 2003, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,003 - vllm.entrypoints.logger - INFO - Received request cmpl-8c661a46c4904905812d580e00b4df9c-0: prompt: 'Hey, what do you think of my new haircut?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1128, 653, 498, 1744, 315, 847, 501, 85724, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,005 - vllm.entrypoints.logger - INFO - Received request cmpl-6282f3c23ce048bcace670adf8cfa786-0: prompt: "Hey there gorgeous! You're looking lovely today. I just had to come over and say hi.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 23511, 0, 1446, 2299, 3330, 16690, 3351, 13, 358, 1101, 1030, 311, 2525, 916, 323, 1977, 15588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,007 - vllm.entrypoints.logger - INFO - Received request cmpl-bc93d72ca8ad4e6e8b844c90fac7b79c-0: prompt: ', can you believe we had to do so many dishes yesterday after the party?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 646, 498, 4411, 582, 1030, 311, 653, 773, 1657, 25779, 13671, 1283, 279, 4614, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,010 - vllm.entrypoints.logger - INFO - Received request cmpl-33ba7a093d88403c861addc83bb7dfa9-0: prompt: 'I decided to keep the environment the same for our new business because I wanted to have familiar surroundings.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 6635, 311, 2506, 279, 4573, 279, 1852, 369, 1039, 501, 2562, 1576, 358, 4829, 311, 614, 11285, 39090, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,020 - vllm.entrypoints.logger - INFO - Received request cmpl-51cc683c159244bfa4d9d930cc0b57a8-0: prompt: 'Hey, Neighbors B, I have really been enjoying playing chess, checkers, and poker lately.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 11, 358, 614, 2167, 1012, 21413, 5619, 32719, 11, 1779, 388, 11, 323, 20814, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,025 - vllm.entrypoints.logger - INFO - Received request cmpl-a5d72ea864784c8f820bf07740b83d48-0: prompt: 'Hey, have you seen the new TV show that everyone is talking about?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3884, 279, 501, 5883, 1473, 429, 5019, 374, 7404, 911, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,026 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-861c9dac70474586a2e8881f2fe9d863-0.
2025-12-19 23:59:08,027 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6ebbf941d0d842149058d4616cfecb44-0.
2025-12-19 23:59:08,028 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-290ffdf079a347858f5b7f8ea1a20fc0-0.
2025-12-19 23:59:08,029 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8c661a46c4904905812d580e00b4df9c-0.
2025-12-19 23:59:08,031 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6282f3c23ce048bcace670adf8cfa786-0.
2025-12-19 23:59:08,032 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc93d72ca8ad4e6e8b844c90fac7b79c-0.
2025-12-19 23:59:08,033 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-33ba7a093d88403c861addc83bb7dfa9-0.
2025-12-19 23:59:08,034 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-51cc683c159244bfa4d9d930cc0b57a8-0.
2025-12-19 23:59:08,035 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a5d72ea864784c8f820bf07740b83d48-0.
2025-12-19 23:59:08,036 - vllm.core.scheduler - INFO - Pending queue size: (9)
2025-12-19 23:59:08,045 - vllm.entrypoints.logger - INFO - Received request cmpl-0860dc62c2854491a6fd4581b8b10b04-0: prompt: " I need to talk to you. I've been feeling really scared and unsafe lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1184, 311, 3061, 311, 498, 13, 358, 3003, 1012, 8266, 2167, 26115, 323, 19860, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,054 - vllm.entrypoints.logger - INFO - Received request cmpl-cb1ace636c414c488058d85b3004125c-0: prompt: 'I hope you enjoyed the prize I gave you, Neighbors B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3900, 498, 14006, 279, 21882, 358, 6551, 498, 11, 4182, 24101, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,067 - vllm.entrypoints.logger - INFO - Received request cmpl-37aebb28926f4e3baf39330faacf8c66-0: prompt: 'Hey, I wanted to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,081 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0860dc62c2854491a6fd4581b8b10b04-0.
2025-12-19 23:59:08,082 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cb1ace636c414c488058d85b3004125c-0.
2025-12-19 23:59:08,083 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-37aebb28926f4e3baf39330faacf8c66-0.
2025-12-19 23:59:08,084 - vllm.entrypoints.logger - INFO - Received request cmpl-b583c57d07c84ecd8e5964abff53d3a7-0: prompt: " I'm glad my boss let me come back to work, but only for a few hours each day.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 15713, 847, 13392, 1077, 752, 2525, 1182, 311, 975, 11, 714, 1172, 369, 264, 2421, 4115, 1817, 1899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,085 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:08,098 - vllm.entrypoints.logger - INFO - Received request cmpl-2609d6401e3c44ca9efc5ef0e28c6eb1-0: prompt: 'I am excited to give my friend this necklace with a heart pendant.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1079, 12035, 311, 2968, 847, 4238, 419, 54447, 448, 264, 4746, 41744, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,103 - vllm.entrypoints.logger - INFO - Received request cmpl-75f3ec9c6450491aac74b1ad203154a0-0: prompt: "I just can't take it anymore. This argument between us is getting so repetitive and frustrating.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 1896, 432, 14584, 13, 1096, 5693, 1948, 601, 374, 3709, 773, 58077, 323, 34611, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,104 - vllm.entrypoints.logger - INFO - Received request cmpl-83dd0e1cd9d5487d9b7e82f05ade98f0-0: prompt: 'Hey, I wanted to tell you about a job opening that I think would be perfect for you. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 3291, 498, 911, 264, 2618, 8568, 429, 358, 1744, 1035, 387, 4727, 369, 498, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,106 - vllm.entrypoints.logger - INFO - Received request cmpl-4276198ae75b4e3486495b9f06c8c23b-0: prompt: 'I am so exhausted from running errands all morning.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1079, 773, 37919, 504, 4303, 1848, 2844, 678, 6556, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,118 - vllm.entrypoints.logger - INFO - Received request cmpl-b174c13149ab4219b7cbdafa7ad2f95f-0: prompt: 'I needed to sweep the floor today and I realized the bristles on my broom are getting really stiff.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 4362, 311, 23146, 279, 6422, 3351, 323, 358, 15043, 279, 1411, 65943, 389, 847, 2896, 316, 525, 3709, 2167, 30061, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,122 - vllm.entrypoints.logger - INFO - Received request cmpl-850bd31126224ad69d70184efa123a35-0: prompt: "Hey, have you guys heard about the party this weekend? I'm definitely planning on going.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=38, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 7598, 6617, 911, 279, 4614, 419, 9001, 30, 358, 2776, 8491, 9115, 389, 2087, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,125 - vllm.entrypoints.logger - INFO - Received request cmpl-c378f62a4d35496fa83e108f6b26713f-0: prompt: "I'm sorry, I'm just feeling really scared right now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 14589, 11, 358, 2776, 1101, 8266, 2167, 26115, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,127 - vllm.entrypoints.logger - INFO - Received request cmpl-38f74871da5c42ff8e804f63015d1082-0: prompt: "I love making eggs! It's always so satisfying to cook them just right.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 3259, 18805, 0, 1084, 594, 2677, 773, 36054, 311, 4296, 1105, 1101, 1290, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,129 - vllm.entrypoints.logger - INFO - Received request cmpl-50f3e73b4f8140adb72ebdcb2a0254ec-0: prompt: " I'm glad my colleague and I were able to work things out after my joke caused offense.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 15713, 847, 36344, 323, 358, 1033, 2952, 311, 975, 2513, 700, 1283, 847, 21646, 8881, 16775, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,133 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b583c57d07c84ecd8e5964abff53d3a7-0.
2025-12-19 23:59:08,134 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2609d6401e3c44ca9efc5ef0e28c6eb1-0.
2025-12-19 23:59:08,135 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-75f3ec9c6450491aac74b1ad203154a0-0.
2025-12-19 23:59:08,137 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-83dd0e1cd9d5487d9b7e82f05ade98f0-0.
2025-12-19 23:59:08,138 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4276198ae75b4e3486495b9f06c8c23b-0.
2025-12-19 23:59:08,139 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b174c13149ab4219b7cbdafa7ad2f95f-0.
2025-12-19 23:59:08,140 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-850bd31126224ad69d70184efa123a35-0.
2025-12-19 23:59:08,141 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c378f62a4d35496fa83e108f6b26713f-0.
2025-12-19 23:59:08,141 - vllm.entrypoints.logger - INFO - Received request cmpl-3220e2b2293548608049a7d01fa105f0-0: prompt: "I'm really glad that you see me as a strong leader. It's important to me to be someone that people can count on.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=35, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 15713, 429, 498, 1490, 752, 438, 264, 3746, 7653, 13, 1084, 594, 2989, 311, 752, 311, 387, 4325, 429, 1251, 646, 1760, 389, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,142 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-38f74871da5c42ff8e804f63015d1082-0.
2025-12-19 23:59:08,143 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-50f3e73b4f8140adb72ebdcb2a0254ec-0.
2025-12-19 23:59:08,144 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3220e2b2293548608049a7d01fa105f0-0.
2025-12-19 23:59:08,145 - vllm.core.scheduler - INFO - Pending queue size: (11)
2025-12-19 23:59:08,150 - vllm.entrypoints.logger - INFO - Received request cmpl-bd3b4733c95f4138bbbf2deb5146309b-0: prompt: 'Hey there, sorry if my music was too loud earlier.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 14589, 421, 847, 4627, 572, 2238, 17361, 6788, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,154 - vllm.entrypoints.logger - INFO - Received request cmpl-2f5b38c01cc840f598561ba065aff3c5-0: prompt: 'Hey, have you heard the good news?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 279, 1661, 3669, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,178 - vllm.entrypoints.logger - INFO - Received request cmpl-3c796cc2d3044a778865875be9cc597d-0: prompt: 'Hi there, how are you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1246, 525, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,191 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bd3b4733c95f4138bbbf2deb5146309b-0.
2025-12-19 23:59:08,192 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2f5b38c01cc840f598561ba065aff3c5-0.
2025-12-19 23:59:08,194 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3c796cc2d3044a778865875be9cc597d-0.
2025-12-19 23:59:08,201 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:08,206 - vllm.entrypoints.logger - INFO - Received request cmpl-9fa02c796b1443f693f85cce960142be-0: prompt: 'Hey, did I tell you that I went to my first ballet performance last night? It was amazing!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 429, 358, 3937, 311, 847, 1156, 70210, 5068, 1537, 3729, 30, 1084, 572, 7897, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,223 - vllm.entrypoints.logger - INFO - Received request cmpl-4170fe8d7502440daf76cb5c781188b8-0: prompt: 'Did you hear that noise outside?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 6723, 429, 11980, 4889, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,233 - vllm.entrypoints.logger - INFO - Received request cmpl-95757d1f1d464f7f8c036e5927e2fb6a-0: prompt: 'Can you believe it? I finally got promoted!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 5499, 2684, 28926, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,245 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9fa02c796b1443f693f85cce960142be-0.
2025-12-19 23:59:08,246 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4170fe8d7502440daf76cb5c781188b8-0.
2025-12-19 23:59:08,247 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-95757d1f1d464f7f8c036e5927e2fb6a-0.
2025-12-19 23:59:08,249 - vllm.core.scheduler - INFO - Pending queue size: (3)
2025-12-19 23:59:08,250 - vllm.entrypoints.logger - INFO - Received request cmpl-a7b3ba092c0f4d91802197ac96ad6e79-0: prompt: 'Hey there, thanks for coming in to talk. I wanted to let you know that I was really impressed with your work on that project last week.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 9339, 369, 5001, 304, 311, 3061, 13, 358, 4829, 311, 1077, 498, 1414, 429, 358, 572, 2167, 24404, 448, 697, 975, 389, 429, 2390, 1537, 2003, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,251 - vllm.entrypoints.logger - INFO - Received request cmpl-59955e2424704cbb95f7f2aa6156e594-0: prompt: "Hey, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,259 - vllm.entrypoints.logger - INFO - Received request cmpl-f2b6c89f72164c30b38f9128e9d169ad-0: prompt: "I can't wait to finally go on my dream vacation to Los Angeles!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 311, 5499, 728, 389, 847, 7904, 20161, 311, 9656, 11902, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,266 - vllm.entrypoints.logger - INFO - Received request cmpl-0c6a50131ac240718c86379af09e98e1-0: prompt: "Hey, Neighbor B! How's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 97163, 425, 0, 2585, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,292 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a7b3ba092c0f4d91802197ac96ad6e79-0.
2025-12-19 23:59:08,300 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-59955e2424704cbb95f7f2aa6156e594-0.
2025-12-19 23:59:08,302 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f2b6c89f72164c30b38f9128e9d169ad-0.
2025-12-19 23:59:08,302 - vllm.entrypoints.logger - INFO - Received request cmpl-d026402ad12a483b81fcc518a0430836-0: prompt: 'I really believe that having a positive attitude makes a big difference in how we view things.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 4411, 429, 3432, 264, 6785, 18915, 3643, 264, 2409, 6672, 304, 1246, 582, 1651, 2513, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,303 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0c6a50131ac240718c86379af09e98e1-0.
2025-12-19 23:59:08,303 - vllm.entrypoints.logger - INFO - Received request cmpl-156edd9eecd64201917f130188310131-0: prompt: 'Hey, I need to tell you something. I did something really stupid.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1184, 311, 3291, 498, 2494, 13, 358, 1521, 2494, 2167, 18253, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,306 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:08,306 - vllm.entrypoints.logger - INFO - Received request cmpl-2340dc925dc1428bbf3e4cc1ca985ae5-0: prompt: 'I was looking at housing prices in our city and I noticed that the average price is $300,000. That seems really high to me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=37, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 3330, 518, 11721, 7576, 304, 1039, 3283, 323, 358, 13686, 429, 279, 5461, 3349, 374, 400, 18, 15, 15, 11, 15, 15, 15, 13, 2938, 4977, 2167, 1550, 311, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,313 - vllm.entrypoints.logger - INFO - Received request cmpl-55c1443cbabc4c0b8126383eea58b6d8-0: prompt: 'Have you ever looked at yourself in the mirror and wondered about your reflection?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 3512, 6966, 518, 6133, 304, 279, 17846, 323, 30056, 911, 697, 21844, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,320 - vllm.entrypoints.logger - INFO - Received request cmpl-f6902d448b9b4bb68be1cbc4747e3e83-0: prompt: 'Hi there, do you have a minute to talk?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 653, 498, 614, 264, 9383, 311, 3061, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,323 - vllm.entrypoints.logger - INFO - Received request cmpl-5215783423e14cc8a1361790e450ab43-0: prompt: "Hi Boss, I wanted to talk to you about something that's been on my mind lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 31569, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 389, 847, 3971, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,324 - vllm.entrypoints.logger - INFO - Received request cmpl-48dce337889e4ecbb944fcbf546a8b55-0: prompt: "I don't really like attention, you know that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 2167, 1075, 6529, 11, 498, 1414, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,349 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d026402ad12a483b81fcc518a0430836-0.
2025-12-19 23:59:08,351 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-156edd9eecd64201917f130188310131-0.
2025-12-19 23:59:08,352 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2340dc925dc1428bbf3e4cc1ca985ae5-0.
2025-12-19 23:59:08,353 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-55c1443cbabc4c0b8126383eea58b6d8-0.
2025-12-19 23:59:08,354 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f6902d448b9b4bb68be1cbc4747e3e83-0.
2025-12-19 23:59:08,355 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5215783423e14cc8a1361790e450ab43-0.
2025-12-19 23:59:08,356 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-48dce337889e4ecbb944fcbf546a8b55-0.
2025-12-19 23:59:08,357 - vllm.core.scheduler - INFO - Pending queue size: (7)
2025-12-19 23:59:08,366 - vllm.entrypoints.logger - INFO - Received request cmpl-0f3b0c96c1b14c65a745c02e08a76b36-0: prompt: " This sun is killing me. Can't we just jump into the pool now?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1096, 7015, 374, 13118, 752, 13, 2980, 944, 582, 1101, 7784, 1119, 279, 7314, 1431, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,383 - vllm.entrypoints.logger - INFO - Received request cmpl-05062afe34fb4e6daa89b681575354a7-0: prompt: 'Boss, something strange happened to me today. I received a package with my name on it, but the address was slightly wrong. It had "1234 Elm Street" instead of "123 Elm Street," so I crossed out the 4 and wrote in a 3.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 2494, 14888, 6932, 311, 752, 3351, 13, 358, 3949, 264, 6328, 448, 847, 829, 389, 432, 11, 714, 279, 2621, 572, 10078, 4969, 13, 1084, 1030, 330, 16, 17, 18, 19, 64229, 6686, 1, 4518, 315, 330, 16, 17, 18, 64229, 6686, 1335, 773, 358, 27031, 700, 279, 220, 19, 323, 6139, 304, 264, 220, 18, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,394 - vllm.entrypoints.logger - INFO - Received request cmpl-372dd39d65954c008a354397ca0d11c4-0: prompt: "I really think that we need to start composting our food waste. It's better for the environment and will reduce the amount of trash we produce.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 1744, 429, 582, 1184, 311, 1191, 56723, 287, 1039, 3607, 12291, 13, 1084, 594, 2664, 369, 279, 4573, 323, 686, 7949, 279, 3311, 315, 22854, 582, 8193, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,402 - vllm.engine.metrics - INFO - Avg prompt throughput: 1232.5 tokens/s, Avg generation throughput: 414.8 tokens/s, Running: 468 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 5.7%, CPU KV cache usage: 0.0%.
2025-12-19 23:59:08,416 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0f3b0c96c1b14c65a745c02e08a76b36-0.
2025-12-19 23:59:08,406 - vllm.entrypoints.logger - INFO - Received request cmpl-1e545de0089143399d5418ffd5c1f409-0: prompt: 'I had to fend off a "long-arm" yesterday. They were trying to take my car and my house.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 311, 94027, 1007, 264, 330, 4825, 66818, 1, 13671, 13, 2379, 1033, 4460, 311, 1896, 847, 1803, 323, 847, 3753, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,417 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-05062afe34fb4e6daa89b681575354a7-0.
2025-12-19 23:59:08,418 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-372dd39d65954c008a354397ca0d11c4-0.
2025-12-19 23:59:08,419 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1e545de0089143399d5418ffd5c1f409-0.
2025-12-19 23:59:08,420 - vllm.entrypoints.logger - INFO - Received request cmpl-351aff19f8fe40d985455bb229d76e51-0: prompt: 'Hi, Teacher.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 29069, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,421 - vllm.entrypoints.logger - INFO - Received request cmpl-7489581b7b64448ca65cfc6d589032cb-0: prompt: "Mentor, I want to tell you something. I've realized that I'm a competent friend who always has great advice and a great listener. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1366, 311, 3291, 498, 2494, 13, 358, 3003, 15043, 429, 358, 2776, 264, 39783, 4238, 879, 2677, 702, 2244, 9462, 323, 264, 2244, 11446, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,421 - vllm.core.scheduler - INFO - Pending queue size: (4)
2025-12-19 23:59:08,465 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-351aff19f8fe40d985455bb229d76e51-0.
2025-12-19 23:59:08,466 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7489581b7b64448ca65cfc6d589032cb-0.
2025-12-19 23:59:08,468 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:08,474 - vllm.entrypoints.logger - INFO - Received request cmpl-61fae064f0374ac992cab600c2353743-0: prompt: "It's funny how dancing can make you feel so alive.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 15173, 1246, 27966, 646, 1281, 498, 2666, 773, 13675, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,476 - vllm.entrypoints.logger - INFO - Received request cmpl-37785e2652864ef9abfb961587306cd7-0: prompt: " I can't believe how intense the protest march was today. I never thought I would have to face police officers in riot gear.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 646, 944, 4411, 1246, 18894, 279, 8665, 15217, 572, 3351, 13, 358, 2581, 3381, 358, 1035, 614, 311, 3579, 4282, 9611, 304, 41497, 14448, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,477 - vllm.entrypoints.logger - INFO - Received request cmpl-e7cd614080f043178adff34be6fb54e1-0: prompt: 'Can you believe I got fired because of a coffee mug? ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 358, 2684, 13895, 1576, 315, 264, 10799, 51489, 30, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,478 - vllm.entrypoints.logger - INFO - Received request cmpl-8b5d50e8680841809cfd840d577f6562-0: prompt: 'Good job, Mentor.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 2618, 11, 91191, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,480 - vllm.entrypoints.logger - INFO - Received request cmpl-498c1458bb5d4e5ab51486b5e7535199-0: prompt: 'You know, I always try to prioritize spending time with my family whenever I can.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2677, 1430, 311, 62552, 10164, 882, 448, 847, 2997, 15356, 358, 646, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,494 - vllm.entrypoints.logger - INFO - Received request cmpl-cf887669ab4f42fa9cbb8607969d63d0-0: prompt: 'I finally made it to New York City after a long road trip from Seattle.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 5499, 1865, 432, 311, 1532, 4261, 4311, 1283, 264, 1293, 5636, 8411, 504, 16355, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,512 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-61fae064f0374ac992cab600c2353743-0.
2025-12-19 23:59:08,514 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-37785e2652864ef9abfb961587306cd7-0.
2025-12-19 23:59:08,515 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e7cd614080f043178adff34be6fb54e1-0.
2025-12-19 23:59:08,516 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8b5d50e8680841809cfd840d577f6562-0.
2025-12-19 23:59:08,517 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-498c1458bb5d4e5ab51486b5e7535199-0.
2025-12-19 23:59:08,520 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cf887669ab4f42fa9cbb8607969d63d0-0.
2025-12-19 23:59:08,521 - vllm.core.scheduler - INFO - Pending queue size: (6)
2025-12-19 23:59:08,531 - vllm.entrypoints.logger - INFO - Received request cmpl-c3af0b7086ab4ba48a27e8500aea5d12-0: prompt: "Hey, just wanted to let you know that I'm at the library right now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1101, 4829, 311, 1077, 498, 1414, 429, 358, 2776, 518, 279, 6733, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,537 - vllm.entrypoints.logger - INFO - Received request cmpl-09763cd365bc40cabcad328e131e9c0f-0: prompt: "Mentor, I wanted to tell you that I'm really grateful for all the advice you gave me about playing soccer when I was younger. I don't think I would've made it as far as I have without your help.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=34, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 4829, 311, 3291, 498, 429, 358, 2776, 2167, 25195, 369, 678, 279, 9462, 498, 6551, 752, 911, 5619, 22174, 979, 358, 572, 14650, 13, 358, 1513, 944, 1744, 358, 1035, 3003, 1865, 432, 438, 3041, 438, 358, 614, 2041, 697, 1492, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,547 - vllm.entrypoints.logger - INFO - Received request cmpl-1550b6b00c054a49956cafdd28ee3bbd-0: prompt: 'Hey neighbor, check out my new clothes! I just went on a shopping spree.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 1779, 700, 847, 501, 15097, 0, 358, 1101, 3937, 389, 264, 11919, 78628, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,548 - vllm.entrypoints.logger - INFO - Received request cmpl-3e5d51e7cea647fb8b038e12634c02b9-0: prompt: 'Hey, do you have a moment to chat?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 614, 264, 4445, 311, 6236, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,550 - vllm.entrypoints.logger - INFO - Received request cmpl-bb448429267a411688b482c75542d8ee-0: prompt: "You know, I really wish I could be more like Neighbors B. He's always so kind and friendly to everyone.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2167, 6426, 358, 1410, 387, 803, 1075, 4182, 24101, 425, 13, 1260, 594, 2677, 773, 3093, 323, 11657, 311, 5019, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,552 - vllm.entrypoints.logger - INFO - Received request cmpl-433faa49f5734c73a2c972f1d52d82ec-0: prompt: "Hey Neighbor B, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 97163, 425, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,554 - vllm.entrypoints.logger - INFO - Received request cmpl-e64b9aa4b85e424a9dbbc376f5db0b8e-0: prompt: 'Hey, I have some great news to share with you!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 614, 1045, 2244, 3669, 311, 4332, 448, 498, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,557 - vllm.entrypoints.logger - INFO - Received request cmpl-01aa60d2a064451f9e48afa17d7c025c-0: prompt: 'Hi, Teacher. Do you remember when I won the trivia contest at the local library a couple of years ago?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 29069, 13, 3155, 498, 6099, 979, 358, 2765, 279, 72932, 13810, 518, 279, 2205, 6733, 264, 5625, 315, 1635, 4134, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,567 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c3af0b7086ab4ba48a27e8500aea5d12-0.
2025-12-19 23:59:08,568 - vllm.entrypoints.logger - INFO - Received request cmpl-8b2bb32544bc4db5a61f85edd38ff2e9-0: prompt: " Ah, it's looking so much better today!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [16366, 11, 432, 594, 3330, 773, 1753, 2664, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,569 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-09763cd365bc40cabcad328e131e9c0f-0.
2025-12-19 23:59:08,569 - vllm.entrypoints.logger - INFO - Received request cmpl-13e4b8926ef54899a23b091be773a980-0: prompt: "Mentor, I'm struggling with my behavior lately. I know I can be bad sometimes and I really want to work on it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 2776, 19962, 448, 847, 7709, 30345, 13, 358, 1414, 358, 646, 387, 3873, 7025, 323, 358, 2167, 1366, 311, 975, 389, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,570 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1550b6b00c054a49956cafdd28ee3bbd-0.
2025-12-19 23:59:08,570 - vllm.entrypoints.logger - INFO - Received request cmpl-f002a9b2e5d64aabbd50391881257174-0: prompt: "You know, I haven't been wearing my grandmother's diamond ring lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 8990, 944, 1012, 12233, 847, 38184, 594, 22205, 10058, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,571 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3e5d51e7cea647fb8b038e12634c02b9-0.
2025-12-19 23:59:08,572 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bb448429267a411688b482c75542d8ee-0.
2025-12-19 23:59:08,573 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-433faa49f5734c73a2c972f1d52d82ec-0.
2025-12-19 23:59:08,574 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e64b9aa4b85e424a9dbbc376f5db0b8e-0.
2025-12-19 23:59:08,575 - vllm.entrypoints.logger - INFO - Received request cmpl-686109736e574f1ebce35a66244d231a-0: prompt: 'I feel good after taking that job test. It was challenging, but I think I did well.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 1661, 1283, 4633, 429, 2618, 1273, 13, 1084, 572, 17003, 11, 714, 358, 1744, 358, 1521, 1632, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,575 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-01aa60d2a064451f9e48afa17d7c025c-0.
2025-12-19 23:59:08,576 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8b2bb32544bc4db5a61f85edd38ff2e9-0.
2025-12-19 23:59:08,577 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-13e4b8926ef54899a23b091be773a980-0.
2025-12-19 23:59:08,581 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f002a9b2e5d64aabbd50391881257174-0.
2025-12-19 23:59:08,582 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-686109736e574f1ebce35a66244d231a-0.
2025-12-19 23:59:08,591 - vllm.core.scheduler - INFO - Pending queue size: (12)
2025-12-19 23:59:08,594 - vllm.entrypoints.logger - INFO - Received request cmpl-20fcd7c2878e4dcfbbdf22ed68515cd3-0: prompt: "I can't deal with the pain anymore.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3484, 448, 279, 6646, 14584, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,595 - vllm.entrypoints.logger - INFO - Received request cmpl-7cfec1b092d94ce2afe02302c278cdeb-0: prompt: "I can't believe I wrote that letter to my ex-boyfriend. I was so vindictive.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 6139, 429, 6524, 311, 847, 505, 84903, 10701, 13, 358, 572, 773, 37805, 849, 533, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,614 - vllm.entrypoints.logger - INFO - Received request cmpl-e301fea77c884d92ae18ac56780e0560-0: prompt: 'Thanks again for coming with me to drop off my car at the garage.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12658, 1549, 369, 5001, 448, 752, 311, 5943, 1007, 847, 1803, 518, 279, 19277, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,616 - vllm.entrypoints.logger - INFO - Received request cmpl-180118312a414eea8fd6984e77d86224-0: prompt: "I feel so free up here in the hills. It's amazing how much nature can help to clear your head.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 773, 1910, 705, 1588, 304, 279, 34131, 13, 1084, 594, 7897, 1246, 1753, 6993, 646, 1492, 311, 2797, 697, 1968, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,618 - vllm.entrypoints.logger - INFO - Received request cmpl-c254b44d2ab749a984babfa53cacce1d-0: prompt: 'Your neck feels so soft.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [7771, 12975, 11074, 773, 8413, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,630 - vllm.entrypoints.logger - INFO - Received request cmpl-39e22d509c124d2f84091cd31d939a17-0: prompt: "I've been really trying to work on believing in myself lately. It's not always easy, but I know it's important if I want to accomplish my goals.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 2167, 4460, 311, 975, 389, 33990, 304, 7037, 30345, 13, 1084, 594, 537, 2677, 4135, 11, 714, 358, 1414, 432, 594, 2989, 421, 358, 1366, 311, 22054, 847, 8845, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,637 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-20fcd7c2878e4dcfbbdf22ed68515cd3-0.
2025-12-19 23:59:08,638 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7cfec1b092d94ce2afe02302c278cdeb-0.
2025-12-19 23:59:08,639 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e301fea77c884d92ae18ac56780e0560-0.
2025-12-19 23:59:08,639 - vllm.entrypoints.logger - INFO - Received request cmpl-1fc71cda241549b2aa020ca32a6f72c9-0: prompt: "You know, I've always valued honesty. It's such an important trait to have in both personal and professional relationships.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 3003, 2677, 32547, 47848, 13, 1084, 594, 1741, 458, 2989, 17567, 311, 614, 304, 2176, 4345, 323, 6584, 11871, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,640 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-180118312a414eea8fd6984e77d86224-0.
2025-12-19 23:59:08,641 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c254b44d2ab749a984babfa53cacce1d-0.
2025-12-19 23:59:08,642 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-39e22d509c124d2f84091cd31d939a17-0.
2025-12-19 23:59:08,645 - vllm.entrypoints.logger - INFO - Received request cmpl-18c08ea1c6154d58a2af8c7012159d1f-0: prompt: "Hi there! Excuse me, but I can't seem to remember your name. Can you remind me?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 0, 38895, 810, 752, 11, 714, 358, 646, 944, 2803, 311, 6099, 697, 829, 13, 2980, 498, 23974, 752, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,652 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1fc71cda241549b2aa020ca32a6f72c9-0.
2025-12-19 23:59:08,653 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-18c08ea1c6154d58a2af8c7012159d1f-0.
2025-12-19 23:59:08,657 - vllm.core.scheduler - INFO - Pending queue size: (8)
2025-12-19 23:59:08,682 - vllm.entrypoints.logger - INFO - Received request cmpl-eb6ca626374846ecaa997cfc72de5b50-0: prompt: 'I have to say, creating a daily routine for myself has really made a difference in my productivity levels.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 311, 1977, 11, 6825, 264, 7298, 14021, 369, 7037, 702, 2167, 1865, 264, 6672, 304, 847, 25148, 5866, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,687 - vllm.entrypoints.logger - INFO - Received request cmpl-fc9aab37e4184e5591883782346ff267-0: prompt: 'Child, from now on I want you to understand that nobody is allowed to talk to me without my permission. I expect them to show me the respect I deserve as an elder.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3652, 11, 504, 1431, 389, 358, 1366, 498, 311, 3535, 429, 18581, 374, 5420, 311, 3061, 311, 752, 2041, 847, 7882, 13, 358, 1720, 1105, 311, 1473, 752, 279, 5091, 358, 22695, 438, 458, 22130, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,702 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eb6ca626374846ecaa997cfc72de5b50-0.
2025-12-19 23:59:08,703 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fc9aab37e4184e5591883782346ff267-0.
2025-12-19 23:59:08,705 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:08,705 - vllm.entrypoints.logger - INFO - Received request cmpl-8ebf397498ed4b5abb2cf9639aced14e-0: prompt: 'You know, I have always believed in helping others. It just brings joy to my heart to see happy faces around me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 614, 2677, 11585, 304, 10476, 3800, 13, 1084, 1101, 12434, 15888, 311, 847, 4746, 311, 1490, 6247, 12300, 2163, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,715 - vllm.entrypoints.logger - INFO - Received request cmpl-2c0f5d3c19bb4a77a461a25320143274-0: prompt: 'Hey, hi there! How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 15588, 1052, 0, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,748 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8ebf397498ed4b5abb2cf9639aced14e-0.
2025-12-19 23:59:08,752 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2c0f5d3c19bb4a77a461a25320143274-0.
2025-12-19 23:59:08,754 - vllm.core.scheduler - INFO - Pending queue size: (2)
2025-12-19 23:59:08,761 - vllm.entrypoints.logger - INFO - Received request cmpl-e81508ee0e494e9a8a11189501abda2c-0: prompt: "Hi, sorry I'm a bit late. Is everything okay?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 14589, 358, 2776, 264, 2699, 3309, 13, 2160, 4297, 16910, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,762 - vllm.entrypoints.logger - INFO - Received request cmpl-8e355c5aaa7e4b39ac71719a37c6229d-0: prompt: "I really enjoy taking photos and capturing the world around me. It's so fulfilling to see beauty in everyday moments.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 4669, 4633, 7249, 323, 39780, 279, 1879, 2163, 752, 13, 1084, 594, 773, 49598, 311, 1490, 13143, 304, 17778, 13943, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,763 - vllm.entrypoints.logger - INFO - Received request cmpl-18bdf11102e64784b100c7b5443416d6-0: prompt: ', I had such a great time hosting my guest last night. They really enjoyed my company and praised me highly afterwards.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 1030, 1741, 264, 2244, 882, 19678, 847, 8640, 1537, 3729, 13, 2379, 2167, 14006, 847, 2813, 323, 36375, 752, 7548, 26807, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,764 - vllm.entrypoints.logger - INFO - Received request cmpl-01dbe0af1a6248f7926bd47d80d34c1e-0: prompt: 'Hey, have you thought any more about the bowling team?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3381, 894, 803, 911, 279, 58196, 2083, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,769 - vllm.entrypoints.logger - INFO - Received request cmpl-93dc7f2bd708471c92a391d1de7e5ac2-0: prompt: " I'm really enjoying this seminar. I feel like I'm learning so much about literature.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 2167, 21413, 419, 53575, 13, 358, 2666, 1075, 358, 2776, 6832, 773, 1753, 911, 17206, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,777 - vllm.entrypoints.logger - INFO - Received request cmpl-3a4b0611a9aa4ecabf5ca8dd04a9577f-0: prompt: "Hey there! I was wondering if I could borrow some money from you. My rent is due soon and I can't afford it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 0, 358, 572, 20293, 421, 358, 1410, 17193, 1045, 3220, 504, 498, 13, 3017, 8016, 374, 4152, 5135, 323, 358, 646, 944, 9946, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,782 - vllm.entrypoints.logger - INFO - Received request cmpl-23bcfd41ca2b409e9dc45784f6125d30-0: prompt: 'Hey, did you hear about my party last weekend?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 847, 4614, 1537, 9001, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,801 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e81508ee0e494e9a8a11189501abda2c-0.
2025-12-19 23:59:08,804 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8e355c5aaa7e4b39ac71719a37c6229d-0.
2025-12-19 23:59:08,807 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-18bdf11102e64784b100c7b5443416d6-0.
2025-12-19 23:59:08,808 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-01dbe0af1a6248f7926bd47d80d34c1e-0.
2025-12-19 23:59:08,809 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-93dc7f2bd708471c92a391d1de7e5ac2-0.
2025-12-19 23:59:08,810 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3a4b0611a9aa4ecabf5ca8dd04a9577f-0.
2025-12-19 23:59:08,811 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-23bcfd41ca2b409e9dc45784f6125d30-0.
2025-12-19 23:59:08,813 - vllm.core.scheduler - INFO - Pending queue size: (7)
2025-12-19 23:59:08,823 - vllm.entrypoints.logger - INFO - Received request cmpl-8781e5844b6246008dfaa553a3da5848-0: prompt: 'I love Neighbors A because it allows me to be adventurous and explore new things.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 4182, 24101, 362, 1576, 432, 6147, 752, 311, 387, 67088, 323, 13186, 501, 2513, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,825 - vllm.entrypoints.logger - INFO - Received request cmpl-80ab4ecf181e482e8e2348f33ac07d30-0: prompt: "I've been doing some research on different career paths, trying to figure out what will get me to where I want to be.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 3730, 1045, 3412, 389, 2155, 6931, 12716, 11, 4460, 311, 7071, 700, 1128, 686, 633, 752, 311, 1380, 358, 1366, 311, 387, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,839 - vllm.entrypoints.logger - INFO - Received request cmpl-b3b7fa8a0f45442fab3b6650f585c8f7-0: prompt: 'Hey Teacher, I have been thinking about how I can create more opportunities for myself and others. Any suggestions?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 29069, 11, 358, 614, 1012, 7274, 911, 1246, 358, 646, 1855, 803, 10488, 369, 7037, 323, 3800, 13, 5765, 18225, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,853 - vllm.entrypoints.logger - INFO - Received request cmpl-1be24dbd848f4c7caf2a3fb9d55f1a10-0: prompt: 'Hey there, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,861 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8781e5844b6246008dfaa553a3da5848-0.
2025-12-19 23:59:08,860 - vllm.entrypoints.logger - INFO - Received request cmpl-cab6eb46340041b0aa19f110051646cf-0: prompt: "It's been rough lately. I don't have a home anymore and I have to sleep on the streets.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 1012, 11165, 30345, 13, 358, 1513, 944, 614, 264, 2114, 14584, 323, 358, 614, 311, 6084, 389, 279, 14371, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,865 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-80ab4ecf181e482e8e2348f33ac07d30-0.
2025-12-19 23:59:08,866 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b3b7fa8a0f45442fab3b6650f585c8f7-0.
2025-12-19 23:59:08,869 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1be24dbd848f4c7caf2a3fb9d55f1a10-0.
2025-12-19 23:59:08,870 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cab6eb46340041b0aa19f110051646cf-0.
2025-12-19 23:59:08,872 - vllm.core.scheduler - INFO - Pending queue size: (5)
2025-12-19 23:59:08,874 - vllm.entrypoints.logger - INFO - Received request cmpl-3eeab5bb98fb48acb15227497510c7ea-0: prompt: ' I got in trouble in class today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2684, 304, 12264, 304, 536, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,879 - vllm.entrypoints.logger - INFO - Received request cmpl-b32bb262c6e643caa969abb63286e0bb-0: prompt: 'Mentor, I want to reach the top of the ladder. I know it will take time, but I am willing to climb up one rung at a time until I finally reach my goal.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1366, 311, 5545, 279, 1909, 315, 279, 35765, 13, 358, 1414, 432, 686, 1896, 882, 11, 714, 358, 1079, 9831, 311, 25367, 705, 825, 1598, 70, 518, 264, 882, 3080, 358, 5499, 5545, 847, 5795, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,882 - vllm.entrypoints.logger - INFO - Received request cmpl-bfd9192c68e84c4ea48c20b0d6b068c8-0: prompt: 'When I was younger, I often felt powerless when my friends would fight. I would feel upset and frustrated, not knowing how to help them. So, I made a vow to myself that I would always try to be understanding and compassionate towards others, in order to help them through difficult times.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4498, 358, 572, 14650, 11, 358, 3545, 6476, 84352, 979, 847, 4780, 1035, 4367, 13, 358, 1035, 2666, 22459, 323, 32530, 11, 537, 14063, 1246, 311, 1492, 1105, 13, 2055, 11, 358, 1865, 264, 39343, 311, 7037, 429, 358, 1035, 2677, 1430, 311, 387, 8660, 323, 59861, 6974, 3800, 11, 304, 1973, 311, 1492, 1105, 1526, 5000, 3039, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,894 - vllm.entrypoints.logger - INFO - Received request cmpl-18c2c77d0995424c9a0d93f9455e8463-0: prompt: "Coach, I can't believe we got fired from our jobs. I was really counting on that income to help pay my bills.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 646, 944, 4411, 582, 2684, 13895, 504, 1039, 6887, 13, 358, 572, 2167, 25009, 389, 429, 7911, 311, 1492, 2291, 847, 18610, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,899 - vllm.entrypoints.logger - INFO - Received request cmpl-0b8ec983b29c418293edca1c94ff07f7-0: prompt: 'Teacher, I feel really sad and helpless right now.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [45065, 11, 358, 2666, 2167, 12421, 323, 62528, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,905 - vllm.entrypoints.logger - INFO - Received request cmpl-c964c11b77a1478f9c51abe29d26493e-0: prompt: "I just can't believe how ignorant I am about so many things.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 4411, 1246, 46980, 358, 1079, 911, 773, 1657, 2513, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,908 - vllm.entrypoints.logger - INFO - Received request cmpl-2ba1f0b9f7174d99a408ddcb858ce685-0: prompt: " I'm really nervous about my presentation tomorrow.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 2167, 22596, 911, 847, 15496, 16577, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,911 - vllm.entrypoints.logger - INFO - Received request cmpl-5c925a7e699b475ea163e1221cdf5156-0: prompt: 'I had such a wonderful time at the beach today. The sound of the waves and the salty breeze was so invigorating.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 1741, 264, 11117, 882, 518, 279, 11321, 3351, 13, 576, 5112, 315, 279, 16876, 323, 279, 73875, 45285, 572, 773, 1529, 69970, 1095, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,913 - vllm.entrypoints.logger - INFO - Received request cmpl-a7a1f12701884acd9f307e4a9ddca67f-0: prompt: "Alright, it's time to head home. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [71486, 11, 432, 594, 882, 311, 1968, 2114, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,915 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3eeab5bb98fb48acb15227497510c7ea-0.
2025-12-19 23:59:08,917 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b32bb262c6e643caa969abb63286e0bb-0.
2025-12-19 23:59:08,918 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bfd9192c68e84c4ea48c20b0d6b068c8-0.
2025-12-19 23:59:08,920 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-18c2c77d0995424c9a0d93f9455e8463-0.
2025-12-19 23:59:08,922 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0b8ec983b29c418293edca1c94ff07f7-0.
2025-12-19 23:59:08,923 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c964c11b77a1478f9c51abe29d26493e-0.
2025-12-19 23:59:08,924 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2ba1f0b9f7174d99a408ddcb858ce685-0.
2025-12-19 23:59:08,925 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5c925a7e699b475ea163e1221cdf5156-0.
2025-12-19 23:59:08,930 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a7a1f12701884acd9f307e4a9ddca67f-0.
2025-12-19 23:59:08,935 - vllm.core.scheduler - WARNING - Sequence group cmpl-127b732aade74712b89e2879b01167f2-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1
2025-12-19 23:59:08,937 - vllm.core.scheduler - INFO - Pending queue size: (14)
2025-12-19 23:59:08,977 - vllm.entrypoints.logger - INFO - Received request cmpl-95e11a6a00ba42bcbcee667fb555aeef-0: prompt: "Wait, hold on, that's not what we're supposed to talk about!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [14190, 11, 3331, 389, 11, 429, 594, 537, 1128, 582, 2299, 9966, 311, 3061, 911, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,982 - vllm.core.scheduler - INFO - Pending queue size: (14)
2025-12-19 23:59:08,988 - vllm.entrypoints.logger - INFO - Received request cmpl-7b56a4a5946b443f9d2225d4f61404ed-0: prompt: ' It was a great feeling helping out at the well today and seeing the villagers appreciate it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1084, 572, 264, 2244, 8266, 10476, 700, 518, 279, 1632, 3351, 323, 9120, 279, 60821, 15401, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,994 - vllm.entrypoints.logger - INFO - Received request cmpl-eec9b4d73a144e0396ecf0eaa523b6fc-0: prompt: "Hi, Neighbors B! How's everything going with your parents?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 4182, 24101, 425, 0, 2585, 594, 4297, 2087, 448, 697, 6562, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:08,996 - vllm.entrypoints.logger - INFO - Received request cmpl-89fc3fa530a6455f94eb7077f7a9a0c1-0: prompt: 'Child, I have some exciting news to share with you. I am going to have a baby!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3652, 11, 358, 614, 1045, 13245, 3669, 311, 4332, 448, 498, 13, 358, 1079, 2087, 311, 614, 264, 8770, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,014 - vllm.entrypoints.logger - INFO - Received request cmpl-9be9b8e94ba843ea9a331bd0b729390d-0: prompt: "Hey neighbor, check out my new shirt! I bought it today and I'm really happy with it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 1779, 700, 847, 501, 15478, 0, 358, 10788, 432, 3351, 323, 358, 2776, 2167, 6247, 448, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,023 - vllm.entrypoints.logger - INFO - Received request cmpl-19267caa016e42848ed4d5190d645ec7-0: prompt: "I just took my favorite shirt off! It's so comfortable to be without it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 3867, 847, 6930, 15478, 1007, 0, 1084, 594, 773, 10655, 311, 387, 2041, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,026 - vllm.entrypoints.logger - INFO - Received request cmpl-b44eed8797574b14abd7f2ccdaaf3b18-0: prompt: "I really think I'm the wittiest person in the room.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 1744, 358, 2776, 279, 289, 1442, 13438, 1697, 304, 279, 3054, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,027 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-95e11a6a00ba42bcbcee667fb555aeef-0.
2025-12-19 23:59:09,028 - vllm.entrypoints.logger - INFO - Received request cmpl-7835bb8c32c149ddae86c71bb741e73c-0: prompt: ' Hi Boss, can I talk to you about something?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [21018, 31569, 11, 646, 358, 3061, 311, 498, 911, 2494, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,028 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7b56a4a5946b443f9d2225d4f61404ed-0.
2025-12-19 23:59:09,030 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eec9b4d73a144e0396ecf0eaa523b6fc-0.
2025-12-19 23:59:09,031 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-89fc3fa530a6455f94eb7077f7a9a0c1-0.
2025-12-19 23:59:09,032 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9be9b8e94ba843ea9a331bd0b729390d-0.
2025-12-19 23:59:09,033 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-19267caa016e42848ed4d5190d645ec7-0.
2025-12-19 23:59:09,034 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b44eed8797574b14abd7f2ccdaaf3b18-0.
2025-12-19 23:59:09,034 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7835bb8c32c149ddae86c71bb741e73c-0.
2025-12-19 23:59:09,037 - vllm.core.scheduler - INFO - Pending queue size: (22)
2025-12-19 23:59:09,037 - vllm.entrypoints.logger - INFO - Received request cmpl-319223f956fc4c1883d1ef1cb86b87d2-0: prompt: 'I really believe that speaking your truth is the only way to lead effectively.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 4411, 429, 12094, 697, 8046, 374, 279, 1172, 1616, 311, 2990, 13444, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,063 - vllm.entrypoints.logger - INFO - Received request cmpl-204b3b3c29b94f40919ba5e19aea0c32-0: prompt: "Hey there, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,066 - vllm.entrypoints.logger - INFO - Received request cmpl-365c2f1fcd6446e1b52832211383ca0d-0: prompt: 'Child, I want to talk to you about something very important today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3652, 11, 358, 1366, 311, 3061, 311, 498, 911, 2494, 1602, 2989, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,068 - vllm.entrypoints.logger - INFO - Received request cmpl-6b3accae376649568c90ee6c554305b8-0: prompt: ' Hey, B! Check out my new trumpet! ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [27553, 11, 425, 0, 4248, 700, 847, 501, 91259, 0, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,077 - vllm.entrypoints.logger - INFO - Received request cmpl-3e5286e10eba4294a02205546498d53c-0: prompt: 'Oh no, I just ruined the soup. I accidentally spat in it while I was stirring.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 902, 11, 358, 1101, 46068, 279, 19174, 13, 358, 32384, 62883, 304, 432, 1393, 358, 572, 53954, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,081 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-319223f956fc4c1883d1ef1cb86b87d2-0.
2025-12-19 23:59:09,082 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-204b3b3c29b94f40919ba5e19aea0c32-0.
2025-12-19 23:59:09,083 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-365c2f1fcd6446e1b52832211383ca0d-0.
2025-12-19 23:59:09,084 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6b3accae376649568c90ee6c554305b8-0.
2025-12-19 23:59:09,085 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3e5286e10eba4294a02205546498d53c-0.
2025-12-19 23:59:09,088 - vllm.core.scheduler - INFO - Pending queue size: (27)
2025-12-19 23:59:09,091 - vllm.entrypoints.logger - INFO - Received request cmpl-4ba7d5663dec491384adc1f044d6bccf-0: prompt: ', guess what? I aced my math test!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 7942, 1128, 30, 358, 1613, 291, 847, 6888, 1273, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,093 - vllm.entrypoints.logger - INFO - Received request cmpl-a35473d6fc5f4d3baebfb846bd7243bb-0: prompt: ' That was a close call. I slipped and hurt my leg while walking on the roof.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2938, 572, 264, 3265, 1618, 13, 358, 42478, 323, 12898, 847, 2472, 1393, 11435, 389, 279, 15134, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,107 - vllm.entrypoints.logger - INFO - Received request cmpl-d789ae6838204f3a9e3edc3ed659d786-0: prompt: 'I was so happy today! My program finally worked perfectly.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 773, 6247, 3351, 0, 3017, 2025, 5499, 6439, 13942, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,112 - vllm.entrypoints.logger - INFO - Received request cmpl-a05447d0c95c4d57b2d0b8a3128bcabc-0: prompt: 'Check out this data! I organized it into a table.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3973, 700, 419, 821, 0, 358, 16645, 432, 1119, 264, 1965, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,129 - vllm.entrypoints.logger - INFO - Received request cmpl-20615558a4994bb89d0c828fc7f26e0d-0: prompt: 'I had a great breakfast this morning! I ate three egg whites and I feel satisfied.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 264, 2244, 17496, 419, 6556, 0, 358, 29812, 2326, 18636, 21874, 323, 358, 2666, 19527, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,132 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4ba7d5663dec491384adc1f044d6bccf-0.
2025-12-19 23:59:09,133 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a35473d6fc5f4d3baebfb846bd7243bb-0.
2025-12-19 23:59:09,134 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d789ae6838204f3a9e3edc3ed659d786-0.
2025-12-19 23:59:09,135 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a05447d0c95c4d57b2d0b8a3128bcabc-0.
2025-12-19 23:59:09,136 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-20615558a4994bb89d0c828fc7f26e0d-0.
2025-12-19 23:59:09,139 - vllm.core.scheduler - INFO - Pending queue size: (32)
2025-12-19 23:59:09,141 - vllm.entrypoints.logger - INFO - Received request cmpl-d5cf48169cfc4a7ab7f238ca238cfffe-0: prompt: 'Here you go, Coach. Enjoy a scoop of mint chocolate chip ice cream. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8420, 498, 728, 11, 27176, 13, 22656, 264, 56407, 315, 28337, 17931, 16392, 9853, 12644, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,145 - vllm.entrypoints.logger - INFO - Received request cmpl-b54904f7ae3448eb800adf35d9049bce-0: prompt: 'Hey there, I was wondering if I could chat with you for a bit?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 358, 572, 20293, 421, 358, 1410, 6236, 448, 498, 369, 264, 2699, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,161 - vllm.entrypoints.logger - INFO - Received request cmpl-7e6524ddf7bd47eb840c715f3cb07447-0: prompt: "Hi, Teacher. I've been struggling with a problem lately and I was hoping to get your advice.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 29069, 13, 358, 3003, 1012, 19962, 448, 264, 3491, 30345, 323, 358, 572, 15652, 311, 633, 697, 9462, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,162 - vllm.entrypoints.logger - INFO - Received request cmpl-d56a63f3984c41b3bf7379a2fce5bc8c-0: prompt: "It's such a nice day today. I'm glad we decided to take a walk.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 1741, 264, 6419, 1899, 3351, 13, 358, 2776, 15713, 582, 6635, 311, 1896, 264, 4227, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,173 - vllm.entrypoints.logger - INFO - Received request cmpl-eac35b8cc92d428196b3031e7033d2e4-0: prompt: 'Hey, do you have any tips on helping with math homework?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 614, 894, 10414, 389, 10476, 448, 6888, 28459, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,183 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d5cf48169cfc4a7ab7f238ca238cfffe-0.
2025-12-19 23:59:09,184 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b54904f7ae3448eb800adf35d9049bce-0.
2025-12-19 23:59:09,185 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7e6524ddf7bd47eb840c715f3cb07447-0.
2025-12-19 23:59:09,186 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d56a63f3984c41b3bf7379a2fce5bc8c-0.
2025-12-19 23:59:09,187 - vllm.entrypoints.logger - INFO - Received request cmpl-cdd2dae0dcf14d569e3fbf34d0dfb3d3-0: prompt: "How's your day been, Classmates B?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4340, 594, 697, 1899, 1012, 11, 3228, 16457, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,187 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eac35b8cc92d428196b3031e7033d2e4-0.
2025-12-19 23:59:09,188 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cdd2dae0dcf14d569e3fbf34d0dfb3d3-0.
2025-12-19 23:59:09,191 - vllm.core.scheduler - INFO - Pending queue size: (38)
2025-12-19 23:59:09,206 - vllm.entrypoints.logger - INFO - Received request cmpl-b126988a819a4aa492e74badd281f1dd-0: prompt: "You know, I've been thinking about pursuing a career in the arts.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 3003, 1012, 7274, 911, 33018, 264, 6931, 304, 279, 18560, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,219 - vllm.entrypoints.logger - INFO - Received request cmpl-37348a00b8904321b55268ab9ab90de0-0: prompt: "I'm really determined to get into a good college.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 10838, 311, 633, 1119, 264, 1661, 7770, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,229 - vllm.entrypoints.logger - INFO - Received request cmpl-8084ba6682ab4843b5e4e324dc7d490a-0: prompt: "I couldn't believe my luck when I saw that woman on the street carrying such a large amount of cash.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 7691, 944, 4411, 847, 15019, 979, 358, 5485, 429, 5220, 389, 279, 8592, 15331, 1741, 264, 3460, 3311, 315, 8350, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,234 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b126988a819a4aa492e74badd281f1dd-0.
2025-12-19 23:59:09,236 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-37348a00b8904321b55268ab9ab90de0-0.
2025-12-19 23:59:09,237 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8084ba6682ab4843b5e4e324dc7d490a-0.
2025-12-19 23:59:09,239 - vllm.core.scheduler - INFO - Pending queue size: (41)
2025-12-19 23:59:09,248 - vllm.entrypoints.logger - INFO - Received request cmpl-109e4b271b954de7aa2de075ef3bf461-0: prompt: ", I have something to tell you that I should have shared a while ago. It's about my sister.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 614, 2494, 311, 3291, 498, 429, 358, 1265, 614, 6094, 264, 1393, 4134, 13, 1084, 594, 911, 847, 12923, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,249 - vllm.entrypoints.logger - INFO - Received request cmpl-180f825594ae4735b578b8e3c7850723-0: prompt: "I'm in such a creative mood today. I'm writing a short story for my English class.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 304, 1741, 264, 11521, 19671, 3351, 13, 358, 2776, 4378, 264, 2805, 3364, 369, 847, 6364, 536, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,252 - vllm.entrypoints.logger - INFO - Received request cmpl-77cc8005514c41cfa247ce37ba0ab5d6-0: prompt: 'I was so nervous before my presentation earlier today, but I managed to calm myself down and give a great presentation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 773, 22596, 1573, 847, 15496, 6788, 3351, 11, 714, 358, 8975, 311, 19300, 7037, 1495, 323, 2968, 264, 2244, 15496, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,266 - vllm.entrypoints.logger - INFO - Received request cmpl-3b9c42d5f40d49d7ad4ecd3433da7909-0: prompt: "Wow, I can't believe I actually beat you in the footrace today, Neighbors B!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 358, 646, 944, 4411, 358, 3520, 9382, 498, 304, 279, 4478, 41183, 3351, 11, 4182, 24101, 425, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,267 - vllm.entrypoints.logger - INFO - Received request cmpl-b3ac8694aeaf43b29a42fd768937cdd9-0: prompt: ' I was feeling a bit stressed out after all that work, so I decided to take a break and read one of my favorite comics, "The Adventures of Captain Amazing." It always helps me de-stress.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 572, 8266, 264, 2699, 31547, 700, 1283, 678, 429, 975, 11, 773, 358, 6635, 311, 1896, 264, 1438, 323, 1349, 825, 315, 847, 6930, 28059, 11, 330, 785, 50579, 315, 21316, 36533, 1189, 1084, 2677, 8609, 752, 409, 5477, 673, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,270 - vllm.entrypoints.logger - INFO - Received request cmpl-57903b87adc848af986b63106d9d1fb4-0: prompt: "Hey, I wanted to share something with you. Today, I tried to give away a book that I've already read.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 4332, 2494, 448, 498, 13, 11201, 11, 358, 6679, 311, 2968, 3123, 264, 2311, 429, 358, 3003, 2669, 1349, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,271 - vllm.entrypoints.logger - INFO - Received request cmpl-4881837a887e4a4dbe63021a3986e20a-0: prompt: 'Hey, have you heard about the new club that just opened up? It sounds like it would be so much fun to go there tonight!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 911, 279, 501, 6335, 429, 1101, 8930, 705, 30, 1084, 10362, 1075, 432, 1035, 387, 773, 1753, 2464, 311, 728, 1052, 17913, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,274 - vllm.entrypoints.logger - INFO - Received request cmpl-78c3ed4d02324b7eab3813d62b02a3e5-0: prompt: "I'm so glad we came to this restaurant. The chicken parmesan was delicious.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 15713, 582, 3697, 311, 419, 10729, 13, 576, 16158, 1346, 8828, 276, 572, 17923, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,277 - vllm.entrypoints.logger - INFO - Received request cmpl-d4135052de5b45658a676eafecc29dc2-0: prompt: 'Hey, have you guys made any plans for this weekend yet?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 7598, 1865, 894, 6649, 369, 419, 9001, 3602, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,279 - vllm.entrypoints.logger - INFO - Received request cmpl-9b3abdb9e1a1496f9e77950125dd0efd-0: prompt: ', would you like to join me for lunch at noon on Tuesday in the school cafeteria?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 1035, 498, 1075, 311, 5138, 752, 369, 15786, 518, 37145, 389, 7589, 304, 279, 2906, 93848, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,283 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-109e4b271b954de7aa2de075ef3bf461-0.
2025-12-19 23:59:09,284 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-180f825594ae4735b578b8e3c7850723-0.
2025-12-19 23:59:09,285 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-77cc8005514c41cfa247ce37ba0ab5d6-0.
2025-12-19 23:59:09,287 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3b9c42d5f40d49d7ad4ecd3433da7909-0.
2025-12-19 23:59:09,288 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b3ac8694aeaf43b29a42fd768937cdd9-0.
2025-12-19 23:59:09,289 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-57903b87adc848af986b63106d9d1fb4-0.
2025-12-19 23:59:09,289 - vllm.entrypoints.logger - INFO - Received request cmpl-0bffc68c6fe74bffb592c2476b5b8d45-0: prompt: "Coach, I've made a big decision. I'm quitting the club.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 3003, 1865, 264, 2409, 5480, 13, 358, 2776, 68084, 279, 6335, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,290 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4881837a887e4a4dbe63021a3986e20a-0.
2025-12-19 23:59:09,291 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-78c3ed4d02324b7eab3813d62b02a3e5-0.
2025-12-19 23:59:09,291 - vllm.entrypoints.logger - INFO - Received request cmpl-8328029625fc4adc97e54ef9094179f2-0: prompt: "I'm so happy you asked me out today, Wife. It feels like forever since we've had a date.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 6247, 498, 4588, 752, 700, 3351, 11, 42408, 13, 1084, 11074, 1075, 15683, 2474, 582, 3003, 1030, 264, 2400, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,292 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d4135052de5b45658a676eafecc29dc2-0.
2025-12-19 23:59:09,293 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9b3abdb9e1a1496f9e77950125dd0efd-0.
2025-12-19 23:59:09,294 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0bffc68c6fe74bffb592c2476b5b8d45-0.
2025-12-19 23:59:09,295 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8328029625fc4adc97e54ef9094179f2-0.
2025-12-19 23:59:09,297 - vllm.core.scheduler - INFO - Pending queue size: (53)
2025-12-19 23:59:09,317 - vllm.entrypoints.logger - INFO - Received request cmpl-ca80ba2a1c4940d39acb33cc98b41330-0: prompt: 'Hey, do you have a spare hour later this afternoon?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 614, 264, 23863, 6460, 2937, 419, 13354, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,322 - vllm.entrypoints.logger - INFO - Received request cmpl-c39994427ed541f58e9ed59b1ea0442b-0: prompt: "Hey neighbor, it's nice to see you again.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 432, 594, 6419, 311, 1490, 498, 1549, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,341 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ca80ba2a1c4940d39acb33cc98b41330-0.
2025-12-19 23:59:09,343 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c39994427ed541f58e9ed59b1ea0442b-0.
2025-12-19 23:59:09,345 - vllm.core.scheduler - INFO - Pending queue size: (55)
2025-12-19 23:59:09,353 - vllm.entrypoints.logger - INFO - Received request cmpl-3b8694b5f02341839573f9e2fb45c933-0: prompt: 'Hey, I just wanted to share something with you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1101, 4829, 311, 4332, 2494, 448, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,356 - vllm.entrypoints.logger - INFO - Received request cmpl-50356b383b384289a44d1a99266a6a81-0: prompt: 'Hi Mentor! I organized a meeting last week and I think it went really well.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 0, 358, 16645, 264, 6438, 1537, 2003, 323, 358, 1744, 432, 3937, 2167, 1632, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,357 - vllm.entrypoints.logger - INFO - Received request cmpl-e1969ffba4ca47c698b6b45c92415cd3-0: prompt: "Hi everyone, I'm back from the hospital!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 5019, 11, 358, 2776, 1182, 504, 279, 8777, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,359 - vllm.entrypoints.logger - INFO - Received request cmpl-98cb44ce7bc24345914e1204398337a4-0: prompt: ', I need to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,361 - vllm.entrypoints.logger - INFO - Received request cmpl-7fe3c1425f1b4d7f96e60726bf4a42ca-0: prompt: 'Hey, can I talk to you for a minute?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 358, 3061, 311, 498, 369, 264, 9383, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,362 - vllm.entrypoints.logger - INFO - Received request cmpl-54afc0d716854c97a3cdb7ec9b221f2c-0: prompt: 'Hey, have you tried this toothpaste I recently bought?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6679, 419, 25507, 56868, 358, 5926, 10788, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,364 - vllm.entrypoints.logger - INFO - Received request cmpl-33627bca6ae54f8d98cdc43938b98f80-0: prompt: 'Hey, have you ever been interested in learning about the solar system?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 1012, 8014, 304, 6832, 911, 279, 12941, 1849, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,365 - vllm.entrypoints.logger - INFO - Received request cmpl-83a2b5b218d04319822e50c87e1c4147-0: prompt: ', guess who I ran into at the park yesterday?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 7942, 879, 358, 10613, 1119, 518, 279, 6118, 13671, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,382 - vllm.entrypoints.logger - INFO - Received request cmpl-edf8242c1ab94da98168a3e589c11f61-0: prompt: 'Hey neighbor, have you heard about my latest decision?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 614, 498, 6617, 911, 847, 5535, 5480, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,387 - vllm.entrypoints.logger - INFO - Received request cmpl-b7a250dcda2643729b725bae716662c1-0: prompt: "Wow, that was an incredible speech I gave today. I didn't expect the crowd to be so receptive.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 429, 572, 458, 15050, 8806, 358, 6551, 3351, 13, 358, 3207, 944, 1720, 279, 13428, 311, 387, 773, 87551, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,390 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3b8694b5f02341839573f9e2fb45c933-0.
2025-12-19 23:59:09,392 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-50356b383b384289a44d1a99266a6a81-0.
2025-12-19 23:59:09,393 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e1969ffba4ca47c698b6b45c92415cd3-0.
2025-12-19 23:59:09,394 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-98cb44ce7bc24345914e1204398337a4-0.
2025-12-19 23:59:09,395 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7fe3c1425f1b4d7f96e60726bf4a42ca-0.
2025-12-19 23:59:09,395 - vllm.entrypoints.logger - INFO - Received request cmpl-0b92980f1f1947228f2ee2c9c5059b4f-0: prompt: "I don't understand why my family always gets on my case. I mean, what's the big deal if my room is messy?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 3535, 3170, 847, 2997, 2677, 5221, 389, 847, 1142, 13, 358, 3076, 11, 1128, 594, 279, 2409, 3484, 421, 847, 3054, 374, 45846, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,396 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-54afc0d716854c97a3cdb7ec9b221f2c-0.
2025-12-19 23:59:09,397 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-33627bca6ae54f8d98cdc43938b98f80-0.
2025-12-19 23:59:09,397 - vllm.entrypoints.logger - INFO - Received request cmpl-efdaf0414fec4015be5e1457ae68e268-0: prompt: "I'm sorry for stepping on your foot earlier. I didn't mean to hurt you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 14589, 369, 35467, 389, 697, 4478, 6788, 13, 358, 3207, 944, 3076, 311, 12898, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,398 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-83a2b5b218d04319822e50c87e1c4147-0.
2025-12-19 23:59:09,399 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-edf8242c1ab94da98168a3e589c11f61-0.
2025-12-19 23:59:09,400 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b7a250dcda2643729b725bae716662c1-0.
2025-12-19 23:59:09,401 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0b92980f1f1947228f2ee2c9c5059b4f-0.
2025-12-19 23:59:09,402 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-efdaf0414fec4015be5e1457ae68e268-0.
2025-12-19 23:59:09,404 - vllm.core.scheduler - INFO - Pending queue size: (67)
2025-12-19 23:59:09,408 - vllm.entrypoints.logger - INFO - Received request cmpl-4ac4f0cbfb6a4202ba3cdcb11bb889de-0: prompt: "Excuse me, Classmates B, I just wanted to mention that it's impolite to interrupt others when they're speaking. It's important to listen and wait for your turn to speak.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [41721, 810, 752, 11, 3228, 16457, 425, 11, 358, 1101, 4829, 311, 6286, 429, 432, 594, 3163, 337, 632, 311, 12667, 3800, 979, 807, 2299, 12094, 13, 1084, 594, 2989, 311, 8844, 323, 3783, 369, 697, 2484, 311, 6468, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,412 - vllm.entrypoints.logger - INFO - Received request cmpl-47825c5e137c40c1ac08060aef4a4b89-0: prompt: "Doctor, I have some news to share with you. I've recently started working on a sculpture of my grandmother, and I feel really content and happy about it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 614, 1045, 3669, 311, 4332, 448, 498, 13, 358, 3003, 5926, 3855, 3238, 389, 264, 49967, 315, 847, 38184, 11, 323, 358, 2666, 2167, 2213, 323, 6247, 911, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,422 - vllm.entrypoints.logger - INFO - Received request cmpl-2ec2771cd25e43bfb67b13c80261cb37-0: prompt: 'Hey, have you ever been on the Kingda Ka roller coaster at Six Flags Great Adventure?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 1012, 389, 279, 6210, 3235, 22309, 28451, 81068, 518, 18680, 33205, 8513, 32012, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,448 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4ac4f0cbfb6a4202ba3cdcb11bb889de-0.
2025-12-19 23:59:09,449 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-47825c5e137c40c1ac08060aef4a4b89-0.
2025-12-19 23:59:09,450 - vllm.entrypoints.logger - INFO - Received request cmpl-22256c1e86794c8984e784f4778beaf4-0: prompt: 'Hey, have you ever written letters to your friends and family to express your gratitude towards them?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 5326, 11931, 311, 697, 4780, 323, 2997, 311, 3158, 697, 45035, 6974, 1105, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,451 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2ec2771cd25e43bfb67b13c80261cb37-0.
2025-12-19 23:59:09,452 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-22256c1e86794c8984e784f4778beaf4-0.
2025-12-19 23:59:09,454 - vllm.core.scheduler - INFO - Pending queue size: (71)
2025-12-19 23:59:09,465 - vllm.entrypoints.logger - INFO - Received request cmpl-8b735c007a5b479894995d6ecdedbcc0-0: prompt: "Hey Neighbor! How's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 97163, 0, 2585, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,467 - vllm.entrypoints.logger - INFO - Received request cmpl-892659bf8b2e479b901d4cafd37768c0-0: prompt: 'I just woke up from a two-hour nap, but I feel exhausted. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 38726, 705, 504, 264, 1378, 21231, 25859, 11, 714, 358, 2666, 37919, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,484 - vllm.entrypoints.logger - INFO - Received request cmpl-237a9a6e523441db9bd777880e7a4624-0: prompt: 'Neighbors B, are you feeling okay? You look pale and sweaty.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [58086, 425, 11, 525, 498, 8266, 16910, 30, 1446, 1401, 27539, 323, 97421, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,491 - vllm.entrypoints.logger - INFO - Received request cmpl-c831950b2fcf4b13a8ca54b8de231540-0: prompt: 'Hey, have you checked out my blog on beauty tips?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 10067, 700, 847, 5010, 389, 13143, 10414, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,492 - vllm.entrypoints.logger - INFO - Received request cmpl-031ffd0e822b442d89d0e6a20ff98d29-0: prompt: 'Hey, I just wanted to tell you about the housewarming gift I gave to my friend.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1101, 4829, 311, 3291, 498, 911, 279, 3753, 86, 32902, 8189, 358, 6551, 311, 847, 4238, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,498 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8b735c007a5b479894995d6ecdedbcc0-0.
2025-12-19 23:59:09,500 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-892659bf8b2e479b901d4cafd37768c0-0.
2025-12-19 23:59:09,501 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-237a9a6e523441db9bd777880e7a4624-0.
2025-12-19 23:59:09,502 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c831950b2fcf4b13a8ca54b8de231540-0.
2025-12-19 23:59:09,503 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-031ffd0e822b442d89d0e6a20ff98d29-0.
2025-12-19 23:59:09,505 - vllm.core.scheduler - INFO - Pending queue size: (76)
2025-12-19 23:59:09,512 - vllm.entrypoints.logger - INFO - Received request cmpl-b2191117bb394cc18ad8263009417767-0: prompt: 'I kind of lost my temper at recess today and pushed one of my friends.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3093, 315, 5558, 847, 6797, 518, 46762, 3351, 323, 15391, 825, 315, 847, 4780, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,514 - vllm.entrypoints.logger - INFO - Received request cmpl-5c08bf8923e34d97b943be6e36578003-0: prompt: 'Hey Mentor, I just wanted to share some exciting news with you. I achieved my goal of running a 5k!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 1101, 4829, 311, 4332, 1045, 13245, 3669, 448, 498, 13, 358, 16994, 847, 5795, 315, 4303, 264, 220, 20, 74, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,514 - vllm.entrypoints.logger - INFO - Received request cmpl-5a37a369b04b45c396acf5889fbcd4ce-0: prompt: 'Mentor, I have some exciting news to share with you. I am now the proud owner of a small yacht!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 1045, 13245, 3669, 311, 4332, 448, 498, 13, 358, 1079, 1431, 279, 12409, 6372, 315, 264, 2613, 71932, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,517 - vllm.entrypoints.logger - INFO - Received request cmpl-f99454c2d69249a994fa4a7cbdfe3b2e-0: prompt: "I can't believe I'm actually retired after thirty years of working at the widget factory. It feels a little strange not having to wake up early and go to work.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 2776, 3520, 21583, 1283, 26127, 1635, 315, 3238, 518, 279, 9086, 8633, 13, 1084, 11074, 264, 2632, 14888, 537, 3432, 311, 15156, 705, 4124, 323, 728, 311, 975, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,525 - vllm.entrypoints.logger - INFO - Received request cmpl-335536fd5e234502949b8f64fbd033eb-0: prompt: "I'm thinking about starting a website to make some extra money.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 7274, 911, 5916, 264, 3910, 311, 1281, 1045, 4960, 3220, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,534 - vllm.entrypoints.logger - INFO - Received request cmpl-2386338113524e1a9d67580203d61ab6-0: prompt: "Doctor, I'm in trouble. I'm stuck in a snowdrift and I can't get out.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 2776, 304, 12264, 13, 358, 2776, 15700, 304, 264, 11794, 3612, 2085, 323, 358, 646, 944, 633, 700, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,548 - vllm.entrypoints.logger - INFO - Received request cmpl-c8a398fed7674beda44385aad92ca80a-0: prompt: 'Hey sweetie, did you see me in the parade today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 10226, 645, 11, 1521, 498, 1490, 752, 304, 279, 36504, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,550 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b2191117bb394cc18ad8263009417767-0.
2025-12-19 23:59:09,552 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5c08bf8923e34d97b943be6e36578003-0.
2025-12-19 23:59:09,553 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5a37a369b04b45c396acf5889fbcd4ce-0.
2025-12-19 23:59:09,554 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f99454c2d69249a994fa4a7cbdfe3b2e-0.
2025-12-19 23:59:09,555 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-335536fd5e234502949b8f64fbd033eb-0.
2025-12-19 23:59:09,556 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2386338113524e1a9d67580203d61ab6-0.
2025-12-19 23:59:09,557 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c8a398fed7674beda44385aad92ca80a-0.
2025-12-19 23:59:09,559 - vllm.core.scheduler - INFO - Pending queue size: (83)
2025-12-19 23:59:09,564 - vllm.entrypoints.logger - INFO - Received request cmpl-4a9919a1ae984a0fa27eded46b19940b-0: prompt: "Good morning, teacher! I'm so excited today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 11079, 0, 358, 2776, 773, 12035, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,582 - vllm.entrypoints.logger - INFO - Received request cmpl-70e8f6efcde244d793f13da308a41f3a-0: prompt: "I have some exciting news to share with you, Neighbors B. I'm getting married!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 1045, 13245, 3669, 311, 4332, 448, 498, 11, 4182, 24101, 425, 13, 358, 2776, 3709, 12224, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,603 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4a9919a1ae984a0fa27eded46b19940b-0.
2025-12-19 23:59:09,604 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-70e8f6efcde244d793f13da308a41f3a-0.
2025-12-19 23:59:09,607 - vllm.core.scheduler - INFO - Pending queue size: (85)
2025-12-19 23:59:09,648 - vllm.entrypoints.logger - INFO - Received request cmpl-c148c6d14bc74ba9a9110edef1dcaec1-0: prompt: "Wow, that was quite a climb. I'm really glad we made it to the top.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 429, 572, 5008, 264, 25367, 13, 358, 2776, 2167, 15713, 582, 1865, 432, 311, 279, 1909, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,650 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c148c6d14bc74ba9a9110edef1dcaec1-0.
2025-12-19 23:59:09,653 - vllm.core.scheduler - INFO - Pending queue size: (86)
2025-12-19 23:59:09,697 - vllm.core.scheduler - INFO - Pending queue size: (86)
2025-12-19 23:59:09,705 - vllm.entrypoints.logger - INFO - Received request cmpl-fdd10b7a2711436fae834408cf54a602-0: prompt: 'You know, I really feel like life is worth fighting for. The rush of adrenaline I get from a tough battle is indescribable.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2167, 2666, 1075, 2272, 374, 5802, 10805, 369, 13, 576, 12973, 315, 78392, 358, 633, 504, 264, 11045, 8049, 374, 1257, 288, 740, 59051, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,740 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fdd10b7a2711436fae834408cf54a602-0.
2025-12-19 23:59:09,744 - vllm.core.scheduler - INFO - Pending queue size: (87)
2025-12-19 23:59:09,786 - vllm.core.scheduler - INFO - Pending queue size: (87)
2025-12-19 23:59:09,812 - vllm.entrypoints.logger - INFO - Received request cmpl-ab50c33e0e424bba9fdd699bb9a345ac-0: prompt: 'Hey, did I tell you about the party I threw last weekend?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 911, 279, 4614, 358, 22192, 1537, 9001, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,827 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ab50c33e0e424bba9fdd699bb9a345ac-0.
2025-12-19 23:59:09,828 - vllm.entrypoints.logger - INFO - Received request cmpl-461cb04b1aa2440c9c39dc6071ee43f8-0: prompt: 'Ive been feeling really confident lately.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3982, 1012, 8266, 2167, 16506, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,830 - vllm.core.scheduler - INFO - Pending queue size: (88)
2025-12-19 23:59:09,831 - vllm.entrypoints.logger - INFO - Received request cmpl-2743f6c24c834060af51e5140229a718-0: prompt: "Doctor, I just want to thank you for all your support during my time in college. I couldn't have done it without you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 1101, 1366, 311, 9702, 498, 369, 678, 697, 1824, 2337, 847, 882, 304, 7770, 13, 358, 7691, 944, 614, 2814, 432, 2041, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,837 - vllm.entrypoints.logger - INFO - Received request cmpl-6f7fc19b9f7042b99cd654a83a4b419f-0: prompt: 'Thanks again for the ride earlier.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12658, 1549, 369, 279, 11877, 6788, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,849 - vllm.entrypoints.logger - INFO - Received request cmpl-85af869f3cec47ea986ed075496aa5d4-0: prompt: "Hey, have you ever been lost in a crowd of people before? It's such a scary feeling.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 1012, 5558, 304, 264, 13428, 315, 1251, 1573, 30, 1084, 594, 1741, 264, 28465, 8266, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,874 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-461cb04b1aa2440c9c39dc6071ee43f8-0.
2025-12-19 23:59:09,875 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2743f6c24c834060af51e5140229a718-0.
2025-12-19 23:59:09,877 - vllm.entrypoints.logger - INFO - Received request cmpl-81b9362d8a684aec8bdbeebea0b7b68f-0: prompt: "Doctor, I need to talk to you about something that's been happening lately. I've been forgetting a lot of things, like where I put my keys or what I had for breakfast.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 12482, 30345, 13, 358, 3003, 1012, 65027, 264, 2696, 315, 2513, 11, 1075, 1380, 358, 2182, 847, 6894, 476, 1128, 358, 1030, 369, 17496, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,877 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f7fc19b9f7042b99cd654a83a4b419f-0.
2025-12-19 23:59:09,878 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-85af869f3cec47ea986ed075496aa5d4-0.
2025-12-19 23:59:09,879 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-81b9362d8a684aec8bdbeebea0b7b68f-0.
2025-12-19 23:59:09,882 - vllm.core.scheduler - INFO - Pending queue size: (93)
2025-12-19 23:59:09,886 - vllm.entrypoints.logger - INFO - Received request cmpl-e16b338ba4774146b9f1617293855ec4-0: prompt: ', are you okay? You fell down!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 525, 498, 16910, 30, 1446, 11052, 1495, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,905 - vllm.entrypoints.logger - INFO - Received request cmpl-485cec481a304220afb8a4f9e9cb8015-0: prompt: 'I think I can get any job I want because of my willingness to work hard and learn new things.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1744, 358, 646, 633, 894, 2618, 358, 1366, 1576, 315, 847, 38275, 311, 975, 2588, 323, 3960, 501, 2513, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,926 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e16b338ba4774146b9f1617293855ec4-0.
2025-12-19 23:59:09,927 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-485cec481a304220afb8a4f9e9cb8015-0.
2025-12-19 23:59:09,930 - vllm.core.scheduler - INFO - Pending queue size: (95)
2025-12-19 23:59:09,937 - vllm.entrypoints.logger - INFO - Received request cmpl-abebe653ce0d4ea29a7fbdd09a9520a8-0: prompt: "I'm really motivated to improve my performance at work.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 26664, 311, 7269, 847, 5068, 518, 975, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,940 - vllm.entrypoints.logger - INFO - Received request cmpl-41d22284dc5b4d4c9a9727ec8b760b87-0: prompt: ", I'm really struggling with money right now. I'm barely making enough to cover rent and bills.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 2167, 19962, 448, 3220, 1290, 1431, 13, 358, 2776, 19460, 3259, 3322, 311, 3421, 8016, 323, 18610, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,950 - vllm.entrypoints.logger - INFO - Received request cmpl-ce9a42db69e440fcb4734899ad08fc18-0: prompt: 'I have to tell you something, I lied to my parents about where I was last night.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 311, 3291, 498, 2494, 11, 358, 46153, 311, 847, 6562, 911, 1380, 358, 572, 1537, 3729, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,951 - vllm.entrypoints.logger - INFO - Received request cmpl-99f6e335734a4af0b927fbc25edd409b-0: prompt: 'The dinner party last night was a success. Everyone loved the food and the table looked great.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [785, 13856, 4614, 1537, 3729, 572, 264, 2393, 13, 21455, 10245, 279, 3607, 323, 279, 1965, 6966, 2244, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,965 - vllm.entrypoints.logger - INFO - Received request cmpl-d3284113fb37407eafd521830e7934b8-0: prompt: "I just picked up these new shoes, they're a limited edition sneaker release. I've been waiting for them for months.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=33, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 12771, 705, 1493, 501, 15294, 11, 807, 2299, 264, 7199, 13688, 20760, 4407, 4879, 13, 358, 3003, 1012, 8580, 369, 1105, 369, 3951, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,968 - vllm.entrypoints.logger - INFO - Received request cmpl-f702dc0119a24d959cc6b29e84e45e6e-0: prompt: ", there's something I need to tell you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 1052, 594, 2494, 358, 1184, 311, 3291, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:09,974 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-abebe653ce0d4ea29a7fbdd09a9520a8-0.
2025-12-19 23:59:09,976 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-41d22284dc5b4d4c9a9727ec8b760b87-0.
2025-12-19 23:59:09,977 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ce9a42db69e440fcb4734899ad08fc18-0.
2025-12-19 23:59:09,978 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-99f6e335734a4af0b927fbc25edd409b-0.
2025-12-19 23:59:09,979 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d3284113fb37407eafd521830e7934b8-0.
2025-12-19 23:59:09,981 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f702dc0119a24d959cc6b29e84e45e6e-0.
2025-12-19 23:59:09,983 - vllm.core.scheduler - INFO - Pending queue size: (101)
2025-12-19 23:59:10,011 - vllm.entrypoints.logger - INFO - Received request cmpl-7063350179774eb2b0dcb669743c8599-0: prompt: "Mentor, guess what? I asked out the girl I've had a crush on for months and she said yes!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 7942, 1128, 30, 358, 4588, 700, 279, 3743, 358, 3003, 1030, 264, 5030, 389, 369, 3951, 323, 1340, 1053, 9834, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,027 - vllm.entrypoints.logger - INFO - Received request cmpl-e3c85f90a228496981076ee5744849d2-0: prompt: "I can't believe my favorite author died. It just doesn't seem real.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 847, 6930, 3150, 8469, 13, 1084, 1101, 3171, 944, 2803, 1931, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,027 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7063350179774eb2b0dcb669743c8599-0.
2025-12-19 23:59:10,028 - vllm.entrypoints.logger - INFO - Received request cmpl-f75b82114d20455993452e0d12ffcf27-0: prompt: "I spent two hours studying math today and still couldn't get it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 7391, 1378, 4115, 20956, 6888, 3351, 323, 2058, 7691, 944, 633, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,030 - vllm.core.scheduler - INFO - Pending queue size: (102)
2025-12-19 23:59:10,042 - vllm.entrypoints.logger - INFO - Received request cmpl-b170d8f1ea5e4d49a4754763aaceaf0c-0: prompt: ' I wish I could defend myself better. I feel so weak and helpless.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 6426, 358, 1410, 10505, 7037, 2664, 13, 358, 2666, 773, 7469, 323, 62528, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,051 - vllm.entrypoints.logger - INFO - Received request cmpl-bd0cf2ea905a471abb054cee06a50956-0: prompt: 'Hi, Neighbor B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 97163, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,055 - vllm.entrypoints.logger - INFO - Received request cmpl-1c3b2b269054419499537d68adf2eac7-0: prompt: 'Hey Mentor, I wanted to let you know that your son has been doing an amazing job as my assistant. He is a hard worker and very dedicated.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 4829, 311, 1077, 498, 1414, 429, 697, 4438, 702, 1012, 3730, 458, 7897, 2618, 438, 847, 17847, 13, 1260, 374, 264, 2588, 11864, 323, 1602, 12235, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,062 - vllm.entrypoints.logger - INFO - Received request cmpl-16ebcd23c1664d3984efa52911c69f62-0: prompt: 'Hi doctor, how are you today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 10668, 11, 1246, 525, 498, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,068 - vllm.entrypoints.logger - INFO - Received request cmpl-771df9c6b9404f08a93b9cbbbf8e742a-0: prompt: 'Hey Mentor, guess what? I bought a bike today on a whim!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 7942, 1128, 30, 358, 10788, 264, 12963, 3351, 389, 264, 73471, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,074 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e3c85f90a228496981076ee5744849d2-0.
2025-12-19 23:59:10,075 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f75b82114d20455993452e0d12ffcf27-0.
2025-12-19 23:59:10,076 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b170d8f1ea5e4d49a4754763aaceaf0c-0.
2025-12-19 23:59:10,077 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bd0cf2ea905a471abb054cee06a50956-0.
2025-12-19 23:59:10,079 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1c3b2b269054419499537d68adf2eac7-0.
2025-12-19 23:59:10,080 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-16ebcd23c1664d3984efa52911c69f62-0.
2025-12-19 23:59:10,081 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-771df9c6b9404f08a93b9cbbbf8e742a-0.
2025-12-19 23:59:10,084 - vllm.core.scheduler - INFO - Pending queue size: (109)
2025-12-19 23:59:10,096 - vllm.entrypoints.logger - INFO - Received request cmpl-31b60687fc9c49e582a9d5dbda308fd3-0: prompt: 'Mmm, this kibble is delicious.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 3821, 11, 419, 595, 82418, 374, 17923, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,097 - vllm.entrypoints.logger - INFO - Received request cmpl-7bf5e07b84494526aa4dfed98c4321ac-0: prompt: 'Hey, have you ever been to the museum?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 1012, 311, 279, 23971, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,099 - vllm.entrypoints.logger - INFO - Received request cmpl-345baa3a5cec49b1ad16cf0b930deb88-0: prompt: "I can't believe what happened to me yesterday. I got into a car accident and had to be rushed to the hospital. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1128, 6932, 311, 752, 13671, 13, 358, 2684, 1119, 264, 1803, 11423, 323, 1030, 311, 387, 32241, 311, 279, 8777, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,125 - vllm.entrypoints.logger - INFO - Received request cmpl-651b0a3b36bb40838492475469e579ba-0: prompt: " That's a beautiful necklace you're wearing, Doctor. Where did you get it?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2938, 594, 264, 6233, 54447, 498, 2299, 12233, 11, 18635, 13, 10967, 1521, 498, 633, 432, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,127 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-31b60687fc9c49e582a9d5dbda308fd3-0.
2025-12-19 23:59:10,129 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7bf5e07b84494526aa4dfed98c4321ac-0.
2025-12-19 23:59:10,130 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-345baa3a5cec49b1ad16cf0b930deb88-0.
2025-12-19 23:59:10,131 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-651b0a3b36bb40838492475469e579ba-0.
2025-12-19 23:59:10,134 - vllm.core.scheduler - INFO - Pending queue size: (113)
2025-12-19 23:59:10,155 - vllm.entrypoints.logger - INFO - Received request cmpl-030506222e984d56baea47eacd6ce58a-0: prompt: 'Wow, these mountains are incredible. I never realized how small and insignificant I am compared to them.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 1493, 23501, 525, 15050, 13, 358, 2581, 15043, 1246, 2613, 323, 72421, 358, 1079, 7707, 311, 1105, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,166 - vllm.entrypoints.logger - INFO - Received request cmpl-64ffba6bd2d945c386d151a721263866-0: prompt: "You know, I was going through a rough patch last week when my computer died on me. I was so frustrated because I had so many important files on it, and I didn't know what to do.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 572, 2087, 1526, 264, 11165, 10900, 1537, 2003, 979, 847, 6366, 8469, 389, 752, 13, 358, 572, 773, 32530, 1576, 358, 1030, 773, 1657, 2989, 3542, 389, 432, 11, 323, 358, 3207, 944, 1414, 1128, 311, 653, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,177 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-030506222e984d56baea47eacd6ce58a-0.
2025-12-19 23:59:10,179 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-64ffba6bd2d945c386d151a721263866-0.
2025-12-19 23:59:10,181 - vllm.core.scheduler - INFO - Pending queue size: (115)
2025-12-19 23:59:10,185 - vllm.entrypoints.logger - INFO - Received request cmpl-ecb42236f497425c8a557ccd21d01070-0: prompt: "Hey, I just got a new follower on my social media. I'm so happy!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1101, 2684, 264, 501, 48207, 389, 847, 3590, 3687, 13, 358, 2776, 773, 6247, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,225 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ecb42236f497425c8a557ccd21d01070-0.
2025-12-19 23:59:10,227 - vllm.core.scheduler - INFO - Pending queue size: (116)
2025-12-19 23:59:10,229 - vllm.entrypoints.logger - INFO - Received request cmpl-e5bc814d2d0f479bba5c5de02705d0aa-0: prompt: 'Hi there! Did you enjoy the tomato I gave you from my garden yesterday?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 0, 14568, 498, 4669, 279, 41020, 358, 6551, 498, 504, 847, 13551, 13671, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,270 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e5bc814d2d0f479bba5c5de02705d0aa-0.
2025-12-19 23:59:10,273 - vllm.core.scheduler - INFO - Pending queue size: (117)
2025-12-19 23:59:10,315 - vllm.core.scheduler - INFO - Pending queue size: (117)
2025-12-19 23:59:10,323 - vllm.entrypoints.logger - INFO - Received request cmpl-4f2f322909cf4720971de7610453c50a-0: prompt: "I had a really great time at lunch today. It's always so nice to catch up with you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 264, 2167, 2244, 882, 518, 15786, 3351, 13, 1084, 594, 2677, 773, 6419, 311, 2287, 705, 448, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,356 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4f2f322909cf4720971de7610453c50a-0.
2025-12-19 23:59:10,359 - vllm.core.scheduler - INFO - Pending queue size: (118)
2025-12-19 23:59:10,370 - vllm.entrypoints.logger - INFO - Received request cmpl-0c6b27230bd74cb5a469cec3e6172e2e-0: prompt: ' I just finished a big case in court this week. It was intense, but I was able to negotiate a favorable settlement for my client.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1101, 8060, 264, 2409, 1142, 304, 5473, 419, 2003, 13, 1084, 572, 18894, 11, 714, 358, 572, 2952, 311, 36567, 264, 36749, 17079, 369, 847, 2943, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,402 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0c6b27230bd74cb5a469cec3e6172e2e-0.
2025-12-19 23:59:10,405 - vllm.core.scheduler - INFO - Pending queue size: (119)
2025-12-19 23:59:10,447 - vllm.core.scheduler - INFO - Pending queue size: (119)
2025-12-19 23:59:10,457 - vllm.entrypoints.logger - INFO - Received request cmpl-26f332354d5f4f2ba7309482d0fae91d-0: prompt: "I'm really glad I was able to remember all the information for my history test.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 15713, 358, 572, 2952, 311, 6099, 678, 279, 1995, 369, 847, 3840, 1273, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,479 - vllm.entrypoints.logger - INFO - Received request cmpl-db07269c9564435db5581b47d2aa2f4e-0: prompt: 'Hi, Neighbor B! How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 97163, 425, 0, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,497 - vllm.entrypoints.logger - INFO - Received request cmpl-babae9c6c2b641e89a200c7b4f32184e-0: prompt: "Hi, I know I messed up again, and I'm sorry for leaving you in the rain. I didn't mean to be mean.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=31, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 358, 1414, 358, 64202, 705, 1549, 11, 323, 358, 2776, 14589, 369, 9380, 498, 304, 279, 11174, 13, 358, 3207, 944, 3076, 311, 387, 3076, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,509 - vllm.entrypoints.logger - INFO - Received request cmpl-0546b7705110451389f8a24f148fe83d-0: prompt: 'I just love the trees that surround our neighborhood. They provide such a sense of security and peace.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 2948, 279, 12408, 429, 8995, 1039, 12534, 13, 2379, 3410, 1741, 264, 5530, 315, 4763, 323, 8919, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,514 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-26f332354d5f4f2ba7309482d0fae91d-0.
2025-12-19 23:59:10,515 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-db07269c9564435db5581b47d2aa2f4e-0.
2025-12-19 23:59:10,516 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-babae9c6c2b641e89a200c7b4f32184e-0.
2025-12-19 23:59:10,517 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0546b7705110451389f8a24f148fe83d-0.
2025-12-19 23:59:10,520 - vllm.core.scheduler - INFO - Pending queue size: (123)
2025-12-19 23:59:10,522 - vllm.entrypoints.logger - INFO - Received request cmpl-0e9189479d864cf8a84d0623636f1e53-0: prompt: "That salad looks really good. I've been craving something fresh and healthy lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4792, 32466, 5868, 2167, 1661, 13, 358, 3003, 1012, 63704, 2494, 7722, 323, 9314, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,541 - vllm.entrypoints.logger - INFO - Received request cmpl-d917ee5e0e074597a972a80267fefd80-0: prompt: "I'm sorry I stole your cigarette earlier. I just couldn't resist the urge.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 14589, 358, 39506, 697, 35113, 6788, 13, 358, 1101, 7691, 944, 22106, 279, 32047, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,544 - vllm.entrypoints.logger - INFO - Received request cmpl-dbe92ea55872427691cf1cd90c460704-0: prompt: "I feel really lonely since my family moved. I don't know anyone in my new neighborhood.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 2167, 39566, 2474, 847, 2997, 7726, 13, 358, 1513, 944, 1414, 5489, 304, 847, 501, 12534, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,562 - vllm.entrypoints.logger - INFO - Received request cmpl-97bc32c01a724dae8142c964fe13e3b6-0: prompt: "I just can't seem to get anything right. I'm such a worthless loser.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 2803, 311, 633, 4113, 1290, 13, 358, 2776, 1741, 264, 65021, 58891, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,564 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0e9189479d864cf8a84d0623636f1e53-0.
2025-12-19 23:59:10,565 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d917ee5e0e074597a972a80267fefd80-0.
2025-12-19 23:59:10,566 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dbe92ea55872427691cf1cd90c460704-0.
2025-12-19 23:59:10,567 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-97bc32c01a724dae8142c964fe13e3b6-0.
2025-12-19 23:59:10,570 - vllm.core.scheduler - INFO - Pending queue size: (127)
2025-12-19 23:59:10,577 - vllm.entrypoints.logger - INFO - Received request cmpl-2ae9606f40f84332b13cf94fca311e28-0: prompt: "I feel so scared, Teacher. I'm afraid of my father and I don't feel safe at home.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 773, 26115, 11, 29069, 13, 358, 2776, 16575, 315, 847, 6981, 323, 358, 1513, 944, 2666, 6092, 518, 2114, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,613 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2ae9606f40f84332b13cf94fca311e28-0.
2025-12-19 23:59:10,616 - vllm.core.scheduler - INFO - Pending queue size: (128)
2025-12-19 23:59:10,619 - vllm.entrypoints.logger - INFO - Received request cmpl-91828dc6a1a84cd2ab234c757da53419-0: prompt: "I can't believe I threw away that stuffed animal my best friend gave me. It was green and purple and so soft.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 22192, 3123, 429, 44313, 9864, 847, 1850, 4238, 6551, 752, 13, 1084, 572, 6176, 323, 24932, 323, 773, 8413, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,658 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-91828dc6a1a84cd2ab234c757da53419-0.
2025-12-19 23:59:10,660 - vllm.entrypoints.logger - INFO - Received request cmpl-7b3417c06ba44f10aa045cd9337ad26c-0: prompt: 'Ah, this music is great. Thanks for letting me borrow your walkman, B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 419, 4627, 374, 2244, 13, 11114, 369, 20194, 752, 17193, 697, 4227, 1515, 11, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,661 - vllm.core.scheduler - INFO - Pending queue size: (129)
2025-12-19 23:59:10,662 - vllm.entrypoints.logger - INFO - Received request cmpl-369cd1c8b1344b8a9018eb73d683c96f-0: prompt: "I'm so excited to start school next week. I've been studying hard this summer and I'm hoping to do really well.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 311, 1191, 2906, 1790, 2003, 13, 358, 3003, 1012, 20956, 2588, 419, 7324, 323, 358, 2776, 15652, 311, 653, 2167, 1632, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,685 - vllm.entrypoints.logger - INFO - Received request cmpl-b054a4b36a314cb8912304f1e463eae1-0: prompt: ' Hey, do you want a cigarette? I remember you saying how much you enjoy smoking.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [27553, 11, 653, 498, 1366, 264, 35113, 30, 358, 6099, 498, 5488, 1246, 1753, 498, 4669, 19578, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,705 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7b3417c06ba44f10aa045cd9337ad26c-0.
2025-12-19 23:59:10,707 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-369cd1c8b1344b8a9018eb73d683c96f-0.
2025-12-19 23:59:10,708 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b054a4b36a314cb8912304f1e463eae1-0.
2025-12-19 23:59:10,710 - vllm.core.scheduler - INFO - Pending queue size: (132)
2025-12-19 23:59:10,724 - vllm.entrypoints.logger - INFO - Received request cmpl-5692cfa538f446ce89ec679be2630bab-0: prompt: 'Hey, have you ever thought about pursuing a degree in library science?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 3381, 911, 33018, 264, 8381, 304, 6733, 8038, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,738 - vllm.entrypoints.logger - INFO - Received request cmpl-321905d30717427db95c50d4b8774ead-0: prompt: "I decided to take the last chance because I didn't want to regret not taking it later. I wanted to see what was on the other side of the door and if there was anything that could help me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 6635, 311, 1896, 279, 1537, 6012, 1576, 358, 3207, 944, 1366, 311, 22231, 537, 4633, 432, 2937, 13, 358, 4829, 311, 1490, 1128, 572, 389, 279, 1008, 3108, 315, 279, 6006, 323, 421, 1052, 572, 4113, 429, 1410, 1492, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,752 - vllm.entrypoints.logger - INFO - Received request cmpl-d2747b6bf4644b5aaa9bb0961ab5a5cd-0: prompt: "Hi Teacher, I wanted to talk to you today about something that's been on my mind.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 29069, 11, 358, 4829, 311, 3061, 311, 498, 3351, 911, 2494, 429, 594, 1012, 389, 847, 3971, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,754 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5692cfa538f446ce89ec679be2630bab-0.
2025-12-19 23:59:10,755 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-321905d30717427db95c50d4b8774ead-0.
2025-12-19 23:59:10,757 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d2747b6bf4644b5aaa9bb0961ab5a5cd-0.
2025-12-19 23:59:10,759 - vllm.core.scheduler - INFO - Pending queue size: (135)
2025-12-19 23:59:10,773 - vllm.entrypoints.logger - INFO - Received request cmpl-26f723855410401cbc65b87710da12fa-0: prompt: ", I'm excited about Jerry's party this weekend. It's great to reconnect with old friends.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=36, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 12035, 911, 28708, 594, 4614, 419, 9001, 13, 1084, 594, 2244, 311, 50036, 448, 2310, 4780, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,790 - vllm.entrypoints.logger - INFO - Received request cmpl-a7decd346fe54443982eb5d05dca8363-0: prompt: 'I went to this amazing free concert in the park last night! One of my favorite bands was playing.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3937, 311, 419, 7897, 1910, 20830, 304, 279, 6118, 1537, 3729, 0, 3776, 315, 847, 6930, 20892, 572, 5619, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,791 - vllm.entrypoints.logger - INFO - Received request cmpl-3601e6c7838b405fb91e8678ace2d625-0: prompt: 'You know, sometimes I think about how easy it is to get away with a crime if you know how to cover your tracks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 7025, 358, 1744, 911, 1246, 4135, 432, 374, 311, 633, 3123, 448, 264, 9778, 421, 498, 1414, 1246, 311, 3421, 697, 13918, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,804 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-26f723855410401cbc65b87710da12fa-0.
2025-12-19 23:59:10,805 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a7decd346fe54443982eb5d05dca8363-0.
2025-12-19 23:59:10,806 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3601e6c7838b405fb91e8678ace2d625-0.
2025-12-19 23:59:10,808 - vllm.core.scheduler - INFO - Pending queue size: (138)
2025-12-19 23:59:10,852 - vllm.core.scheduler - INFO - Pending queue size: (138)
2025-12-19 23:59:10,859 - vllm.entrypoints.logger - INFO - Received request cmpl-2e2d2cf8ff0d4f659855807ade11ee0c-0: prompt: 'Come on, wave goodbye to the other kids.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [28851, 389, 11, 12060, 46455, 311, 279, 1008, 6837, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,895 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2e2d2cf8ff0d4f659855807ade11ee0c-0.
2025-12-19 23:59:10,898 - vllm.core.scheduler - INFO - Pending queue size: (139)
2025-12-19 23:59:10,932 - vllm.entrypoints.logger - INFO - Received request cmpl-24955b37fd5743f589401f5109ab34e0-0: prompt: "Hey, it's good to see you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 432, 594, 1661, 311, 1490, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,941 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-24955b37fd5743f589401f5109ab34e0-0.
2025-12-19 23:59:10,943 - vllm.entrypoints.logger - INFO - Received request cmpl-fabed41a95094749b5f25ce7714c6c3a-0: prompt: 'You know, B, I realized something the other day.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 425, 11, 358, 15043, 2494, 279, 1008, 1899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,944 - vllm.core.scheduler - INFO - Pending queue size: (140)
2025-12-19 23:59:10,954 - vllm.entrypoints.logger - INFO - Received request cmpl-efabc3cee1fd48c0ba67202add4cae89-0: prompt: "I've been thinking a lot about my plans for the future.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 7274, 264, 2696, 911, 847, 6649, 369, 279, 3853, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:10,987 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fabed41a95094749b5f25ce7714c6c3a-0.
2025-12-19 23:59:10,989 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-efabc3cee1fd48c0ba67202add4cae89-0.
2025-12-19 23:59:10,992 - vllm.core.scheduler - INFO - Pending queue size: (142)
2025-12-19 23:59:11,014 - vllm.entrypoints.logger - INFO - Received request cmpl-bc1cacde4fa94a4ea05b4feec145a00a-0: prompt: 'Hey, have you heard back from that literary agent I submitted your manuscript to?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 1182, 504, 429, 31365, 8315, 358, 14634, 697, 46813, 311, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,035 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc1cacde4fa94a4ea05b4feec145a00a-0.
2025-12-19 23:59:11,038 - vllm.core.scheduler - INFO - Pending queue size: (143)
2025-12-19 23:59:11,048 - vllm.entrypoints.logger - INFO - Received request cmpl-a4982dcc14104e16abea58a6d3232b03-0: prompt: "Mentor, I can't find my cat anywhere. I've looked in the bathroom, under the bed, and in the closet. Where could she be?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 646, 944, 1477, 847, 8251, 12379, 13, 358, 3003, 6966, 304, 279, 14852, 11, 1212, 279, 4845, 11, 323, 304, 279, 31944, 13, 10967, 1410, 1340, 387, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,057 - vllm.entrypoints.logger - INFO - Received request cmpl-4bbb13cb6de9419a821fb7803415661c-0: prompt: "Do you remember when I told you about how I always wanted this house? I still can't believe it's mine.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5404, 498, 6099, 979, 358, 3229, 498, 911, 1246, 358, 2677, 4829, 419, 3753, 30, 358, 2058, 646, 944, 4411, 432, 594, 10485, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,073 - vllm.entrypoints.logger - INFO - Received request cmpl-cd9ddd89a104498ebfab01fb2b3e7a89-0: prompt: 'I have some serious work to do today. I have found out that the mayor has been taking bribes!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 1045, 6001, 975, 311, 653, 3351, 13, 358, 614, 1730, 700, 429, 279, 16923, 702, 1012, 4633, 75612, 288, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,082 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a4982dcc14104e16abea58a6d3232b03-0.
2025-12-19 23:59:11,083 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4bbb13cb6de9419a821fb7803415661c-0.
2025-12-19 23:59:11,084 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cd9ddd89a104498ebfab01fb2b3e7a89-0.
2025-12-19 23:59:11,087 - vllm.core.scheduler - INFO - Pending queue size: (146)
2025-12-19 23:59:11,099 - vllm.entrypoints.logger - INFO - Received request cmpl-ecd480aff1104c75a4728b846afbb26c-0: prompt: "I can't wait for the school's annual sports day. I'm pretty confident in my skills.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 369, 279, 2906, 594, 9775, 9833, 1899, 13, 358, 2776, 5020, 16506, 304, 847, 7361, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,122 - vllm.entrypoints.logger - INFO - Received request cmpl-6044cc01488348098e8fe7d8d117afb8-0: prompt: 'Hey Neighbor B, I have something to tell you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 97163, 425, 11, 358, 614, 2494, 311, 3291, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,125 - vllm.entrypoints.logger - INFO - Received request cmpl-a3c8e272995843f282dfe325036137e4-0: prompt: 'You know what I did today? I finally mustered up the courage to sing a solo in my singing group!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 1128, 358, 1521, 3351, 30, 358, 5499, 1969, 12336, 705, 279, 24744, 311, 7780, 264, 13529, 304, 847, 25083, 1874, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,131 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ecd480aff1104c75a4728b846afbb26c-0.
2025-12-19 23:59:11,132 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6044cc01488348098e8fe7d8d117afb8-0.
2025-12-19 23:59:11,133 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a3c8e272995843f282dfe325036137e4-0.
2025-12-19 23:59:11,136 - vllm.core.scheduler - INFO - Pending queue size: (149)
2025-12-19 23:59:11,150 - vllm.entrypoints.logger - INFO - Received request cmpl-a84c02052a964296bc37f0a438c26cec-0: prompt: "I love reading on the subway. It's such a peaceful way to relax after a long day at work.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 5290, 389, 279, 43246, 13, 1084, 594, 1741, 264, 25650, 1616, 311, 11967, 1283, 264, 1293, 1899, 518, 975, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,179 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a84c02052a964296bc37f0a438c26cec-0.
2025-12-19 23:59:11,181 - vllm.entrypoints.logger - INFO - Received request cmpl-fea8e9578a4e4ec484f70b285056564e-0: prompt: "I'm so excited for my date tonight! I feel like I look pretty good.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 369, 847, 2400, 17913, 0, 358, 2666, 1075, 358, 1401, 5020, 1661, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,182 - vllm.core.scheduler - INFO - Pending queue size: (150)
2025-12-19 23:59:11,184 - vllm.entrypoints.logger - INFO - Received request cmpl-fd12a24341e0427d8e0a57d54059f606-0: prompt: "I'm really worried about my job. I think I might get laid off soon.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 17811, 911, 847, 2618, 13, 358, 1744, 358, 2578, 633, 17113, 1007, 5135, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,188 - vllm.entrypoints.logger - INFO - Received request cmpl-bc2c9d85e6b74ec498f35939b13fde37-0: prompt: "Hey there, Neighbor B! How's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 97163, 425, 0, 2585, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,196 - vllm.entrypoints.logger - INFO - Received request cmpl-8ba7a40c72b740568483c37e4b635d3c-0: prompt: "We had so much fun last night staying up until 3am talking and laughing. It was great getting to know each other better and now I don't have to sleep since I'm already awake!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1654, 1030, 773, 1753, 2464, 1537, 3729, 19429, 705, 3080, 220, 18, 309, 7404, 323, 31581, 13, 1084, 572, 2244, 3709, 311, 1414, 1817, 1008, 2664, 323, 1431, 358, 1513, 944, 614, 311, 6084, 2474, 358, 2776, 2669, 34347, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,207 - vllm.entrypoints.logger - INFO - Received request cmpl-ba3611044a364601a3e94f3779d43640-0: prompt: 'Hey, Neighbors A! Long time no talk, how have you been?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 362, 0, 5724, 882, 902, 3061, 11, 1246, 614, 498, 1012, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,226 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fea8e9578a4e4ec484f70b285056564e-0.
2025-12-19 23:59:11,227 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fd12a24341e0427d8e0a57d54059f606-0.
2025-12-19 23:59:11,228 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc2c9d85e6b74ec498f35939b13fde37-0.
2025-12-19 23:59:11,229 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8ba7a40c72b740568483c37e4b635d3c-0.
2025-12-19 23:59:11,230 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ba3611044a364601a3e94f3779d43640-0.
2025-12-19 23:59:11,233 - vllm.core.scheduler - INFO - Pending queue size: (155)
2025-12-19 23:59:11,243 - vllm.entrypoints.logger - INFO - Received request cmpl-8ec7df03a8fd4ce2ac0b0bbba63f0f56-0: prompt: 'Hey, did I tell you that I got invited to a party by my friend Sarah?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 429, 358, 2684, 18218, 311, 264, 4614, 553, 847, 4238, 20445, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,249 - vllm.entrypoints.logger - INFO - Received request cmpl-810162d3134f4341a3af698a2413f596-0: prompt: "Hey there, how's your day going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=43, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 594, 697, 1899, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,267 - vllm.entrypoints.logger - INFO - Received request cmpl-0f30f4d603b94cf29bfb75b472c4ad8d-0: prompt: "You know what? I'm always proud of myself when I can be counted on to keep secrets and fulfill my promises.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 1128, 30, 358, 2776, 2677, 12409, 315, 7037, 979, 358, 646, 387, 29994, 389, 311, 2506, 23594, 323, 20423, 847, 20647, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,276 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8ec7df03a8fd4ce2ac0b0bbba63f0f56-0.
2025-12-19 23:59:11,278 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-810162d3134f4341a3af698a2413f596-0.
2025-12-19 23:59:11,279 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0f30f4d603b94cf29bfb75b472c4ad8d-0.
2025-12-19 23:59:11,281 - vllm.core.scheduler - INFO - Pending queue size: (158)
2025-12-19 23:59:11,326 - vllm.core.scheduler - INFO - Pending queue size: (158)
2025-12-19 23:59:11,326 - vllm.entrypoints.logger - INFO - Received request cmpl-74a21467b287460586d1970d31393935-0: prompt: 'Hey, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,342 - vllm.entrypoints.logger - INFO - Received request cmpl-fa864bcf14b54e3c9db25b4a88a99c62-0: prompt: ' Boss, I have some exciting news! I booked my first show at the Blue Note!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [31569, 11, 358, 614, 1045, 13245, 3669, 0, 358, 32970, 847, 1156, 1473, 518, 279, 8697, 7036, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,343 - vllm.entrypoints.logger - INFO - Received request cmpl-74d6dc83d42548adad18260126e278e9-0: prompt: 'This is such a beautiful day at the beach. The warm sand between my toes and the sound of the waves is so calming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 1741, 264, 6233, 1899, 518, 279, 11321, 13, 576, 8205, 9278, 1948, 847, 44613, 323, 279, 5112, 315, 279, 16876, 374, 773, 77629, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,345 - vllm.entrypoints.logger - INFO - Received request cmpl-146f349b811e4f96ae2ed8e2d8b4ed52-0: prompt: 'How are you feeling today, Doctor?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4340, 525, 498, 8266, 3351, 11, 18635, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,352 - vllm.entrypoints.logger - INFO - Received request cmpl-c84e07d424834a408710e46206386cf6-0: prompt: 'Thank you so much for helping me out today,. I really appreciate it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13060, 498, 773, 1753, 369, 10476, 752, 700, 3351, 17515, 358, 2167, 15401, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,355 - vllm.entrypoints.logger - INFO - Received request cmpl-98bd5ba15838465a8f30be2daa066315-0: prompt: ', have you been following the news about the economic crisis?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 614, 498, 1012, 2701, 279, 3669, 911, 279, 6955, 11251, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,370 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-74a21467b287460586d1970d31393935-0.
2025-12-19 23:59:11,371 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fa864bcf14b54e3c9db25b4a88a99c62-0.
2025-12-19 23:59:11,372 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-74d6dc83d42548adad18260126e278e9-0.
2025-12-19 23:59:11,373 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-146f349b811e4f96ae2ed8e2d8b4ed52-0.
2025-12-19 23:59:11,375 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c84e07d424834a408710e46206386cf6-0.
2025-12-19 23:59:11,376 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-98bd5ba15838465a8f30be2daa066315-0.
2025-12-19 23:59:11,378 - vllm.core.scheduler - WARNING - Sequence group cmpl-1fdee2678fd2434f935cdf07d9c1f006-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51
2025-12-19 23:59:11,378 - vllm.entrypoints.logger - INFO - Received request cmpl-cc3c30be93c4448da8f7ea8723cb7a0b-0: prompt: "I'm feeling really motivated lately. I feel like I'm on the right track to achieving my goals.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 2167, 26664, 30345, 13, 358, 2666, 1075, 358, 2776, 389, 279, 1290, 3754, 311, 31045, 847, 8845, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,379 - vllm.core.scheduler - INFO - Pending queue size: (164)
2025-12-19 23:59:11,391 - vllm.entrypoints.logger - INFO - Received request cmpl-6746ad6206e34befabc651a1e680da38-0: prompt: 'Hey, how did your math homework go last night?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 1521, 697, 6888, 28459, 728, 1537, 3729, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,412 - vllm.entrypoints.logger - INFO - Received request cmpl-4d7c6702bd1e4d49bc9f140936dbc767-0: prompt: "I can't believe you did that, Co-workers B. You know how important this project is to me and now it's all messed up.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 498, 1521, 429, 11, 3539, 62284, 425, 13, 1446, 1414, 1246, 2989, 419, 2390, 374, 311, 752, 323, 1431, 432, 594, 678, 64202, 705, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,417 - vllm.entrypoints.logger - INFO - Received request cmpl-cad071cd97154fa3930aaa3c78d09aea-0: prompt: "Hey, how's your day going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 697, 1899, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,424 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cc3c30be93c4448da8f7ea8723cb7a0b-0.
2025-12-19 23:59:11,425 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6746ad6206e34befabc651a1e680da38-0.
2025-12-19 23:59:11,426 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d7c6702bd1e4d49bc9f140936dbc767-0.
2025-12-19 23:59:11,427 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cad071cd97154fa3930aaa3c78d09aea-0.
2025-12-19 23:59:11,428 - vllm.entrypoints.logger - INFO - Received request cmpl-21ae8244c1f944e593a49b5e565ce8f8-0: prompt: "It's been a tough few weeks. My father's health has been declining rapidly, and it's been hard to balance work and taking care of him.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 1012, 264, 11045, 2421, 5555, 13, 3017, 6981, 594, 2820, 702, 1012, 42748, 18512, 11, 323, 432, 594, 1012, 2588, 311, 8172, 975, 323, 4633, 2453, 315, 1435, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,430 - vllm.core.scheduler - INFO - Pending queue size: (168)
2025-12-19 23:59:11,442 - vllm.entrypoints.logger - INFO - Received request cmpl-343ce057f2bd4affbede4128f7f5317c-0: prompt: 'Honey, do you need any help with the dishes tonight?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [39, 2534, 11, 653, 498, 1184, 894, 1492, 448, 279, 25779, 17913, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,473 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-21ae8244c1f944e593a49b5e565ce8f8-0.
2025-12-19 23:59:11,477 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-343ce057f2bd4affbede4128f7f5317c-0.
2025-12-19 23:59:11,479 - vllm.entrypoints.logger - INFO - Received request cmpl-84600c59aba948e8b430234ac2897826-0: prompt: 'Hi Mentor, how are you feeling today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 1246, 525, 498, 8266, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,480 - vllm.core.scheduler - INFO - Pending queue size: (170)
2025-12-19 23:59:11,523 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-84600c59aba948e8b430234ac2897826-0.
2025-12-19 23:59:11,526 - vllm.core.scheduler - INFO - Pending queue size: (171)
2025-12-19 23:59:11,559 - vllm.entrypoints.logger - INFO - Received request cmpl-6fb47396a66e4c54a940ac0a7e71a777-0: prompt: "I don't know why I'm feeling like this. My eyes keep watering and I feel so overwhelmed.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 3170, 358, 2776, 8266, 1075, 419, 13, 3017, 6414, 2506, 72741, 323, 358, 2666, 773, 42106, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,570 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6fb47396a66e4c54a940ac0a7e71a777-0.
2025-12-19 23:59:11,573 - vllm.core.scheduler - INFO - Pending queue size: (172)
2025-12-19 23:59:11,597 - vllm.entrypoints.logger - INFO - Received request cmpl-341b21efcbfc401e8ec9ab9c155695ef-0: prompt: "Hey Coach, how's it going today?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 27176, 11, 1246, 594, 432, 2087, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,600 - vllm.entrypoints.logger - INFO - Received request cmpl-ad284ffe3ec5455fa2d4a04e55b80906-0: prompt: ', your drink is empty. Let me get you a new one.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 697, 7027, 374, 4287, 13, 6771, 752, 633, 498, 264, 501, 825, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,617 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-341b21efcbfc401e8ec9ab9c155695ef-0.
2025-12-19 23:59:11,618 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ad284ffe3ec5455fa2d4a04e55b80906-0.
2025-12-19 23:59:11,620 - vllm.core.scheduler - INFO - Pending queue size: (174)
2025-12-19 23:59:11,627 - vllm.entrypoints.logger - INFO - Received request cmpl-ed9bda8e1f5c44978835997a8c4c802c-0: prompt: "I don't know what to do. I feel like I'm addicted to flirting.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 1128, 311, 653, 13, 358, 2666, 1075, 358, 2776, 56627, 311, 87242, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,654 - vllm.entrypoints.logger - INFO - Received request cmpl-d71a65fade4a4dd9857658269d6a1c5a-0: prompt: 'Hi there, how are you doing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1246, 525, 498, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,664 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ed9bda8e1f5c44978835997a8c4c802c-0.
2025-12-19 23:59:11,665 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d71a65fade4a4dd9857658269d6a1c5a-0.
2025-12-19 23:59:11,668 - vllm.core.scheduler - INFO - Pending queue size: (176)
2025-12-19 23:59:11,713 - vllm.core.scheduler - INFO - Pending queue size: (176)
2025-12-19 23:59:11,756 - vllm.core.scheduler - INFO - Pending queue size: (176)
2025-12-19 23:59:11,798 - vllm.core.scheduler - INFO - Pending queue size: (176)
2025-12-19 23:59:11,840 - vllm.core.scheduler - INFO - Pending queue size: (176)
2025-12-19 23:59:11,882 - vllm.core.scheduler - INFO - Pending queue size: (176)
2025-12-19 23:59:11,899 - vllm.entrypoints.logger - INFO - Received request cmpl-4b860790e98c4f4f9aab2c1603a2f99e-0: prompt: 'Can you believe it, B? I was walking in the woods and suddenly, a bear appeared out of nowhere.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 11, 425, 30, 358, 572, 11435, 304, 279, 32533, 323, 14843, 11, 264, 11722, 9723, 700, 315, 27112, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,911 - vllm.entrypoints.logger - INFO - Received request cmpl-bc11c804f0004846ac458e361f746950-0: prompt: 'Hey there, Neighbor B! How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 97163, 425, 0, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,924 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4b860790e98c4f4f9aab2c1603a2f99e-0.
2025-12-19 23:59:11,926 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc11c804f0004846ac458e361f746950-0.
2025-12-19 23:59:11,926 - vllm.entrypoints.logger - INFO - Received request cmpl-03ca708491a54f9cb8f89f4b3c129501-0: prompt: ", I'm really trying to figure out how to be independent these days.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 2167, 4460, 311, 7071, 700, 1246, 311, 387, 9489, 1493, 2849, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,927 - vllm.entrypoints.logger - INFO - Received request cmpl-574392e0dd594c7d8f9ea30c196fcc12-0: prompt: 'Hey, do you ever wonder about the world around us? Like, how everything works and why things happen?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 3512, 5775, 911, 279, 1879, 2163, 601, 30, 8909, 11, 1246, 4297, 4278, 323, 3170, 2513, 3537, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,928 - vllm.core.scheduler - INFO - Pending queue size: (178)
2025-12-19 23:59:11,946 - vllm.entrypoints.logger - INFO - Received request cmpl-8d6be15a2c5e4b21b50552416acc0ebb-0: prompt: 'Mentor, I am a very determined person. If I set my mind to something, nothing can stop me from achieving it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1079, 264, 1602, 10838, 1697, 13, 1416, 358, 738, 847, 3971, 311, 2494, 11, 4302, 646, 2936, 752, 504, 31045, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,972 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-03ca708491a54f9cb8f89f4b3c129501-0.
2025-12-19 23:59:11,973 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-574392e0dd594c7d8f9ea30c196fcc12-0.
2025-12-19 23:59:11,975 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8d6be15a2c5e4b21b50552416acc0ebb-0.
2025-12-19 23:59:11,977 - vllm.core.scheduler - INFO - Pending queue size: (181)
2025-12-19 23:59:11,978 - vllm.entrypoints.logger - INFO - Received request cmpl-4ef960498f5e4420bac7d3c006660a9f-0: prompt: ' Hey, I met someone today with a beautiful flower tattoo on their arm.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [27553, 11, 358, 2270, 4325, 3351, 448, 264, 6233, 22351, 31794, 389, 862, 6773, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,982 - vllm.entrypoints.logger - INFO - Received request cmpl-c47ccfa8be2f4573b01b6f2b3133925f-0: prompt: ' That was odd, did you see that student just leave the classroom?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2938, 572, 10322, 11, 1521, 498, 1490, 429, 5458, 1101, 5274, 279, 24017, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:11,993 - vllm.entrypoints.logger - INFO - Received request cmpl-ced0099ed6b84497af232a53801f3092-0: prompt: "I can't believe I found your passport so quickly, it was so easy!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 1730, 697, 25458, 773, 6157, 11, 432, 572, 773, 4135, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,021 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4ef960498f5e4420bac7d3c006660a9f-0.
2025-12-19 23:59:12,022 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c47ccfa8be2f4573b01b6f2b3133925f-0.
2025-12-19 23:59:12,024 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ced0099ed6b84497af232a53801f3092-0.
2025-12-19 23:59:12,026 - vllm.core.scheduler - INFO - Pending queue size: (184)
2025-12-19 23:59:12,046 - vllm.entrypoints.logger - INFO - Received request cmpl-4f819821db2748f08026e751bf747225-0: prompt: 'Hey, Neighbor B! How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 97163, 425, 0, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,070 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4f819821db2748f08026e751bf747225-0.
2025-12-19 23:59:12,073 - vllm.core.scheduler - INFO - Pending queue size: (185)
2025-12-19 23:59:12,075 - vllm.entrypoints.logger - INFO - Received request cmpl-79a5b75da64f4702bc62f11a35731751-0: prompt: "Hey, B! Have you heard? I've decided to pursue a career as a professional choreographer!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 0, 12243, 498, 6617, 30, 358, 3003, 6635, 311, 22729, 264, 6931, 438, 264, 6584, 49571, 41506, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,102 - vllm.entrypoints.logger - INFO - Received request cmpl-0d110ec4cda94ee885d9c1a1bfca14ac-0: prompt: "Hey, Babe, how's it going today?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 73348, 11, 1246, 594, 432, 2087, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,116 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-79a5b75da64f4702bc62f11a35731751-0.
2025-12-19 23:59:12,119 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d110ec4cda94ee885d9c1a1bfca14ac-0.
2025-12-19 23:59:12,122 - vllm.core.scheduler - INFO - Pending queue size: (187)
2025-12-19 23:59:12,167 - vllm.core.scheduler - INFO - Pending queue size: (187)
2025-12-19 23:59:12,212 - vllm.core.scheduler - INFO - Pending queue size: (187)
2025-12-19 23:59:12,255 - vllm.core.scheduler - INFO - Pending queue size: (187)
2025-12-19 23:59:12,298 - vllm.core.scheduler - INFO - Pending queue size: (187)
2025-12-19 23:59:12,342 - vllm.core.scheduler - INFO - Pending queue size: (187)
2025-12-19 23:59:12,385 - vllm.core.scheduler - INFO - Pending queue size: (187)
2025-12-19 23:59:12,406 - vllm.entrypoints.logger - INFO - Received request cmpl-f78ae0073f7443628e2e7befd46e5afd-0: prompt: 'Hey, did you see me flying over your new house today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 1490, 752, 16307, 916, 697, 501, 3753, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,427 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f78ae0073f7443628e2e7befd46e5afd-0.
2025-12-19 23:59:12,434 - vllm.core.scheduler - INFO - Pending queue size: (188)
2025-12-19 23:59:12,455 - vllm.entrypoints.logger - INFO - Received request cmpl-6bdc2bf4a72c4c099e60cc35d5166ea0-0: prompt: 'You look especially beautiful today, my love.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1401, 5310, 6233, 3351, 11, 847, 2948, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,476 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6bdc2bf4a72c4c099e60cc35d5166ea0-0.
2025-12-19 23:59:12,480 - vllm.core.scheduler - INFO - Pending queue size: (189)
2025-12-19 23:59:12,523 - vllm.core.scheduler - INFO - Pending queue size: (189)
2025-12-19 23:59:12,566 - vllm.core.scheduler - INFO - Pending queue size: (189)
2025-12-19 23:59:12,602 - vllm.entrypoints.logger - INFO - Received request cmpl-ea01bfdaab1841869b43eaa9f40bf804-0: prompt: "I'm really excited about the progress I've made with my plan. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 12035, 911, 279, 5098, 358, 3003, 1865, 448, 847, 3119, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,609 - vllm.entrypoints.logger - INFO - Received request cmpl-2f6e3e478a5246a983d99a7b1edcd910-0: prompt: 'Have you noticed anything different about my hair lately?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 13686, 4113, 2155, 911, 847, 6869, 30345, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,611 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ea01bfdaab1841869b43eaa9f40bf804-0.
2025-12-19 23:59:12,615 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2f6e3e478a5246a983d99a7b1edcd910-0.
2025-12-19 23:59:12,618 - vllm.core.scheduler - INFO - Pending queue size: (191)
2025-12-19 23:59:12,662 - vllm.core.scheduler - INFO - Pending queue size: (191)
2025-12-19 23:59:12,704 - vllm.core.scheduler - INFO - Pending queue size: (191)
2025-12-19 23:59:12,746 - vllm.core.scheduler - INFO - Pending queue size: (191)
2025-12-19 23:59:12,790 - vllm.core.scheduler - INFO - Pending queue size: (191)
2025-12-19 23:59:12,835 - vllm.core.scheduler - INFO - Pending queue size: (191)
2025-12-19 23:59:12,878 - vllm.core.scheduler - INFO - Pending queue size: (191)
2025-12-19 23:59:12,921 - vllm.core.scheduler - INFO - Pending queue size: (191)
2025-12-19 23:59:12,922 - vllm.entrypoints.logger - INFO - Received request cmpl-0d2e426b2b91482cbed49c1acf51f6ba-0: prompt: ', I had the wildest night last night.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 1030, 279, 30231, 4979, 3729, 1537, 3729, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:12,963 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d2e426b2b91482cbed49c1acf51f6ba-0.
2025-12-19 23:59:12,966 - vllm.core.scheduler - INFO - Pending queue size: (192)
2025-12-19 23:59:12,997 - vllm.entrypoints.logger - INFO - Received request cmpl-18623868e21844588d4263f0d89203db-0: prompt: ' lost the key to a lock and now I feel sad. I used that lock to keep my bike safe, but now I have to find a new way to keep it secure.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5558, 279, 1376, 311, 264, 5296, 323, 1431, 358, 2666, 12421, 13, 358, 1483, 429, 5296, 311, 2506, 847, 12963, 6092, 11, 714, 1431, 358, 614, 311, 1477, 264, 501, 1616, 311, 2506, 432, 9767, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,000 - vllm.entrypoints.logger - INFO - Received request cmpl-6f050eec0c1a4ff38eb92baa51c479cd-0: prompt: " I am so angry right now! My mother doesn't understand me, my father is too strict, and my brother always gets all the attention!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1079, 773, 18514, 1290, 1431, 0, 3017, 6554, 3171, 944, 3535, 752, 11, 847, 6981, 374, 2238, 7304, 11, 323, 847, 10641, 2677, 5221, 678, 279, 6529, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,009 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-18623868e21844588d4263f0d89203db-0.
2025-12-19 23:59:13,010 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f050eec0c1a4ff38eb92baa51c479cd-0.
2025-12-19 23:59:13,012 - vllm.core.scheduler - INFO - Pending queue size: (194)
2025-12-19 23:59:13,014 - vllm.entrypoints.logger - INFO - Received request cmpl-64caaefcf6474bf59d4fe12211584d44-0: prompt: 'Doctor, I was caught in heavy traffic yesterday and I missed the chance to buy the medicine before the pharmacy closed.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 572, 10568, 304, 8811, 9442, 13671, 323, 358, 13628, 279, 6012, 311, 3695, 279, 15712, 1573, 279, 32300, 7877, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,056 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-64caaefcf6474bf59d4fe12211584d44-0.
2025-12-19 23:59:13,059 - vllm.core.scheduler - INFO - Pending queue size: (195)
2025-12-19 23:59:13,101 - vllm.core.scheduler - INFO - Pending queue size: (195)
2025-12-19 23:59:13,143 - vllm.core.scheduler - INFO - Pending queue size: (195)
2025-12-19 23:59:13,165 - vllm.entrypoints.logger - INFO - Received request cmpl-4f30f4258d444d14869c5bdd6f182ac5-0: prompt: "Wow, this wedding is beautiful. I'm glad we came.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 419, 13008, 374, 6233, 13, 358, 2776, 15713, 582, 3697, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,169 - vllm.entrypoints.logger - INFO - Received request cmpl-ba76268add974e8cad260645a62edc46-0: prompt: "Hey, did I tell you about the party I went to at Joe's house over the weekend?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 911, 279, 4614, 358, 3937, 311, 518, 12846, 594, 3753, 916, 279, 9001, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,185 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4f30f4258d444d14869c5bdd6f182ac5-0.
2025-12-19 23:59:13,187 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ba76268add974e8cad260645a62edc46-0.
2025-12-19 23:59:13,189 - vllm.core.scheduler - INFO - Pending queue size: (197)
2025-12-19 23:59:13,226 - vllm.entrypoints.logger - INFO - Received request cmpl-36861d9663ac4922b86781e0e528365e-0: prompt: "Hey, what's up, Babe?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1128, 594, 705, 11, 73348, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,232 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-36861d9663ac4922b86781e0e528365e-0.
2025-12-19 23:59:13,235 - vllm.core.scheduler - INFO - Pending queue size: (198)
2025-12-19 23:59:13,279 - vllm.core.scheduler - INFO - Pending queue size: (198)
2025-12-19 23:59:13,281 - vllm.entrypoints.logger - INFO - Received request cmpl-f738407042804a12b2470b71a81926df-0: prompt: ' I need to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1184, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,321 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f738407042804a12b2470b71a81926df-0.
2025-12-19 23:59:13,324 - vllm.core.scheduler - INFO - Pending queue size: (199)
2025-12-19 23:59:13,367 - vllm.core.scheduler - INFO - Pending queue size: (199)
2025-12-19 23:59:13,408 - vllm.engine.metrics - INFO - Avg prompt throughput: 518.9 tokens/s, Avg generation throughput: 27.8 tokens/s, Running: 512 reqs, Swapped: 95 reqs, Pending: 103 reqs, GPU KV cache usage: 5.8%, CPU KV cache usage: 1.3%.
2025-12-19 23:59:13,411 - vllm.core.scheduler - INFO - Pending queue size: (199)
2025-12-19 23:59:13,453 - vllm.core.scheduler - INFO - Pending queue size: (199)
2025-12-19 23:59:13,494 - vllm.core.scheduler - INFO - Pending queue size: (199)
2025-12-19 23:59:13,495 - vllm.entrypoints.logger - INFO - Received request cmpl-046aec3fc6d7447f87ec272c1ea0fa71-0: prompt: "This is a beautiful day, don't you think?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 264, 6233, 1899, 11, 1513, 944, 498, 1744, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,535 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-046aec3fc6d7447f87ec272c1ea0fa71-0.
2025-12-19 23:59:13,538 - vllm.core.scheduler - INFO - Pending queue size: (200)
2025-12-19 23:59:13,544 - vllm.entrypoints.logger - INFO - Received request cmpl-b7602a115fd147afa922596453a6a536-0: prompt: "Can you believe it's been a few weeks since I won the lottery?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 594, 1012, 264, 2421, 5555, 2474, 358, 2765, 279, 38239, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,580 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b7602a115fd147afa922596453a6a536-0.
2025-12-19 23:59:13,585 - vllm.core.scheduler - INFO - Pending queue size: (201)
2025-12-19 23:59:13,633 - vllm.core.scheduler - WARNING - Sequence group cmpl-bf55f9cb60104d0db5a751747ca0eff3-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101
2025-12-19 23:59:13,634 - vllm.core.scheduler - INFO - Pending queue size: (201)
2025-12-19 23:59:13,677 - vllm.core.scheduler - INFO - Pending queue size: (201)
2025-12-19 23:59:13,718 - vllm.core.scheduler - INFO - Pending queue size: (201)
2025-12-19 23:59:13,760 - vllm.core.scheduler - INFO - Pending queue size: (201)
2025-12-19 23:59:13,805 - vllm.core.scheduler - INFO - Pending queue size: (201)
2025-12-19 23:59:13,848 - vllm.core.scheduler - INFO - Pending queue size: (201)
2025-12-19 23:59:13,869 - vllm.entrypoints.logger - INFO - Received request cmpl-2ff4934124424c638d7bea4055117f78-0: prompt: "Look at the camera, B. It's important to make eye contact when taking a picture.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10380, 518, 279, 6249, 11, 425, 13, 1084, 594, 2989, 311, 1281, 7912, 3645, 979, 4633, 264, 6802, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,889 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2ff4934124424c638d7bea4055117f78-0.
2025-12-19 23:59:13,891 - vllm.core.scheduler - INFO - Pending queue size: (202)
2025-12-19 23:59:13,933 - vllm.core.scheduler - INFO - Pending queue size: (202)
2025-12-19 23:59:13,938 - vllm.entrypoints.logger - INFO - Received request cmpl-ccf392435eee406ba8d2d864a17f7425-0: prompt: 'You need to make a choice. Either you can come with me or you can stay here.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1184, 311, 1281, 264, 5754, 13, 20988, 498, 646, 2525, 448, 752, 476, 498, 646, 4717, 1588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:13,978 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ccf392435eee406ba8d2d864a17f7425-0.
2025-12-19 23:59:13,981 - vllm.core.scheduler - INFO - Pending queue size: (203)
2025-12-19 23:59:14,028 - vllm.core.scheduler - INFO - Pending queue size: (203)
2025-12-19 23:59:14,059 - vllm.entrypoints.logger - INFO - Received request cmpl-a6705e64a0884f94896e33ee232d4e5d-0: prompt: "So, I've been thinking about my options for our future.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 358, 3003, 1012, 7274, 911, 847, 2606, 369, 1039, 3853, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,070 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a6705e64a0884f94896e33ee232d4e5d-0.
2025-12-19 23:59:14,073 - vllm.core.scheduler - INFO - Pending queue size: (204)
2025-12-19 23:59:14,096 - vllm.entrypoints.logger - INFO - Received request cmpl-6853c49263b64b5289403d27436bbc43-0: prompt: "Mentor, I've been thinking about what you said about putting myself first. I really want to make my health and happiness a priority.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 3003, 1012, 7274, 911, 1128, 498, 1053, 911, 10687, 7037, 1156, 13, 358, 2167, 1366, 311, 1281, 847, 2820, 323, 23009, 264, 10619, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,115 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6853c49263b64b5289403d27436bbc43-0.
2025-12-19 23:59:14,117 - vllm.core.scheduler - INFO - Pending queue size: (205)
2025-12-19 23:59:14,123 - vllm.entrypoints.logger - INFO - Received request cmpl-1a2a334eecb34bc7a22944e8ae8311e8-0: prompt: 'Hey, have you seen my new car yet?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3884, 847, 501, 1803, 3602, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,155 - vllm.entrypoints.logger - INFO - Received request cmpl-8abe8e27d4d842d6960b0811c86f0617-0: prompt: "Hey, I just wanted to tell you that I think you're really intelligent.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1101, 4829, 311, 3291, 498, 429, 358, 1744, 498, 2299, 2167, 24514, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,160 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1a2a334eecb34bc7a22944e8ae8311e8-0.
2025-12-19 23:59:14,161 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8abe8e27d4d842d6960b0811c86f0617-0.
2025-12-19 23:59:14,164 - vllm.core.scheduler - INFO - Pending queue size: (207)
2025-12-19 23:59:14,208 - vllm.core.scheduler - INFO - Pending queue size: (207)
2025-12-19 23:59:14,230 - vllm.entrypoints.logger - INFO - Received request cmpl-d69d6011923b41b79b58e49592798eb7-0: prompt: "I can't believe she's really leaving. I feel like I've lost a part of myself.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1340, 594, 2167, 9380, 13, 358, 2666, 1075, 358, 3003, 5558, 264, 949, 315, 7037, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,253 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d69d6011923b41b79b58e49592798eb7-0.
2025-12-19 23:59:14,255 - vllm.core.scheduler - INFO - Pending queue size: (208)
2025-12-19 23:59:14,266 - vllm.entrypoints.logger - INFO - Received request cmpl-93794cdbf4ba4276aa7a831d4f50f16c-0: prompt: 'I was thinking about you earlier today, B. How are you doing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 7274, 911, 498, 6788, 3351, 11, 425, 13, 2585, 525, 498, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,297 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-93794cdbf4ba4276aa7a831d4f50f16c-0.
2025-12-19 23:59:14,300 - vllm.core.scheduler - INFO - Pending queue size: (209)
2025-12-19 23:59:14,341 - vllm.core.scheduler - INFO - Pending queue size: (209)
2025-12-19 23:59:14,341 - vllm.entrypoints.logger - INFO - Received request cmpl-a3a2f3c730a941c3b80fca82533b96fc-0: prompt: 'Hi Neighbor B, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 97163, 425, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,383 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a3a2f3c730a941c3b80fca82533b96fc-0.
2025-12-19 23:59:14,386 - vllm.core.scheduler - INFO - Pending queue size: (210)
2025-12-19 23:59:14,429 - vllm.core.scheduler - INFO - Pending queue size: (210)
2025-12-19 23:59:14,471 - vllm.core.scheduler - INFO - Pending queue size: (210)
2025-12-19 23:59:14,514 - vllm.core.scheduler - INFO - Pending queue size: (210)
2025-12-19 23:59:14,556 - vllm.core.scheduler - INFO - Pending queue size: (210)
2025-12-19 23:59:14,562 - vllm.entrypoints.logger - INFO - Received request cmpl-ff47ef90e25840fbac198ddd9d5cabcf-0: prompt: "I've been practicing my basketball skills a lot lately. I used to struggle with shooting, but now I feel a lot more confident.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 35566, 847, 19240, 7361, 264, 2696, 30345, 13, 358, 1483, 311, 14651, 448, 10441, 11, 714, 1431, 358, 2666, 264, 2696, 803, 16506, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,602 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ff47ef90e25840fbac198ddd9d5cabcf-0.
2025-12-19 23:59:14,607 - vllm.core.scheduler - INFO - Pending queue size: (211)
2025-12-19 23:59:14,650 - vllm.core.scheduler - INFO - Pending queue size: (211)
2025-12-19 23:59:14,685 - vllm.entrypoints.logger - INFO - Received request cmpl-64166a48eeb840d4adeae9b7048d5fec-0: prompt: "Thanks for letting me hold onto your bag, it wasn't a problem at all.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12658, 369, 20194, 752, 3331, 8630, 697, 8968, 11, 432, 5710, 944, 264, 3491, 518, 678, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,691 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-64166a48eeb840d4adeae9b7048d5fec-0.
2025-12-19 23:59:14,694 - vllm.core.scheduler - INFO - Pending queue size: (212)
2025-12-19 23:59:14,736 - vllm.core.scheduler - INFO - Pending queue size: (212)
2025-12-19 23:59:14,778 - vllm.core.scheduler - INFO - Pending queue size: (212)
2025-12-19 23:59:14,787 - vllm.entrypoints.logger - INFO - Received request cmpl-5dec77b03c2c4f4ea557a02bec0ab8a8-0: prompt: "Here, have a piece of chocolate. I bought a whole box and can't finish it all by myself.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8420, 11, 614, 264, 6573, 315, 17931, 13, 358, 10788, 264, 4361, 3745, 323, 646, 944, 6248, 432, 678, 553, 7037, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,822 - vllm.entrypoints.logger - INFO - Received request cmpl-c239347a1d6e495690d112ab765465c4-0: prompt: 'Hey B, I have something to tell you. I quit my desk job and joined the army.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 425, 11, 358, 614, 2494, 311, 3291, 498, 13, 358, 16835, 847, 18010, 2618, 323, 10859, 279, 13390, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,820 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5dec77b03c2c4f4ea557a02bec0ab8a8-0.
2025-12-19 23:59:14,826 - vllm.core.scheduler - INFO - Pending queue size: (213)
2025-12-19 23:59:14,868 - vllm.entrypoints.logger - INFO - Received request cmpl-63fab28684cd4da5b352d4b6b80dae52-0: prompt: " Here's the contract we talked about, B. I'll be responsible for your care starting from next month.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5692, 594, 279, 5116, 582, 14897, 911, 11, 425, 13, 358, 3278, 387, 8480, 369, 697, 2453, 5916, 504, 1790, 2254, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,869 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c239347a1d6e495690d112ab765465c4-0.
2025-12-19 23:59:14,871 - vllm.core.scheduler - INFO - Pending queue size: (214)
2025-12-19 23:59:14,880 - vllm.entrypoints.logger - INFO - Received request cmpl-5ee2c53b6f674f8285a3ec54b2ad64a2-0: prompt: "Oh man, I still can't believe we went skydiving together.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 883, 11, 358, 2058, 646, 944, 4411, 582, 3937, 12884, 611, 287, 3786, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,882 - vllm.entrypoints.logger - INFO - Received request cmpl-bbcf7b04e50b425a938f4af148335e5a-0: prompt: 'I just wanted to show my support for you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 4829, 311, 1473, 847, 1824, 369, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,888 - vllm.entrypoints.logger - INFO - Received request cmpl-2cb0781f4d3b4406bbdf347435ffdf23-0: prompt: "I'm feeling so lonely since we moved to our new neighborhood. I don't know anyone here.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 773, 39566, 2474, 582, 7726, 311, 1039, 501, 12534, 13, 358, 1513, 944, 1414, 5489, 1588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,898 - vllm.entrypoints.logger - INFO - Received request cmpl-aa0113d156de42b3a5c105b05fa732c3-0: prompt: "Hey Mentor, it's been a while since we talked. I wanted to catch up with you about something that's been on my mind lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 432, 594, 1012, 264, 1393, 2474, 582, 14897, 13, 358, 4829, 311, 2287, 705, 448, 498, 911, 2494, 429, 594, 1012, 389, 847, 3971, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:14,915 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-63fab28684cd4da5b352d4b6b80dae52-0.
2025-12-19 23:59:14,921 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5ee2c53b6f674f8285a3ec54b2ad64a2-0.
2025-12-19 23:59:14,922 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bbcf7b04e50b425a938f4af148335e5a-0.
2025-12-19 23:59:14,923 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2cb0781f4d3b4406bbdf347435ffdf23-0.
2025-12-19 23:59:14,924 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-aa0113d156de42b3a5c105b05fa732c3-0.
2025-12-19 23:59:14,926 - vllm.core.scheduler - INFO - Pending queue size: (219)
2025-12-19 23:59:14,972 - vllm.core.scheduler - INFO - Pending queue size: (219)
2025-12-19 23:59:14,974 - vllm.entrypoints.logger - INFO - Received request cmpl-10a0ddb806124155a7af7016be93f8b1-0: prompt: 'Hey, do you have a minute? I wanted to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 614, 264, 9383, 30, 358, 4829, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,015 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-10a0ddb806124155a7af7016be93f8b1-0.
2025-12-19 23:59:15,018 - vllm.core.scheduler - INFO - Pending queue size: (220)
2025-12-19 23:59:15,050 - vllm.entrypoints.logger - INFO - Received request cmpl-d08d9c1fa85748f7af0fac7231075586-0: prompt: "Hey, Neighbors B. I've been feeling really anxious lately and I don't know what to do about it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 13, 358, 3003, 1012, 8266, 2167, 37000, 30345, 323, 358, 1513, 944, 1414, 1128, 311, 653, 911, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,062 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d08d9c1fa85748f7af0fac7231075586-0.
2025-12-19 23:59:15,063 - vllm.entrypoints.logger - INFO - Received request cmpl-c9154d15d44e46f79f780314372d1d1b-0: prompt: 'Hey there! Fancy bumping into you in the cereal aisle.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 0, 81799, 27575, 287, 1119, 498, 304, 279, 61133, 59679, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,064 - vllm.core.scheduler - INFO - Pending queue size: (221)
2025-12-19 23:59:15,107 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c9154d15d44e46f79f780314372d1d1b-0.
2025-12-19 23:59:15,111 - vllm.core.scheduler - INFO - Pending queue size: (222)
2025-12-19 23:59:15,152 - vllm.entrypoints.logger - INFO - Received request cmpl-6f34e72fa2eb4004b24eabf195272eaa-0: prompt: 'Hey, I saw you in the hall earlier. I promised to help with the project, remember?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 5485, 498, 304, 279, 13994, 6788, 13, 358, 18951, 311, 1492, 448, 279, 2390, 11, 6099, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,155 - vllm.core.scheduler - INFO - Pending queue size: (222)
2025-12-19 23:59:15,171 - vllm.entrypoints.logger - INFO - Received request cmpl-53bc9b3044924cf0a10fe01d5c016193-0: prompt: 'I had to give a presentation in class today, and I was so scared.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 311, 2968, 264, 15496, 304, 536, 3351, 11, 323, 358, 572, 773, 26115, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,199 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f34e72fa2eb4004b24eabf195272eaa-0.
2025-12-19 23:59:15,200 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-53bc9b3044924cf0a10fe01d5c016193-0.
2025-12-19 23:59:15,203 - vllm.core.scheduler - INFO - Pending queue size: (224)
2025-12-19 23:59:15,243 - vllm.entrypoints.logger - INFO - Received request cmpl-827e82d97e2a45f7aba7fe291bd4d02c-0: prompt: ' Hey, Classmates B, you were looking pretty hot on the dance floor last night.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [27553, 11, 3228, 16457, 425, 11, 498, 1033, 3330, 5020, 4017, 389, 279, 15254, 6422, 1537, 3729, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,246 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-827e82d97e2a45f7aba7fe291bd4d02c-0.
2025-12-19 23:59:15,249 - vllm.core.scheduler - INFO - Pending queue size: (225)
2025-12-19 23:59:15,293 - vllm.entrypoints.logger - INFO - Received request cmpl-8fc61688cd084844b9a8ab4771001b76-0: prompt: "Hey, neighbor. How's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 9565, 13, 2585, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,294 - vllm.core.scheduler - INFO - Pending queue size: (225)
2025-12-19 23:59:15,337 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8fc61688cd084844b9a8ab4771001b76-0.
2025-12-19 23:59:15,340 - vllm.core.scheduler - INFO - Pending queue size: (226)
2025-12-19 23:59:15,385 - vllm.core.scheduler - INFO - Pending queue size: (226)
2025-12-19 23:59:15,429 - vllm.core.scheduler - INFO - Pending queue size: (226)
2025-12-19 23:59:15,446 - vllm.entrypoints.logger - INFO - Received request cmpl-1b257c85231648a1a5673550d40f4bfa-0: prompt: 'Ugh, I hate going to the dentist. Last time I went, I stuck my tongue out at the dentist and got scolded for it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 12213, 2087, 311, 279, 49251, 13, 7996, 882, 358, 3937, 11, 358, 15700, 847, 24459, 700, 518, 279, 49251, 323, 2684, 1136, 813, 291, 369, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,472 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1b257c85231648a1a5673550d40f4bfa-0.
2025-12-19 23:59:15,476 - vllm.core.scheduler - INFO - Pending queue size: (227)
2025-12-19 23:59:15,519 - vllm.core.scheduler - INFO - Pending queue size: (227)
2025-12-19 23:59:15,523 - vllm.entrypoints.logger - INFO - Received request cmpl-bd3f36b7215141d7a43a101b580165ff-0: prompt: "My supervisor really gets on my case when I act impulsively. It's like everything I do is wrong.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5050, 37319, 2167, 5221, 389, 847, 1142, 979, 358, 1160, 96779, 3132, 13, 1084, 594, 1075, 4297, 358, 653, 374, 4969, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,530 - vllm.entrypoints.logger - INFO - Received request cmpl-3408812e0e144d3c855b2b2ade1ac4e7-0: prompt: 'Hi, how are you feeling? I came to visit you today. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 1246, 525, 498, 8266, 30, 358, 3697, 311, 3947, 498, 3351, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,537 - vllm.entrypoints.logger - INFO - Received request cmpl-e1b7accde4b04ea8865c07853cd30ecc-0: prompt: 'I always try to consider alternative possibilities and be prepared for any situation.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 1430, 311, 2908, 10555, 23607, 323, 387, 10030, 369, 894, 6534, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,562 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bd3f36b7215141d7a43a101b580165ff-0.
2025-12-19 23:59:15,564 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3408812e0e144d3c855b2b2ade1ac4e7-0.
2025-12-19 23:59:15,565 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e1b7accde4b04ea8865c07853cd30ecc-0.
2025-12-19 23:59:15,567 - vllm.core.scheduler - INFO - Pending queue size: (230)
2025-12-19 23:59:15,618 - vllm.core.scheduler - INFO - Pending queue size: (230)
2025-12-19 23:59:15,663 - vllm.core.scheduler - INFO - Pending queue size: (230)
2025-12-19 23:59:15,705 - vllm.core.scheduler - INFO - Pending queue size: (230)
2025-12-19 23:59:15,721 - vllm.entrypoints.logger - INFO - Received request cmpl-f6cd159f48c6439f95d047a0490461a7-0: prompt: 'Being a part of the peacekeeping force is such an honor.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [33142, 264, 949, 315, 279, 8919, 32394, 5344, 374, 1741, 458, 15669, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,747 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f6cd159f48c6439f95d047a0490461a7-0.
2025-12-19 23:59:15,751 - vllm.core.scheduler - INFO - Pending queue size: (231)
2025-12-19 23:59:15,775 - vllm.entrypoints.logger - INFO - Received request cmpl-fc05fc76242848f8badfbf86a3b7ffc0-0: prompt: "I'm glad we went to the doctor today. It's always good to make sure we're in good health.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 15713, 582, 3937, 311, 279, 10668, 3351, 13, 1084, 594, 2677, 1661, 311, 1281, 2704, 582, 2299, 304, 1661, 2820, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,780 - vllm.entrypoints.logger - INFO - Received request cmpl-8dbabceb4c8b487f870dd1cd69b8138a-0: prompt: 'You know, I really regret not paying more attention in school and taking advantage of all the learning opportunities.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2167, 22231, 537, 12515, 803, 6529, 304, 2906, 323, 4633, 9423, 315, 678, 279, 6832, 10488, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,782 - vllm.entrypoints.logger - INFO - Received request cmpl-c00f0454f7934998a6e4a4ab6a0f7a0e-0: prompt: ", I'm not sure if I can help you cheat on your upcoming test.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 537, 2704, 421, 358, 646, 1492, 498, 40768, 389, 697, 14487, 1273, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,793 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fc05fc76242848f8badfbf86a3b7ffc0-0.
2025-12-19 23:59:15,794 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8dbabceb4c8b487f870dd1cd69b8138a-0.
2025-12-19 23:59:15,796 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c00f0454f7934998a6e4a4ab6a0f7a0e-0.
2025-12-19 23:59:15,799 - vllm.core.scheduler - INFO - Pending queue size: (234)
2025-12-19 23:59:15,819 - vllm.entrypoints.logger - INFO - Received request cmpl-c0e9a039fd02467f866b62003e885215-0: prompt: ' I left the shack. The sun was shining and the birds were singing. I felt free for the first time in a long time.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2115, 279, 76290, 13, 576, 7015, 572, 47925, 323, 279, 19654, 1033, 25083, 13, 358, 6476, 1910, 369, 279, 1156, 882, 304, 264, 1293, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,822 - vllm.entrypoints.logger - INFO - Received request cmpl-d769a6f717324d1f94ba54142b7f76c7-0: prompt: 'Happiness is such an important aspect of our lives.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [39, 66291, 374, 1741, 458, 2989, 12893, 315, 1039, 6305, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,830 - vllm.entrypoints.logger - INFO - Received request cmpl-d33c3c24ca6a45abacd4c2e4e48f7369-0: prompt: "I'm so bored with my job. I feel like I need to do something adventurous to add some excitement to my life.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 33286, 448, 847, 2618, 13, 358, 2666, 1075, 358, 1184, 311, 653, 2494, 67088, 311, 912, 1045, 27262, 311, 847, 2272, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,842 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c0e9a039fd02467f866b62003e885215-0.
2025-12-19 23:59:15,844 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d769a6f717324d1f94ba54142b7f76c7-0.
2025-12-19 23:59:15,845 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d33c3c24ca6a45abacd4c2e4e48f7369-0.
2025-12-19 23:59:15,848 - vllm.core.scheduler - INFO - Pending queue size: (237)
2025-12-19 23:59:15,864 - vllm.entrypoints.logger - INFO - Received request cmpl-1180c9b266574ca9a9205d836721c246-0: prompt: "I can't wait for our speech to be over. It's nerve-wracking.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 369, 1039, 8806, 311, 387, 916, 13, 1084, 594, 30915, 2630, 81, 8985, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,869 - vllm.entrypoints.logger - INFO - Received request cmpl-5c50b08582c04849b64b8e6498efc2cf-0: prompt: 'Hey, do you remember our first date in high school?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 6099, 1039, 1156, 2400, 304, 1550, 2906, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,891 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1180c9b266574ca9a9205d836721c246-0.
2025-12-19 23:59:15,892 - vllm.entrypoints.logger - INFO - Received request cmpl-578359d6daac4e0aa2deac8b4884881d-0: prompt: "I can't believe I climbed up that mountain! It was such an adventure.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 44419, 705, 429, 16301, 0, 1084, 572, 1741, 458, 17943, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,893 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5c50b08582c04849b64b8e6498efc2cf-0.
2025-12-19 23:59:15,894 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-578359d6daac4e0aa2deac8b4884881d-0.
2025-12-19 23:59:15,896 - vllm.core.scheduler - WARNING - Sequence group cmpl-267daf0da50b4520a5f1662761b8b430-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151
2025-12-19 23:59:15,898 - vllm.core.scheduler - INFO - Pending queue size: (240)
2025-12-19 23:59:15,942 - vllm.core.scheduler - INFO - Pending queue size: (240)
2025-12-19 23:59:15,955 - vllm.entrypoints.logger - INFO - Received request cmpl-bc312514d2c540df93f3b5a7633e3037-0: prompt: "I'm really nervous about flying to see my family for Christmas.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 22596, 911, 16307, 311, 1490, 847, 2997, 369, 10074, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,983 - vllm.entrypoints.logger - INFO - Received request cmpl-ebe0a1c05ade46b7a347db5a06cda56b-0: prompt: 'So, let me show you how to grip the racket first. You want to hold it with your dominant hand and place your non-dominant hand on the throat of the racket.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 1077, 752, 1473, 498, 1246, 311, 24676, 279, 81512, 1156, 13, 1446, 1366, 311, 3331, 432, 448, 697, 24456, 1424, 323, 1992, 697, 2477, 1737, 7970, 517, 1424, 389, 279, 27591, 315, 279, 81512, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:15,985 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc312514d2c540df93f3b5a7633e3037-0.
2025-12-19 23:59:15,986 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ebe0a1c05ade46b7a347db5a06cda56b-0.
2025-12-19 23:59:15,988 - vllm.core.scheduler - INFO - Pending queue size: (242)
2025-12-19 23:59:16,033 - vllm.core.scheduler - INFO - Pending queue size: (242)
2025-12-19 23:59:16,059 - vllm.entrypoints.logger - INFO - Received request cmpl-8d755f9c713b4ff8b486b270f37c9c4a-0: prompt: "I'm feeling really down lately. My best friend moved away and I can't seem to shake this feeling of sadness and loneliness.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 2167, 1495, 30345, 13, 3017, 1850, 4238, 7726, 3123, 323, 358, 646, 944, 2803, 311, 26025, 419, 8266, 315, 50878, 323, 73940, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,076 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8d755f9c713b4ff8b486b270f37c9c4a-0.
2025-12-19 23:59:16,078 - vllm.core.scheduler - INFO - Pending queue size: (243)
2025-12-19 23:59:16,082 - vllm.entrypoints.logger - INFO - Received request cmpl-54db9155ca31422c8a19ef5f2f126363-0: prompt: "I'm glad that I finally received my mother's remains. It brings me a sense of peace to know that she's finally home.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 15713, 429, 358, 5499, 3949, 847, 6554, 594, 8458, 13, 1084, 12434, 752, 264, 5530, 315, 8919, 311, 1414, 429, 1340, 594, 5499, 2114, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,107 - vllm.entrypoints.logger - INFO - Received request cmpl-06b6d2692b50454488e4cdbdcda159ff-0: prompt: "I just got a letter from Joe in the mail. He thanked me for my help and said he's doing well.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 2684, 264, 6524, 504, 12846, 304, 279, 8072, 13, 1260, 56495, 752, 369, 847, 1492, 323, 1053, 566, 594, 3730, 1632, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,122 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-54db9155ca31422c8a19ef5f2f126363-0.
2025-12-19 23:59:16,123 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-06b6d2692b50454488e4cdbdcda159ff-0.
2025-12-19 23:59:16,126 - vllm.core.scheduler - INFO - Pending queue size: (245)
2025-12-19 23:59:16,169 - vllm.core.scheduler - INFO - Pending queue size: (245)
2025-12-19 23:59:16,190 - vllm.entrypoints.logger - INFO - Received request cmpl-a61d0a1704554e63a350215015c23e2c-0: prompt: "Hi Mentor! I'm so excited to show you something I just wrote.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 0, 358, 2776, 773, 12035, 311, 1473, 498, 2494, 358, 1101, 6139, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,213 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a61d0a1704554e63a350215015c23e2c-0.
2025-12-19 23:59:16,216 - vllm.core.scheduler - INFO - Pending queue size: (246)
2025-12-19 23:59:16,260 - vllm.core.scheduler - INFO - Pending queue size: (246)
2025-12-19 23:59:16,302 - vllm.core.scheduler - INFO - Pending queue size: (246)
2025-12-19 23:59:16,327 - vllm.entrypoints.logger - INFO - Received request cmpl-79fe127bbed849bb85f8721f04e7ee81-0: prompt: "Hey, I'm glad I caught you. I found some clothes in my closet that I thought you might like.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 2776, 15713, 358, 10568, 498, 13, 358, 1730, 1045, 15097, 304, 847, 31944, 429, 358, 3381, 498, 2578, 1075, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,342 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-79fe127bbed849bb85f8721f04e7ee81-0.
2025-12-19 23:59:16,345 - vllm.core.scheduler - INFO - Pending queue size: (247)
2025-12-19 23:59:16,359 - vllm.entrypoints.logger - INFO - Received request cmpl-91232b240eff436ba3d4bfc30dbf60e5-0: prompt: 'Hey there, how are you doing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 525, 498, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,365 - vllm.entrypoints.logger - INFO - Received request cmpl-b03af070468c4741accf5d1bc9280321-0: prompt: 'Coach, I have some sad news. My father passed away.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 614, 1045, 12421, 3669, 13, 3017, 6981, 5823, 3123, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,388 - vllm.entrypoints.logger - INFO - Received request cmpl-de065a40baee45c6921740d87d9a9e44-0: prompt: 'Have you heard about the girl who invented a riddle game for her friends?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 6617, 911, 279, 3743, 879, 35492, 264, 435, 3310, 1809, 369, 1059, 4780, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,388 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-91232b240eff436ba3d4bfc30dbf60e5-0.
2025-12-19 23:59:16,390 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b03af070468c4741accf5d1bc9280321-0.
2025-12-19 23:59:16,391 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-de065a40baee45c6921740d87d9a9e44-0.
2025-12-19 23:59:16,391 - vllm.entrypoints.logger - INFO - Received request cmpl-907fd6e2438f411f8d36e90784d005b1-0: prompt: ' Wow, Doctor, you look stunning in that dress!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [45717, 11, 18635, 11, 498, 1401, 19850, 304, 429, 8511, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,393 - vllm.core.scheduler - INFO - Pending queue size: (250)
2025-12-19 23:59:16,396 - vllm.entrypoints.logger - INFO - Received request cmpl-d85340e4af2148668398b5269780a5f4-0: prompt: 'Hey, have you ever thought about writing a book?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 3381, 911, 4378, 264, 2311, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,398 - vllm.entrypoints.logger - INFO - Received request cmpl-78494e912c27443181a5dc09c619c339-0: prompt: "Hey, sorry I didn't make it to the store earlier. I got caught up with a project at work and lost track of time.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 14589, 358, 3207, 944, 1281, 432, 311, 279, 3553, 6788, 13, 358, 2684, 10568, 705, 448, 264, 2390, 518, 975, 323, 5558, 3754, 315, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,429 - vllm.entrypoints.logger - INFO - Received request cmpl-63614e2dfa724dd2940e6d38b0940812-0: prompt: 'You know, I really appreciate our friendship.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2167, 15401, 1039, 26509, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,436 - vllm.entrypoints.logger - INFO - Received request cmpl-bf4cd956899441d8a05382dad1ffafe8-0: prompt: "You know what I've been working on lately? Writing. I've always had a love for it, and I feel like now is a good time to start pursuing it more seriously.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 1128, 358, 3003, 1012, 3238, 389, 30345, 30, 23893, 13, 358, 3003, 2677, 1030, 264, 2948, 369, 432, 11, 323, 358, 2666, 1075, 1431, 374, 264, 1661, 882, 311, 1191, 33018, 432, 803, 13919, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,437 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-907fd6e2438f411f8d36e90784d005b1-0.
2025-12-19 23:59:16,438 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d85340e4af2148668398b5269780a5f4-0.
2025-12-19 23:59:16,440 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-78494e912c27443181a5dc09c619c339-0.
2025-12-19 23:59:16,441 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-63614e2dfa724dd2940e6d38b0940812-0.
2025-12-19 23:59:16,442 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bf4cd956899441d8a05382dad1ffafe8-0.
2025-12-19 23:59:16,445 - vllm.core.scheduler - INFO - Pending queue size: (255)
2025-12-19 23:59:16,460 - vllm.entrypoints.logger - INFO - Received request cmpl-1dbd20116983463da73d8f8711471119-0: prompt: "I've decided to take online courses to become more independent with my learning.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 6635, 311, 1896, 2860, 13980, 311, 3635, 803, 9489, 448, 847, 6832, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,474 - vllm.entrypoints.logger - INFO - Received request cmpl-7b2911791da2469d920000cc2a343634-0: prompt: 'That was so much fun when you gave me a piggy-back ride the other day. I felt like a kid again.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4792, 572, 773, 1753, 2464, 979, 498, 6551, 752, 264, 23694, 4577, 15461, 11877, 279, 1008, 1899, 13, 358, 6476, 1075, 264, 10369, 1549, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,488 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1dbd20116983463da73d8f8711471119-0.
2025-12-19 23:59:16,488 - vllm.entrypoints.logger - INFO - Received request cmpl-7387b803c0f64c15a025b247fa9680b6-0: prompt: "I just washed my hands with soap and water. It's important to do that to keep germs away.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 37493, 847, 6078, 448, 26785, 323, 3015, 13, 1084, 594, 2989, 311, 653, 429, 311, 2506, 17239, 1011, 3123, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,490 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7b2911791da2469d920000cc2a343634-0.
2025-12-19 23:59:16,491 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7387b803c0f64c15a025b247fa9680b6-0.
2025-12-19 23:59:16,493 - vllm.core.scheduler - INFO - Pending queue size: (258)
2025-12-19 23:59:16,494 - vllm.entrypoints.logger - INFO - Received request cmpl-3cc1b6f0142245ea9a931aad6d67c22a-0: prompt: 'Can you believe it? I can finally play without worrying about getting hurt.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 646, 5499, 1486, 2041, 39776, 911, 3709, 12898, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,500 - vllm.entrypoints.logger - INFO - Received request cmpl-c98110c62b8b4fd3bf99043e6041625d-0: prompt: "Man, that was so much fun today! I'm glad you came over.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1658, 11, 429, 572, 773, 1753, 2464, 3351, 0, 358, 2776, 15713, 498, 3697, 916, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,502 - vllm.entrypoints.logger - INFO - Received request cmpl-ff02274bd22944d2b6d232f97b4a9a89-0: prompt: "Hi there, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,525 - vllm.entrypoints.logger - INFO - Received request cmpl-5cc896a147b94d249563fbfb69ed4009-0: prompt: "I'm really excited about my new job in the automobile industry. I can't believe how much money I'm making just by selling cars.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=36, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 12035, 911, 847, 501, 2618, 304, 279, 34428, 4958, 13, 358, 646, 944, 4411, 1246, 1753, 3220, 358, 2776, 3259, 1101, 553, 11236, 9331, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,529 - vllm.entrypoints.logger - INFO - Received request cmpl-c07020fad8644edd8cf1132ff6851fae-0: prompt: 'Hey, can I talk to you about something?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 358, 3061, 311, 498, 911, 2494, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,535 - vllm.entrypoints.logger - INFO - Received request cmpl-6e949e1cb4174a5e8b8ddd7a75006421-0: prompt: 'Hi Mentor, I broke a lamp at home and I feel really bad about it. So, I went to the store and got a new one to replace it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 14422, 264, 27962, 518, 2114, 323, 358, 2666, 2167, 3873, 911, 432, 13, 2055, 11, 358, 3937, 311, 279, 3553, 323, 2684, 264, 501, 825, 311, 8290, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,536 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3cc1b6f0142245ea9a931aad6d67c22a-0.
2025-12-19 23:59:16,537 - vllm.entrypoints.logger - INFO - Received request cmpl-6e979c84cbc54d198b3d03807f0d4277-0: prompt: "I really enjoy painting. It's a great way to relax and let your creativity flow.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 4669, 18824, 13, 1084, 594, 264, 2244, 1616, 311, 11967, 323, 1077, 697, 27597, 6396, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,538 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c98110c62b8b4fd3bf99043e6041625d-0.
2025-12-19 23:59:16,539 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ff02274bd22944d2b6d232f97b4a9a89-0.
2025-12-19 23:59:16,540 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5cc896a147b94d249563fbfb69ed4009-0.
2025-12-19 23:59:16,541 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c07020fad8644edd8cf1132ff6851fae-0.
2025-12-19 23:59:16,542 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6e949e1cb4174a5e8b8ddd7a75006421-0.
2025-12-19 23:59:16,543 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6e979c84cbc54d198b3d03807f0d4277-0.
2025-12-19 23:59:16,546 - vllm.core.scheduler - INFO - Pending queue size: (265)
2025-12-19 23:59:16,561 - vllm.entrypoints.logger - INFO - Received request cmpl-51562fc6514147db9f771b45504b137a-0: prompt: 'Thank you for being such a great friend, Mentor. Your presence always makes me feel better.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13060, 498, 369, 1660, 1741, 264, 2244, 4238, 11, 91191, 13, 4615, 9362, 2677, 3643, 752, 2666, 2664, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,584 - vllm.entrypoints.logger - INFO - Received request cmpl-9690d38f6909410998bb19b68ed6684c-0: prompt: "You know, Neighbors B, I always like to eat with you. You're always so polite and I feel satisfied after our meals together.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 4182, 24101, 425, 11, 358, 2677, 1075, 311, 8180, 448, 498, 13, 1446, 2299, 2677, 773, 47787, 323, 358, 2666, 19527, 1283, 1039, 20969, 3786, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,591 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-51562fc6514147db9f771b45504b137a-0.
2025-12-19 23:59:16,614 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9690d38f6909410998bb19b68ed6684c-0.
2025-12-19 23:59:16,594 - vllm.entrypoints.logger - INFO - Received request cmpl-08ec48a5a5e94feeabd14d787e85d1b3-0: prompt: "Oh no, I feel really bad for accidentally stepping on that man's foot. He looked like he was in so much pain.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 902, 11, 358, 2666, 2167, 3873, 369, 32384, 35467, 389, 429, 883, 594, 4478, 13, 1260, 6966, 1075, 566, 572, 304, 773, 1753, 6646, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,616 - vllm.core.scheduler - INFO - Pending queue size: (267)
2025-12-19 23:59:16,618 - vllm.entrypoints.logger - INFO - Received request cmpl-ffcf37192f174ca8b34b76fc74cf76b0-0: prompt: 'Ah, that was refreshing. I just went on a walk and bought myself a drink.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 429, 572, 35918, 13, 358, 1101, 3937, 389, 264, 4227, 323, 10788, 7037, 264, 7027, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,632 - vllm.entrypoints.logger - INFO - Received request cmpl-2028ce273a6f4b77a7fd3952f624f0ce-0: prompt: "Mentor, I just realized how much I've benefited from your advice over the years. I wouldn't be where I am today if it wasn't for your guidance.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1101, 15043, 1246, 1753, 358, 3003, 55028, 504, 697, 9462, 916, 279, 1635, 13, 358, 8270, 944, 387, 1380, 358, 1079, 3351, 421, 432, 5710, 944, 369, 697, 18821, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,659 - vllm.entrypoints.logger - INFO - Received request cmpl-7d9e389aa37746958f8f080f07955f1a-0: prompt: "I've been really enjoying learning about neuroscience lately. It's fascinating how the brain works.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 2167, 21413, 6832, 911, 91948, 30345, 13, 1084, 594, 26291, 1246, 279, 8109, 4278, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,660 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-08ec48a5a5e94feeabd14d787e85d1b3-0.
2025-12-19 23:59:16,661 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ffcf37192f174ca8b34b76fc74cf76b0-0.
2025-12-19 23:59:16,663 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2028ce273a6f4b77a7fd3952f624f0ce-0.
2025-12-19 23:59:16,664 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7d9e389aa37746958f8f080f07955f1a-0.
2025-12-19 23:59:16,666 - vllm.core.scheduler - INFO - Pending queue size: (271)
2025-12-19 23:59:16,680 - vllm.entrypoints.logger - INFO - Received request cmpl-e43815db6b464bc7bcea9c94f4d15db7-0: prompt: 'Hey, can we talk on the phone soon? I have something important to discuss.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 582, 3061, 389, 279, 4540, 5135, 30, 358, 614, 2494, 2989, 311, 4263, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,689 - vllm.entrypoints.logger - INFO - Received request cmpl-d954fa1b70ff4c3fa301b6c86f4c65c4-0: prompt: 'I feel so excited today!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 773, 12035, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,710 - vllm.entrypoints.logger - INFO - Received request cmpl-f6b219cc7a7b4bc38b970abb1bd75704-0: prompt: 'Hey, B, do you want to join the softball team with me?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 11, 653, 498, 1366, 311, 5138, 279, 95268, 2083, 448, 752, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,710 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e43815db6b464bc7bcea9c94f4d15db7-0.
2025-12-19 23:59:16,712 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d954fa1b70ff4c3fa301b6c86f4c65c4-0.
2025-12-19 23:59:16,713 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f6b219cc7a7b4bc38b970abb1bd75704-0.
2025-12-19 23:59:16,716 - vllm.core.scheduler - INFO - Pending queue size: (274)
2025-12-19 23:59:16,734 - vllm.entrypoints.logger - INFO - Received request cmpl-b368aea3fe77439aa51811fbe4fe9973-0: prompt: 'Hey Mentor, I wanted to show you something. Look at how fast I can type on the computer!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 4829, 311, 1473, 498, 2494, 13, 9192, 518, 1246, 4937, 358, 646, 943, 389, 279, 6366, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,759 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b368aea3fe77439aa51811fbe4fe9973-0.
2025-12-19 23:59:16,762 - vllm.core.scheduler - INFO - Pending queue size: (275)
2025-12-19 23:59:16,769 - vllm.entrypoints.logger - INFO - Received request cmpl-c8729e35be7044bd83712b769ee504e4-0: prompt: "I still can't find my phone. I've looked through my bag multiple times now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2058, 646, 944, 1477, 847, 4540, 13, 358, 3003, 6966, 1526, 847, 8968, 5248, 3039, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,789 - vllm.entrypoints.logger - INFO - Received request cmpl-c8c2542c0e9542b580dba04f99ae2fb3-0: prompt: 'Is something bothering you, Co-workers B? You look a bit worried.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3872, 2494, 90159, 498, 11, 3539, 62284, 425, 30, 1446, 1401, 264, 2699, 17811, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,799 - vllm.entrypoints.logger - INFO - Received request cmpl-e94d68817d1544afb45abfb25ca7cdf4-0: prompt: 'You know, I really care about you, Classmates B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2167, 2453, 911, 498, 11, 3228, 16457, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,806 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c8729e35be7044bd83712b769ee504e4-0.
2025-12-19 23:59:16,807 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c8c2542c0e9542b580dba04f99ae2fb3-0.
2025-12-19 23:59:16,808 - vllm.entrypoints.logger - INFO - Received request cmpl-72280f67a2954fcfa0493306fdc3996b-0: prompt: 'Hey, guess what? I aced my AP History final!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 7942, 1128, 30, 358, 1613, 291, 847, 10106, 11099, 1590, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,808 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e94d68817d1544afb45abfb25ca7cdf4-0.
2025-12-19 23:59:16,810 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-72280f67a2954fcfa0493306fdc3996b-0.
2025-12-19 23:59:16,812 - vllm.core.scheduler - INFO - Pending queue size: (279)
2025-12-19 23:59:16,837 - vllm.entrypoints.logger - INFO - Received request cmpl-d1f587b3c29149c2988d7988e37f5346-0: prompt: "I can't believe they accused me of breaking the window. I was in my room the whole time!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 807, 13185, 752, 315, 14719, 279, 3241, 13, 358, 572, 304, 847, 3054, 279, 4361, 882, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,841 - vllm.entrypoints.logger - INFO - Received request cmpl-355bfb2c0ae743a5bbd977738f4f1edc-0: prompt: "I'm feeling really nervous about my upcoming math test.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 2167, 22596, 911, 847, 14487, 6888, 1273, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,853 - vllm.entrypoints.logger - INFO - Received request cmpl-c3923264fdf7488190b2ce1c2a44a315-0: prompt: "Here, let me wrap you up in this blanket. It's so cold outside, I don't want you to catch a cold.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8420, 11, 1077, 752, 15061, 498, 705, 304, 419, 38039, 13, 1084, 594, 773, 9255, 4889, 11, 358, 1513, 944, 1366, 498, 311, 2287, 264, 9255, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,856 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d1f587b3c29149c2988d7988e37f5346-0.
2025-12-19 23:59:16,857 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-355bfb2c0ae743a5bbd977738f4f1edc-0.
2025-12-19 23:59:16,858 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c3923264fdf7488190b2ce1c2a44a315-0.
2025-12-19 23:59:16,861 - vllm.core.scheduler - INFO - Pending queue size: (282)
2025-12-19 23:59:16,861 - vllm.entrypoints.logger - INFO - Received request cmpl-c270b9f6c3834884ae9226d411469f57-0: prompt: "I'm so excited to finally be heading home!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 311, 5499, 387, 14496, 2114, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,862 - vllm.entrypoints.logger - INFO - Received request cmpl-d2b3b7a965dd4aca98ced13faf2810b1-0: prompt: 'Hi there, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,866 - vllm.entrypoints.logger - INFO - Received request cmpl-b062421d56dd4e95bd4cee90003d9375-0: prompt: 'So, I wanted to talk to you today about the plan for the upcoming project. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 358, 4829, 311, 3061, 311, 498, 3351, 911, 279, 3119, 369, 279, 14487, 2390, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,877 - vllm.entrypoints.logger - INFO - Received request cmpl-bda1b5e78a0a4df992bcd331fd9a0a34-0: prompt: 'Ouch! Why did you not see that you are blocking my path and kick my leg?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [46, 1387, 0, 8429, 1521, 498, 537, 1490, 429, 498, 525, 22188, 847, 1815, 323, 10323, 847, 2472, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,880 - vllm.entrypoints.logger - INFO - Received request cmpl-c547ae8f98804c3b811ece508c5c0a98-0: prompt: "Doctor, I've been feeling overwhelmed lately. Everyone wants to be my friend and it's hard to keep up with all the social demands.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 3003, 1012, 8266, 42106, 30345, 13, 21455, 6801, 311, 387, 847, 4238, 323, 432, 594, 2588, 311, 2506, 705, 448, 678, 279, 3590, 18154, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,895 - vllm.entrypoints.logger - INFO - Received request cmpl-02ca00b82c2947a3a9b907a8513ae4f8-0: prompt: " Boss, can we talk about something that's been bothering me for a while now?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [31569, 11, 646, 582, 3061, 911, 2494, 429, 594, 1012, 90159, 752, 369, 264, 1393, 1431, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,904 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c270b9f6c3834884ae9226d411469f57-0.
2025-12-19 23:59:16,906 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d2b3b7a965dd4aca98ced13faf2810b1-0.
2025-12-19 23:59:16,906 - vllm.entrypoints.logger - INFO - Received request cmpl-a0ae5c2dfa984c6eac09fa92eef369b5-0: prompt: 'Mentor, I have some exciting news to share with you! I am going to have a baby!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 1045, 13245, 3669, 311, 4332, 448, 498, 0, 358, 1079, 2087, 311, 614, 264, 8770, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,907 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b062421d56dd4e95bd4cee90003d9375-0.
2025-12-19 23:59:16,908 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bda1b5e78a0a4df992bcd331fd9a0a34-0.
2025-12-19 23:59:16,909 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c547ae8f98804c3b811ece508c5c0a98-0.
2025-12-19 23:59:16,910 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-02ca00b82c2947a3a9b907a8513ae4f8-0.
2025-12-19 23:59:16,911 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a0ae5c2dfa984c6eac09fa92eef369b5-0.
2025-12-19 23:59:16,914 - vllm.core.scheduler - INFO - Pending queue size: (289)
2025-12-19 23:59:16,925 - vllm.entrypoints.logger - INFO - Received request cmpl-07c23610e568474ba9495e25dc835410-0: prompt: "Hey, it's cool to see you here in the chatroom!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 432, 594, 7010, 311, 1490, 498, 1588, 304, 279, 6236, 2966, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,958 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-07c23610e568474ba9495e25dc835410-0.
2025-12-19 23:59:16,959 - vllm.entrypoints.logger - INFO - Received request cmpl-8cf92df2f533491296b5b0fe20f38dea-0: prompt: "I just can't seem to get along with you, Neighbors B. We always end up fighting.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 2803, 311, 633, 3156, 448, 498, 11, 4182, 24101, 425, 13, 1205, 2677, 835, 705, 10805, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,961 - vllm.core.scheduler - INFO - Pending queue size: (290)
2025-12-19 23:59:16,974 - vllm.entrypoints.logger - INFO - Received request cmpl-64a966919e9744918b0ee37b55d02391-0: prompt: 'Hey, I brought you a sandwich. You looked really hungry.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 7117, 498, 264, 27874, 13, 1446, 6966, 2167, 28956, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:16,979 - vllm.entrypoints.logger - INFO - Received request cmpl-760e425923a74f19b026d0970abb437e-0: prompt: 'B, I have something very important to tell you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [33, 11, 358, 614, 2494, 1602, 2989, 311, 3291, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,004 - vllm.entrypoints.logger - INFO - Received request cmpl-5294fe7f396745099bf9b83bb2b1f404-0: prompt: "I love swimming, it's one of my favorite ways to relax.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 23380, 11, 432, 594, 825, 315, 847, 6930, 5510, 311, 11967, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,005 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8cf92df2f533491296b5b0fe20f38dea-0.
2025-12-19 23:59:17,006 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-64a966919e9744918b0ee37b55d02391-0.
2025-12-19 23:59:17,008 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-760e425923a74f19b026d0970abb437e-0.
2025-12-19 23:59:17,009 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5294fe7f396745099bf9b83bb2b1f404-0.
2025-12-19 23:59:17,012 - vllm.core.scheduler - INFO - Pending queue size: (294)
2025-12-19 23:59:17,013 - vllm.entrypoints.logger - INFO - Received request cmpl-6e98b7729dba4efdad09f095e1e4aea8-0: prompt: 'Can you imagine if the sun actually disappeared? It would be so scary.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 12793, 421, 279, 7015, 3520, 28396, 30, 1084, 1035, 387, 773, 28465, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,018 - vllm.entrypoints.logger - INFO - Received request cmpl-54d66269652b405f8537fcef8f347cea-0: prompt: "I can't believe I have to pay for the vase I broke on your porch. I mean, it was just a vase.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 614, 311, 2291, 369, 279, 92384, 358, 14422, 389, 697, 44647, 13, 358, 3076, 11, 432, 572, 1101, 264, 92384, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,024 - vllm.entrypoints.logger - INFO - Received request cmpl-de23f861bdbc49a89c35945fb93632ee-0: prompt: ' Remember that one time I took your umbrella and you got all mad?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [19881, 429, 825, 882, 358, 3867, 697, 47898, 323, 498, 2684, 678, 12796, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,047 - vllm.entrypoints.logger - INFO - Received request cmpl-9191cfbe205a4dfcaecb98180fff72c6-0: prompt: 'Working in the garden today was so refreshing and energizing.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [33978, 304, 279, 13551, 3351, 572, 773, 35918, 323, 4501, 4849, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,049 - vllm.entrypoints.logger - INFO - Received request cmpl-2e5d0ff8b78245bab6fb3189d694e3fd-0: prompt: "I can't believe you lied to me about everything, B. How could you do this to me?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 498, 46153, 311, 752, 911, 4297, 11, 425, 13, 2585, 1410, 498, 653, 419, 311, 752, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,051 - vllm.entrypoints.logger - INFO - Received request cmpl-8bc24cada81045a08ae014b1bfc46d49-0: prompt: 'Mentor, I just have to say, I am constantly impressed by your intelligence and quick wit.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1101, 614, 311, 1977, 11, 358, 1079, 14971, 24404, 553, 697, 11229, 323, 3974, 37367, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,056 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6e98b7729dba4efdad09f095e1e4aea8-0.
2025-12-19 23:59:17,057 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-54d66269652b405f8537fcef8f347cea-0.
2025-12-19 23:59:17,058 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-de23f861bdbc49a89c35945fb93632ee-0.
2025-12-19 23:59:17,059 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9191cfbe205a4dfcaecb98180fff72c6-0.
2025-12-19 23:59:17,060 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2e5d0ff8b78245bab6fb3189d694e3fd-0.
2025-12-19 23:59:17,061 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8bc24cada81045a08ae014b1bfc46d49-0.
2025-12-19 23:59:17,064 - vllm.core.scheduler - INFO - Pending queue size: (300)
2025-12-19 23:59:17,066 - vllm.entrypoints.logger - INFO - Received request cmpl-3145c01785854586a825ef2de0483b3c-0: prompt: "I love making floral arrangements. It's my way of spreading joy to others.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 3259, 45019, 27804, 13, 1084, 594, 847, 1616, 315, 30035, 15888, 311, 3800, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,079 - vllm.entrypoints.logger - INFO - Received request cmpl-c6a0fe22bdd14757962a3c1e7c50ddfb-0: prompt: 'You know, I keep getting compliments from people about my intelligence and appearance. Its really nice to be respected so much.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 2506, 3709, 71639, 504, 1251, 911, 847, 11229, 323, 11094, 13, 1084, 748, 2167, 6419, 311, 387, 30287, 773, 1753, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,085 - vllm.entrypoints.logger - INFO - Received request cmpl-40212a2c92d34992b3588becb05d8175-0: prompt: 'Hi there, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,099 - vllm.entrypoints.logger - INFO - Received request cmpl-443dea62240b442b86a020e4926fa1d5-0: prompt: "Mentor, I need to talk to you about something that's been weighing on my mind.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 46726, 389, 847, 3971, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,107 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3145c01785854586a825ef2de0483b3c-0.
2025-12-19 23:59:17,109 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c6a0fe22bdd14757962a3c1e7c50ddfb-0.
2025-12-19 23:59:17,110 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-40212a2c92d34992b3588becb05d8175-0.
2025-12-19 23:59:17,111 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-443dea62240b442b86a020e4926fa1d5-0.
2025-12-19 23:59:17,113 - vllm.core.scheduler - INFO - Pending queue size: (304)
2025-12-19 23:59:17,118 - vllm.entrypoints.logger - INFO - Received request cmpl-bbdadd48c3a44f21ba7d5dd38fabedff-0: prompt: ", I want to apologize for what I said during our argument. I didn't mean to hurt you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 1366, 311, 36879, 369, 1128, 358, 1053, 2337, 1039, 5693, 13, 358, 3207, 944, 3076, 311, 12898, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,123 - vllm.entrypoints.logger - INFO - Received request cmpl-a77801bdef4d4c17b82a8b99c82a8dfe-0: prompt: "Mentor, I've been thinking a lot about what I want to do in the future, and I think I was born to lead. I remember feeling confident from the day I could walk.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 3003, 1012, 7274, 264, 2696, 911, 1128, 358, 1366, 311, 653, 304, 279, 3853, 11, 323, 358, 1744, 358, 572, 9223, 311, 2990, 13, 358, 6099, 8266, 16506, 504, 279, 1899, 358, 1410, 4227, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,145 - vllm.entrypoints.logger - INFO - Received request cmpl-1cfbba3b18fc410590bbfd309a2d169d-0: prompt: "I threw a pink rubber ball for the cat earlier and it was so happy. It's amazing how the little things can bring so much joy.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 22192, 264, 18217, 22674, 4935, 369, 279, 8251, 6788, 323, 432, 572, 773, 6247, 13, 1084, 594, 7897, 1246, 279, 2632, 2513, 646, 4446, 773, 1753, 15888, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,157 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bbdadd48c3a44f21ba7d5dd38fabedff-0.
2025-12-19 23:59:17,158 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a77801bdef4d4c17b82a8b99c82a8dfe-0.
2025-12-19 23:59:17,160 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1cfbba3b18fc410590bbfd309a2d169d-0.
2025-12-19 23:59:17,162 - vllm.core.scheduler - INFO - Pending queue size: (307)
2025-12-19 23:59:17,167 - vllm.entrypoints.logger - INFO - Received request cmpl-1f0d841dd84640489766c56cfa89a878-0: prompt: "Hey, guess what? I sold a jar of peanut butter today and earned $2. I'm planning to save the money to buy a new bike.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 7942, 1128, 30, 358, 6088, 264, 29595, 315, 49833, 14100, 3351, 323, 15303, 400, 17, 13, 358, 2776, 9115, 311, 3581, 279, 3220, 311, 3695, 264, 501, 12963, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,173 - vllm.entrypoints.logger - INFO - Received request cmpl-7754df2af3224b77b42b98dec4ab1b19-0: prompt: "Oh man, I can't believe I made such a careless mistake at the restaurant earlier.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 883, 11, 358, 646, 944, 4411, 358, 1865, 1741, 264, 83734, 16523, 518, 279, 10729, 6788, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,178 - vllm.entrypoints.logger - INFO - Received request cmpl-6843456e403e43918117021001ce920f-0: prompt: "I'm feeling pretty good about my job lately. I think I'm going to get a raise soon.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 5020, 1661, 911, 847, 2618, 30345, 13, 358, 1744, 358, 2776, 2087, 311, 633, 264, 4828, 5135, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,181 - vllm.entrypoints.logger - INFO - Received request cmpl-be716a53819c40aea6f31a53b107f288-0: prompt: 'This is nice. Sitting by a bonfire with you. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 6419, 13, 83754, 553, 264, 7814, 10796, 448, 498, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,190 - vllm.entrypoints.logger - INFO - Received request cmpl-8ace96d28f524fab83ce8fe67848d377-0: prompt: 'Hey, Neighbor B! How are you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 97163, 425, 0, 2585, 525, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,206 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1f0d841dd84640489766c56cfa89a878-0.
2025-12-19 23:59:17,207 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7754df2af3224b77b42b98dec4ab1b19-0.
2025-12-19 23:59:17,208 - vllm.entrypoints.logger - INFO - Received request cmpl-19bdcf11210c418cb9391ce01707f4a7-0: prompt: ", I'm flying out to New York tomorrow and I'm so excited to explore the city!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 16307, 700, 311, 1532, 4261, 16577, 323, 358, 2776, 773, 12035, 311, 13186, 279, 3283, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,209 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6843456e403e43918117021001ce920f-0.
2025-12-19 23:59:17,210 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-be716a53819c40aea6f31a53b107f288-0.
2025-12-19 23:59:17,211 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8ace96d28f524fab83ce8fe67848d377-0.
2025-12-19 23:59:17,212 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-19bdcf11210c418cb9391ce01707f4a7-0.
2025-12-19 23:59:17,215 - vllm.core.scheduler - INFO - Pending queue size: (313)
2025-12-19 23:59:17,222 - vllm.entrypoints.logger - INFO - Received request cmpl-bfb233b470ab4999a59ac098ea121988-0: prompt: 'Hey, did you hear what happened today? I had to rush to your rescue!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 1128, 6932, 3351, 30, 358, 1030, 311, 12973, 311, 697, 17186, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,228 - vllm.entrypoints.logger - INFO - Received request cmpl-62e29f0df1694d9a8d9fc70b66aedb47-0: prompt: "Coach, I really need your help. I don't understand how to do this drill. It's very confusing and difficult for me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 2167, 1184, 697, 1492, 13, 358, 1513, 944, 3535, 1246, 311, 653, 419, 30546, 13, 1084, 594, 1602, 30615, 323, 5000, 369, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,261 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bfb233b470ab4999a59ac098ea121988-0.
2025-12-19 23:59:17,263 - vllm.entrypoints.logger - INFO - Received request cmpl-6dff5daaa4f7470390aa2e5ee476f232-0: prompt: 'Hey there, congrats on becoming the chairman!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 30169, 1862, 389, 10454, 279, 21201, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,266 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-62e29f0df1694d9a8d9fc70b66aedb47-0.
2025-12-19 23:59:17,267 - vllm.entrypoints.logger - INFO - Received request cmpl-b217c568916f4cd8bb06b33983623d20-0: prompt: 'Hey, have you watched the documentary on how the environment affects animals and plants?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 15384, 279, 24954, 389, 1246, 279, 4573, 21501, 9898, 323, 10779, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,267 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6dff5daaa4f7470390aa2e5ee476f232-0.
2025-12-19 23:59:17,270 - vllm.core.scheduler - INFO - Pending queue size: (316)
2025-12-19 23:59:17,274 - vllm.entrypoints.logger - INFO - Received request cmpl-c970f480b21947a587e97013bb0c7c13-0: prompt: "I feel really accomplished because I can cook well. I've learned how to make so many different dishes and I love doing it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 2167, 26237, 1576, 358, 646, 4296, 1632, 13, 358, 3003, 9498, 1246, 311, 1281, 773, 1657, 2155, 25779, 323, 358, 2948, 3730, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,276 - vllm.entrypoints.logger - INFO - Received request cmpl-d55d5f3db1194d45acc99208b5962733-0: prompt: 'I was so proud of myself yesterday. I stood up for my little sister when she was being bullied at school.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 773, 12409, 315, 7037, 13671, 13, 358, 14638, 705, 369, 847, 2632, 12923, 979, 1340, 572, 1660, 77813, 518, 2906, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,290 - vllm.entrypoints.logger - INFO - Received request cmpl-4d9a4bf97e19474b904c983ce9a4a16f-0: prompt: "I can't wait to see you again, my love.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 311, 1490, 498, 1549, 11, 847, 2948, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,291 - vllm.entrypoints.logger - INFO - Received request cmpl-8b7e8107f1b6421591a7debdb57cb64c-0: prompt: 'This is quite an experience, being on a sailboat for the first time.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 5008, 458, 3139, 11, 1660, 389, 264, 29403, 37765, 369, 279, 1156, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,303 - vllm.entrypoints.logger - INFO - Received request cmpl-8494aac61e794c58ae78e3b31100a616-0: prompt: 'Mentor, you look really cold and shaky. Do you want to come inside my house to warm up?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 498, 1401, 2167, 9255, 323, 83413, 13, 3155, 498, 1366, 311, 2525, 4766, 847, 3753, 311, 8205, 705, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,314 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b217c568916f4cd8bb06b33983623d20-0.
2025-12-19 23:59:17,315 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c970f480b21947a587e97013bb0c7c13-0.
2025-12-19 23:59:17,315 - vllm.entrypoints.logger - INFO - Received request cmpl-ad29679aa306429d9953bc50e157f804-0: prompt: 'It was so strange, I was at the bank earlier and I just had this sudden urge to escape.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 572, 773, 14888, 11, 358, 572, 518, 279, 6073, 6788, 323, 358, 1101, 1030, 419, 10968, 32047, 311, 12449, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,316 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d55d5f3db1194d45acc99208b5962733-0.
2025-12-19 23:59:17,317 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d9a4bf97e19474b904c983ce9a4a16f-0.
2025-12-19 23:59:17,318 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8b7e8107f1b6421591a7debdb57cb64c-0.
2025-12-19 23:59:17,319 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8494aac61e794c58ae78e3b31100a616-0.
2025-12-19 23:59:17,320 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ad29679aa306429d9953bc50e157f804-0.
2025-12-19 23:59:17,322 - vllm.entrypoints.logger - INFO - Received request cmpl-3b60d17662674828a1f23b146f8cffa5-0: prompt: 'Hey, have you ever considered investing in a second business?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 6509, 24965, 304, 264, 2086, 2562, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,322 - vllm.core.scheduler - INFO - Pending queue size: (323)
2025-12-19 23:59:17,332 - vllm.entrypoints.logger - INFO - Received request cmpl-f3665a2481bb468ca82bee583533bfc9-0: prompt: ' I went to the park yesterday to meet new people.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 3937, 311, 279, 6118, 13671, 311, 3367, 501, 1251, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,333 - vllm.entrypoints.logger - INFO - Received request cmpl-fa7cd4f192ef45f5b4b87eff49afab98-0: prompt: 'It was so great catching up with my former neighbor yesterday. We had so much to talk about after all these years.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 572, 773, 2244, 33068, 705, 448, 847, 4741, 9565, 13671, 13, 1205, 1030, 773, 1753, 311, 3061, 911, 1283, 678, 1493, 1635, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,358 - vllm.entrypoints.logger - INFO - Received request cmpl-8f4cc39d3b4043e29658130fec51085d-0: prompt: 'Hey, did you hear about what I did to Classmates B?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 1128, 358, 1521, 311, 3228, 16457, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,362 - vllm.entrypoints.logger - INFO - Received request cmpl-6f849e5614484d738709d041b1d0e27d-0: prompt: " I can't stop thinking about my grandma. It's been a few weeks since she passed away, but I still feel sad and alone.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 646, 944, 2936, 7274, 911, 847, 82677, 13, 1084, 594, 1012, 264, 2421, 5555, 2474, 1340, 5823, 3123, 11, 714, 358, 2058, 2666, 12421, 323, 7484, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,366 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3b60d17662674828a1f23b146f8cffa5-0.
2025-12-19 23:59:17,367 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f3665a2481bb468ca82bee583533bfc9-0.
2025-12-19 23:59:17,368 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fa7cd4f192ef45f5b4b87eff49afab98-0.
2025-12-19 23:59:17,369 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8f4cc39d3b4043e29658130fec51085d-0.
2025-12-19 23:59:17,371 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f849e5614484d738709d041b1d0e27d-0.
2025-12-19 23:59:17,373 - vllm.core.scheduler - INFO - Pending queue size: (328)
2025-12-19 23:59:17,382 - vllm.entrypoints.logger - INFO - Received request cmpl-1d1d99604b274dc2b51f97dd3e87e8c6-0: prompt: "I've been thinking a lot lately about how I express myself. I feel like sometimes I struggle to find the right words.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 7274, 264, 2696, 30345, 911, 1246, 358, 3158, 7037, 13, 358, 2666, 1075, 7025, 358, 14651, 311, 1477, 279, 1290, 4244, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,389 - vllm.entrypoints.logger - INFO - Received request cmpl-11a0c5193b08467096de2a9647f4ea6c-0: prompt: 'Coach, I made a mistake the other day. I saw a creature and thought it was a dog, but it was actually a coyote. I feel really bad about it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=33, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 1865, 264, 16523, 279, 1008, 1899, 13, 358, 5485, 264, 17218, 323, 3381, 432, 572, 264, 5562, 11, 714, 432, 572, 3520, 264, 74609, 1272, 13, 358, 2666, 2167, 3873, 911, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,398 - vllm.entrypoints.logger - INFO - Received request cmpl-6beea8b8a3294b9aaf3e7e845aafcdf1-0: prompt: 'Hey, I found your umbrella outside. Do you need it for your rain dance?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1730, 697, 47898, 4889, 13, 3155, 498, 1184, 432, 369, 697, 11174, 15254, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,414 - vllm.entrypoints.logger - INFO - Received request cmpl-5aa8e65b3e2a4eddb76af99bd6608e97-0: prompt: "I have to say, Co-workers B, I'm really uncomfortable with your philosophy on lying and cheating.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 311, 1977, 11, 3539, 62284, 425, 11, 358, 2776, 2167, 28113, 448, 697, 19128, 389, 20446, 323, 41723, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,417 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1d1d99604b274dc2b51f97dd3e87e8c6-0.
2025-12-19 23:59:17,418 - vllm.entrypoints.logger - INFO - Received request cmpl-817961bc2d4f4a339d778fdf03b89b75-0: prompt: ', I think we should definitely start working on our project today. We need to stay on top of things.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 1744, 582, 1265, 8491, 1191, 3238, 389, 1039, 2390, 3351, 13, 1205, 1184, 311, 4717, 389, 1909, 315, 2513, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,418 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-11a0c5193b08467096de2a9647f4ea6c-0.
2025-12-19 23:59:17,419 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6beea8b8a3294b9aaf3e7e845aafcdf1-0.
2025-12-19 23:59:17,421 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5aa8e65b3e2a4eddb76af99bd6608e97-0.
2025-12-19 23:59:17,421 - vllm.entrypoints.logger - INFO - Received request cmpl-0970d8fec8a34c90b26c92a9eff676ee-0: prompt: 'I just wanted to thank you for always being such a great listener, B. It means a lot to me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 4829, 311, 9702, 498, 369, 2677, 1660, 1741, 264, 2244, 11446, 11, 425, 13, 1084, 3363, 264, 2696, 311, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,422 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-817961bc2d4f4a339d778fdf03b89b75-0.
2025-12-19 23:59:17,423 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0970d8fec8a34c90b26c92a9eff676ee-0.
2025-12-19 23:59:17,425 - vllm.core.scheduler - INFO - Pending queue size: (334)
2025-12-19 23:59:17,443 - vllm.entrypoints.logger - INFO - Received request cmpl-69499d77525d478eb5387689bfea98e2-0: prompt: "Hey, how's the project going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 279, 2390, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,460 - vllm.entrypoints.logger - INFO - Received request cmpl-bedff8c4078e44cd990133aa268012d8-0: prompt: 'Hey, I visited Joe yesterday. It was so good to see him after so long.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 11994, 12846, 13671, 13, 1084, 572, 773, 1661, 311, 1490, 1435, 1283, 773, 1293, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,469 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-69499d77525d478eb5387689bfea98e2-0.
2025-12-19 23:59:17,471 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bedff8c4078e44cd990133aa268012d8-0.
2025-12-19 23:59:17,472 - vllm.entrypoints.logger - INFO - Received request cmpl-f648e143bd654cd8a067bf333169417e-0: prompt: 'You know, I have always been able to feel what others are going through. I can sense their emotions and their struggles.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 614, 2677, 1012, 2952, 311, 2666, 1128, 3800, 525, 2087, 1526, 13, 358, 646, 5530, 862, 21261, 323, 862, 27870, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,473 - vllm.core.scheduler - INFO - Pending queue size: (336)
2025-12-19 23:59:17,516 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f648e143bd654cd8a067bf333169417e-0.
2025-12-19 23:59:17,519 - vllm.core.scheduler - INFO - Pending queue size: (337)
2025-12-19 23:59:17,525 - vllm.entrypoints.logger - INFO - Received request cmpl-0987c09099784422aa89d71f18d59dda-0: prompt: "Thanks for offering to help me out, B. I've thought about it and I've decided to accept your offer.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12658, 369, 10004, 311, 1492, 752, 700, 11, 425, 13, 358, 3003, 3381, 911, 432, 323, 358, 3003, 6635, 311, 4193, 697, 3010, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,550 - vllm.entrypoints.logger - INFO - Received request cmpl-42a7da5d7a62402f896fbf1a968335c5-0: prompt: ', how are your children doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 1246, 525, 697, 2841, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,563 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0987c09099784422aa89d71f18d59dda-0.
2025-12-19 23:59:17,565 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-42a7da5d7a62402f896fbf1a968335c5-0.
2025-12-19 23:59:17,568 - vllm.core.scheduler - INFO - Pending queue size: (339)
2025-12-19 23:59:17,575 - vllm.entrypoints.logger - INFO - Received request cmpl-9b32893136844024822acc9ed1d9226c-0: prompt: "I'm just so angry about the state of the world right now. It feels like things are getting worse and worse.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 1101, 773, 18514, 911, 279, 1584, 315, 279, 1879, 1290, 1431, 13, 1084, 11074, 1075, 2513, 525, 3709, 10960, 323, 10960, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,613 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9b32893136844024822acc9ed1d9226c-0.
2025-12-19 23:59:17,618 - vllm.entrypoints.logger - INFO - Received request cmpl-425630eace8f4c24b8e802e568d4e44f-0: prompt: '*scrubs teeth with toothbrush* Ah, nothing like a fresh clean mouth.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9, 25065, 15738, 17832, 448, 25507, 36061, 9, 16366, 11, 4302, 1075, 264, 7722, 4240, 10780, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,623 - vllm.core.scheduler - INFO - Pending queue size: (340)
2025-12-19 23:59:17,634 - vllm.entrypoints.logger - INFO - Received request cmpl-d1f259e04ac34d12bcb6a39adde11faa-0: prompt: 'Hey, do you know if Sarah is home right now?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 1414, 421, 20445, 374, 2114, 1290, 1431, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,644 - vllm.entrypoints.logger - INFO - Received request cmpl-2cea9af01e64431f9e42ccfb22a83b19-0: prompt: "Hey, Neighbors B! I just wanted to let you know that I'm available to help you out with anything if you need it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 0, 358, 1101, 4829, 311, 1077, 498, 1414, 429, 358, 2776, 2500, 311, 1492, 498, 700, 448, 4113, 421, 498, 1184, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,658 - vllm.entrypoints.logger - INFO - Received request cmpl-64dd187bae384a53b37f6c09600d9b1b-0: prompt: "Hey, I'm sorry but I think I need to cancel our plans for today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 2776, 14589, 714, 358, 1744, 358, 1184, 311, 9121, 1039, 6649, 369, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,669 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-425630eace8f4c24b8e802e568d4e44f-0.
2025-12-19 23:59:17,670 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d1f259e04ac34d12bcb6a39adde11faa-0.
2025-12-19 23:59:17,672 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2cea9af01e64431f9e42ccfb22a83b19-0.
2025-12-19 23:59:17,673 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-64dd187bae384a53b37f6c09600d9b1b-0.
2025-12-19 23:59:17,675 - vllm.core.scheduler - INFO - Pending queue size: (344)
2025-12-19 23:59:17,720 - vllm.core.scheduler - INFO - Pending queue size: (344)
2025-12-19 23:59:17,738 - vllm.entrypoints.logger - INFO - Received request cmpl-7279482990314634a7d175e6f7cee9ff-0: prompt: 'Hey, did you hear about that new car I bought?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 429, 501, 1803, 358, 10788, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,747 - vllm.entrypoints.logger - INFO - Received request cmpl-33be025d7d1f44449dee0c61d681623d-0: prompt: 'Hi Mentor, how are you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 1246, 525, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,763 - vllm.entrypoints.logger - INFO - Received request cmpl-e0f1bfad9fd64a7598d0b694cd3ce260-0: prompt: 'Hi there, can I talk to you for a minute?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 646, 358, 3061, 311, 498, 369, 264, 9383, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,764 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7279482990314634a7d175e6f7cee9ff-0.
2025-12-19 23:59:17,765 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-33be025d7d1f44449dee0c61d681623d-0.
2025-12-19 23:59:17,766 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e0f1bfad9fd64a7598d0b694cd3ce260-0.
2025-12-19 23:59:17,768 - vllm.core.scheduler - INFO - Pending queue size: (347)
2025-12-19 23:59:17,780 - vllm.entrypoints.logger - INFO - Received request cmpl-dff255df335c4042ad945566a9fd0e99-0: prompt: 'Hi! I went to the library today and got a book on how to use a computer.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 0, 358, 3937, 311, 279, 6733, 3351, 323, 2684, 264, 2311, 389, 1246, 311, 990, 264, 6366, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,781 - vllm.entrypoints.logger - INFO - Received request cmpl-5346a7fa2dcd42518a38692def8adbc1-0: prompt: "I can't believe how nervous I am for this presentation.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1246, 22596, 358, 1079, 369, 419, 15496, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,797 - vllm.entrypoints.logger - INFO - Received request cmpl-6f07ce54d572496d8f49491deb8b5030-0: prompt: "Today, I met so many interesting people at the park. It's amazing how many different backgrounds and experiences people have.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 2270, 773, 1657, 7040, 1251, 518, 279, 6118, 13, 1084, 594, 7897, 1246, 1657, 2155, 35476, 323, 11449, 1251, 614, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,798 - vllm.entrypoints.logger - INFO - Received request cmpl-e2fcad807b484164ba7dc4d45e602386-0: prompt: 'My family always knows how to cheer me up.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5050, 2997, 2677, 8788, 1246, 311, 25032, 752, 705, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,799 - vllm.entrypoints.logger - INFO - Received request cmpl-c77957cbfb67461293b653353d273131-0: prompt: "I'm so tired of swimming with all these other fish. I just want to be able to swim in peace and quiet for once.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 19227, 315, 23380, 448, 678, 1493, 1008, 7640, 13, 358, 1101, 1366, 311, 387, 2952, 311, 16191, 304, 8919, 323, 11340, 369, 3055, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,802 - vllm.entrypoints.logger - INFO - Received request cmpl-6442892fea5f45818bcf79538b20f7ba-0: prompt: "You know, I've been thinking a lot lately about how I approach problem-solving. I try to be really thoughtful and consider all the options before making a decision.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 3003, 1012, 7274, 264, 2696, 30345, 911, 1246, 358, 5486, 3491, 98146, 13, 358, 1430, 311, 387, 2167, 42666, 323, 2908, 678, 279, 2606, 1573, 3259, 264, 5480, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,812 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dff255df335c4042ad945566a9fd0e99-0.
2025-12-19 23:59:17,813 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5346a7fa2dcd42518a38692def8adbc1-0.
2025-12-19 23:59:17,814 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f07ce54d572496d8f49491deb8b5030-0.
2025-12-19 23:59:17,815 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e2fcad807b484164ba7dc4d45e602386-0.
2025-12-19 23:59:17,817 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c77957cbfb67461293b653353d273131-0.
2025-12-19 23:59:17,818 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6442892fea5f45818bcf79538b20f7ba-0.
2025-12-19 23:59:17,818 - vllm.entrypoints.logger - INFO - Received request cmpl-f9aa3f3caeba40fcb0b830fb5375f1c3-0: prompt: "I hate waiting. It's nerve-wracking.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 12213, 8580, 13, 1084, 594, 30915, 2630, 81, 8985, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,820 - vllm.core.scheduler - INFO - Pending queue size: (353)
2025-12-19 23:59:17,822 - vllm.entrypoints.logger - INFO - Received request cmpl-4474f9afa44246f4a3d9ad5961123aeb-0: prompt: 'Hey, Classmates B, do you remember when you taught me how to carve that wooden figurine?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 3228, 16457, 425, 11, 653, 498, 6099, 979, 498, 15599, 752, 1246, 311, 79637, 429, 22360, 55655, 482, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,831 - vllm.entrypoints.logger - INFO - Received request cmpl-f38a0a5b2af1468b9a92e3b9d56c21b8-0: prompt: 'Hi, I would like to schedule an appointment to see the doctor.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 358, 1035, 1075, 311, 9700, 458, 17635, 311, 1490, 279, 10668, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,845 - vllm.entrypoints.logger - INFO - Received request cmpl-1bc98497229249f087fb86b4100b51a1-0: prompt: "I don't know what to do. I woke up this morning and realized I don't even know the guy's name.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 1128, 311, 653, 13, 358, 38726, 705, 419, 6556, 323, 15043, 358, 1513, 944, 1496, 1414, 279, 7412, 594, 829, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,849 - vllm.entrypoints.logger - INFO - Received request cmpl-84c4b7571d36453a94b2864dedf9c063-0: prompt: 'Hey, have you noticed anything different about my house?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 13686, 4113, 2155, 911, 847, 3753, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,864 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f9aa3f3caeba40fcb0b830fb5375f1c3-0.
2025-12-19 23:59:17,865 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4474f9afa44246f4a3d9ad5961123aeb-0.
2025-12-19 23:59:17,869 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f38a0a5b2af1468b9a92e3b9d56c21b8-0.
2025-12-19 23:59:17,870 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1bc98497229249f087fb86b4100b51a1-0.
2025-12-19 23:59:17,871 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-84c4b7571d36453a94b2864dedf9c063-0.
2025-12-19 23:59:17,874 - vllm.core.scheduler - INFO - Pending queue size: (358)
2025-12-19 23:59:17,904 - vllm.entrypoints.logger - INFO - Received request cmpl-a4e1c50c89194acc8d4d901c70f9fa9a-0: prompt: 'Hi, Neighbors B. I need to talk to you about something serious.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 4182, 24101, 425, 13, 358, 1184, 311, 3061, 311, 498, 911, 2494, 6001, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,917 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a4e1c50c89194acc8d4d901c70f9fa9a-0.
2025-12-19 23:59:17,920 - vllm.core.scheduler - INFO - Pending queue size: (359)
2025-12-19 23:59:17,931 - vllm.entrypoints.logger - INFO - Received request cmpl-bdba74e2a6d54556b801457f1247e536-0: prompt: "I've been feeling suffocated lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 8266, 8489, 509, 657, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,950 - vllm.entrypoints.logger - INFO - Received request cmpl-983713f203f94d469e6fc811261c9752-0: prompt: "Hey, I have to say something. I'm not really feeling your new haircut, Neighbors B.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 614, 311, 1977, 2494, 13, 358, 2776, 537, 2167, 8266, 697, 501, 85724, 11, 4182, 24101, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,959 - vllm.entrypoints.logger - INFO - Received request cmpl-b413e02437f74884b26f25f023577e89-0: prompt: "I'm so excited to be going on a drive with you, Mentor.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 311, 387, 2087, 389, 264, 6541, 448, 498, 11, 91191, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,961 - vllm.entrypoints.logger - INFO - Received request cmpl-14271f2bca6d4934807cb42158fb2ad1-0: prompt: 'Hey, do you remember that secret spot I was telling you about?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 6099, 429, 6234, 7702, 358, 572, 11629, 498, 911, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,963 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bdba74e2a6d54556b801457f1247e536-0.
2025-12-19 23:59:17,964 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-983713f203f94d469e6fc811261c9752-0.
2025-12-19 23:59:17,965 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b413e02437f74884b26f25f023577e89-0.
2025-12-19 23:59:17,966 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-14271f2bca6d4934807cb42158fb2ad1-0.
2025-12-19 23:59:17,969 - vllm.core.scheduler - INFO - Pending queue size: (363)
2025-12-19 23:59:17,970 - vllm.entrypoints.logger - INFO - Received request cmpl-72a350324c0041ccae6c32d1796c2f5e-0: prompt: 'Hey, I wanted to tell you about something exciting that happened at a party I went to last week.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 3291, 498, 911, 2494, 13245, 429, 6932, 518, 264, 4614, 358, 3937, 311, 1537, 2003, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,986 - vllm.entrypoints.logger - INFO - Received request cmpl-adc436b5ec5b49c2b998344177f830e0-0: prompt: "I'm really sorry, I didn't mean to step on your foot like that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 14589, 11, 358, 3207, 944, 3076, 311, 3019, 389, 697, 4478, 1075, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:17,989 - vllm.entrypoints.logger - INFO - Received request cmpl-9bb8c980e18a4b61822003f375a8f152-0: prompt: "I have a big presentation today. I want to make sure I impress my boss and show that I'm committed to the success of the company.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 264, 2409, 15496, 3351, 13, 358, 1366, 311, 1281, 2704, 358, 9897, 847, 13392, 323, 1473, 429, 358, 2776, 11163, 311, 279, 2393, 315, 279, 2813, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,014 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-72a350324c0041ccae6c32d1796c2f5e-0.
2025-12-19 23:59:18,015 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-adc436b5ec5b49c2b998344177f830e0-0.
2025-12-19 23:59:18,015 - vllm.entrypoints.logger - INFO - Received request cmpl-5d4c5b9567474bf7b6814fff9a1d0b38-0: prompt: 'I had a busy morning today. Went to the grocery store, the post office, and the bank.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 264, 13028, 6556, 3351, 13, 53759, 311, 279, 29587, 3553, 11, 279, 1736, 5163, 11, 323, 279, 6073, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,016 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9bb8c980e18a4b61822003f375a8f152-0.
2025-12-19 23:59:18,017 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5d4c5b9567474bf7b6814fff9a1d0b38-0.
2025-12-19 23:59:18,017 - vllm.entrypoints.logger - INFO - Received request cmpl-a669af37250c4ebe91b30d253a04d163-0: prompt: ", I'm so excited to get started on this new project at work! I asked my boss for more information and he gave me a detailed explanation. Now I feel confident in my abilities to help with the development.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 773, 12035, 311, 633, 3855, 389, 419, 501, 2390, 518, 975, 0, 358, 4588, 847, 13392, 369, 803, 1995, 323, 566, 6551, 752, 264, 11682, 16148, 13, 4695, 358, 2666, 16506, 304, 847, 17541, 311, 1492, 448, 279, 4401, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,019 - vllm.core.scheduler - INFO - Pending queue size: (367)
2025-12-19 23:59:18,028 - vllm.entrypoints.logger - INFO - Received request cmpl-c22df347497c4aa2b6547ab8affa52aa-0: prompt: "You seem really tense, honey. How's work been?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 2803, 2167, 42687, 11, 25744, 13, 2585, 594, 975, 1012, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,030 - vllm.entrypoints.logger - INFO - Received request cmpl-fcf61fd974e64a0c87b018f318609d6f-0: prompt: "Oh no, it's almost time for me to leave. I don't want to be late.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 902, 11, 432, 594, 4558, 882, 369, 752, 311, 5274, 13, 358, 1513, 944, 1366, 311, 387, 3309, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,038 - vllm.entrypoints.logger - INFO - Received request cmpl-55d79a7c2c524fb686197316958f5568-0: prompt: ', I have something to share with you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 614, 2494, 311, 4332, 448, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,043 - vllm.entrypoints.logger - INFO - Received request cmpl-20d96179e72149dd9f787e1de5e8cd0a-0: prompt: 'Hi Doctor, how are you doing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 18635, 11, 1246, 525, 498, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,048 - vllm.entrypoints.logger - INFO - Received request cmpl-759b6030f8c141af8b3bb5539226f247-0: prompt: 'Look what I made for you and your cousins today! Beautiful dresses!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10380, 1128, 358, 1865, 369, 498, 323, 697, 59907, 3351, 0, 19490, 36762, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,051 - vllm.entrypoints.logger - INFO - Received request cmpl-76c311eba2674a38bdef6f6f173f3833-0: prompt: "Hey, I wanted to share some good news with you. I've been feeling more confident lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 4332, 1045, 1661, 3669, 448, 498, 13, 358, 3003, 1012, 8266, 803, 16506, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,052 - vllm.entrypoints.logger - INFO - Received request cmpl-d76b19d10ab64de5832738af61f63878-0: prompt: "You know, I've been thinking a lot lately about how important it is to stand up for what you believe in.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 3003, 1012, 7274, 264, 2696, 30345, 911, 1246, 2989, 432, 374, 311, 2498, 705, 369, 1128, 498, 4411, 304, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,053 - vllm.entrypoints.logger - INFO - Received request cmpl-a7608d96c7964dfe8f6a7776fabd407f-0: prompt: 'Look, I finally filled up this plastic bag with all the money I saved up! ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10380, 11, 358, 5499, 10199, 705, 419, 12188, 8968, 448, 678, 279, 3220, 358, 6781, 705, 0, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,063 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a669af37250c4ebe91b30d253a04d163-0.
2025-12-19 23:59:18,064 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c22df347497c4aa2b6547ab8affa52aa-0.
2025-12-19 23:59:18,065 - vllm.entrypoints.logger - INFO - Received request cmpl-49da257874a6473bab247e09bf4d7705-0: prompt: 'That was a great kiss.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4792, 572, 264, 2244, 21057, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,065 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fcf61fd974e64a0c87b018f318609d6f-0.
2025-12-19 23:59:18,066 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-55d79a7c2c524fb686197316958f5568-0.
2025-12-19 23:59:18,067 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-20d96179e72149dd9f787e1de5e8cd0a-0.
2025-12-19 23:59:18,068 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-759b6030f8c141af8b3bb5539226f247-0.
2025-12-19 23:59:18,069 - vllm.entrypoints.logger - INFO - Received request cmpl-d975b6dedec541948004a5221c574666-0: prompt: 'I called my mom, my dad, and my best friend yesterday. It was great to catch up with them.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2598, 847, 3368, 11, 847, 17760, 11, 323, 847, 1850, 4238, 13671, 13, 1084, 572, 2244, 311, 2287, 705, 448, 1105, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,069 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-76c311eba2674a38bdef6f6f173f3833-0.
2025-12-19 23:59:18,070 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d76b19d10ab64de5832738af61f63878-0.
2025-12-19 23:59:18,071 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a7608d96c7964dfe8f6a7776fabd407f-0.
2025-12-19 23:59:18,072 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-49da257874a6473bab247e09bf4d7705-0.
2025-12-19 23:59:18,073 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d975b6dedec541948004a5221c574666-0.
2025-12-19 23:59:18,075 - vllm.core.scheduler - INFO - Pending queue size: (378)
2025-12-19 23:59:18,082 - vllm.entrypoints.logger - INFO - Received request cmpl-d27a0b7b97bf4a3f8de0cf5a954fdcf7-0: prompt: "Doctor, I need to talk to you about something that's been bothering me. I recently discovered that someone I thought was a friend was actually an imposter. I feel so foolish for not realizing it sooner. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 90159, 752, 13, 358, 5926, 11105, 429, 4325, 358, 3381, 572, 264, 4238, 572, 3520, 458, 732, 45401, 13, 358, 2666, 773, 45237, 369, 537, 43014, 432, 30273, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,088 - vllm.entrypoints.logger - INFO - Received request cmpl-d2a99f9420a54c40aa8e4db654538d14-0: prompt: "Hey, do you have a minute to talk? I'm feeling a bit uncomfortable right now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 614, 264, 9383, 311, 3061, 30, 358, 2776, 8266, 264, 2699, 28113, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,102 - vllm.entrypoints.logger - INFO - Received request cmpl-915f58e2ea094d9da638f89d22017d7f-0: prompt: 'Hey, Neighbor B. I noticed that the fence between our properties is looking a bit worn out. Would you like some help fixing it up?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 97163, 425, 13, 358, 13686, 429, 279, 24650, 1948, 1039, 5888, 374, 3330, 264, 2699, 23704, 700, 13, 18885, 498, 1075, 1045, 1492, 35251, 432, 705, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,119 - vllm.entrypoints.logger - INFO - Received request cmpl-d152f04b78294d9da01d870d70a96f08-0: prompt: 'Hey, thanks again for inviting me to dinner with your family the other night. I had a really great time.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 9339, 1549, 369, 41192, 752, 311, 13856, 448, 697, 2997, 279, 1008, 3729, 13, 358, 1030, 264, 2167, 2244, 882, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,119 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d27a0b7b97bf4a3f8de0cf5a954fdcf7-0.
2025-12-19 23:59:18,121 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d2a99f9420a54c40aa8e4db654538d14-0.
2025-12-19 23:59:18,122 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-915f58e2ea094d9da638f89d22017d7f-0.
2025-12-19 23:59:18,123 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d152f04b78294d9da01d870d70a96f08-0.
2025-12-19 23:59:18,126 - vllm.core.scheduler - INFO - Pending queue size: (382)
2025-12-19 23:59:18,127 - vllm.entrypoints.logger - INFO - Received request cmpl-35caffe9b89e4db18a9e816dffcbfcd1-0: prompt: ", I just wanted to check in and see how you're doing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 1101, 4829, 311, 1779, 304, 323, 1490, 1246, 498, 2299, 3730, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,138 - vllm.entrypoints.logger - INFO - Received request cmpl-b1386bf4dfab4ccc966cbe701f6c5615-0: prompt: 'Hey, do you remember the time we went on that trip together during our senior year at Boston College? That was so much fun.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 6099, 279, 882, 582, 3937, 389, 429, 8411, 3786, 2337, 1039, 9990, 1042, 518, 10196, 9126, 30, 2938, 572, 773, 1753, 2464, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,156 - vllm.entrypoints.logger - INFO - Received request cmpl-589e86f4874a4a17a947346cf6ab958f-0: prompt: 'Did you know that I always wanted to be a doctor when I was younger?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 1414, 429, 358, 2677, 4829, 311, 387, 264, 10668, 979, 358, 572, 14650, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,169 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-35caffe9b89e4db18a9e816dffcbfcd1-0.
2025-12-19 23:59:18,171 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b1386bf4dfab4ccc966cbe701f6c5615-0.
2025-12-19 23:59:18,172 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-589e86f4874a4a17a947346cf6ab958f-0.
2025-12-19 23:59:18,174 - vllm.core.scheduler - INFO - Pending queue size: (385)
2025-12-19 23:59:18,177 - vllm.entrypoints.logger - INFO - Received request cmpl-95dbf23f265c4b8a953b15aed9aba24a-0: prompt: "I can't stand that guy. He always thinks he's so much smarter than everyone else, but he's really just full of himself.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 2498, 429, 7412, 13, 1260, 2677, 15482, 566, 594, 773, 1753, 46478, 1091, 5019, 770, 11, 714, 566, 594, 2167, 1101, 2480, 315, 5561, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,209 - vllm.entrypoints.logger - INFO - Received request cmpl-b37a14bd21724040990a73ec8b72e5a7-0: prompt: 'I believe that bringing peace offerings is essential for creating a peaceful atmosphere.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 4411, 429, 12678, 8919, 32835, 374, 7565, 369, 6825, 264, 25650, 16566, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,218 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-95dbf23f265c4b8a953b15aed9aba24a-0.
2025-12-19 23:59:18,219 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b37a14bd21724040990a73ec8b72e5a7-0.
2025-12-19 23:59:18,222 - vllm.core.scheduler - INFO - Pending queue size: (387)
2025-12-19 23:59:18,225 - vllm.entrypoints.logger - INFO - Received request cmpl-f9b01a92ca4940dfa9002e25a45c8a9a-0: prompt: 'Hey, check out this photo I just sent you! Can you believe it? This is my new house!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1779, 700, 419, 6548, 358, 1101, 3208, 498, 0, 2980, 498, 4411, 432, 30, 1096, 374, 847, 501, 3753, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,229 - vllm.entrypoints.logger - INFO - Received request cmpl-0848b52c424b4e61b00307f3c9e67a49-0: prompt: 'Hey neighbor, you seem a little down. Is everything okay?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 498, 2803, 264, 2632, 1495, 13, 2160, 4297, 16910, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,232 - vllm.entrypoints.logger - INFO - Received request cmpl-5ada4ddca27642939c73c30690435845-0: prompt: "Can you believe it? I'm actually running for office now!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 2776, 3520, 4303, 369, 5163, 1431, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,272 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f9b01a92ca4940dfa9002e25a45c8a9a-0.
2025-12-19 23:59:18,273 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0848b52c424b4e61b00307f3c9e67a49-0.
2025-12-19 23:59:18,274 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5ada4ddca27642939c73c30690435845-0.
2025-12-19 23:59:18,277 - vllm.core.scheduler - INFO - Pending queue size: (390)
2025-12-19 23:59:18,291 - vllm.entrypoints.logger - INFO - Received request cmpl-a653d077f9e34bd5bd318764c2850a4b-0: prompt: "I can't believe how lucky she is to have found a place to stay. It's so important to have a roof over your head.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1246, 17605, 1340, 374, 311, 614, 1730, 264, 1992, 311, 4717, 13, 1084, 594, 773, 2989, 311, 614, 264, 15134, 916, 697, 1968, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,294 - vllm.entrypoints.logger - INFO - Received request cmpl-be411fc1c4cb437f9fda4f67861117df-0: prompt: "Hey, B! How's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 0, 2585, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,298 - vllm.entrypoints.logger - INFO - Received request cmpl-e7a9ca854de04c98864396942e60d177-0: prompt: 'Hey, let me help you with those groceries.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1077, 752, 1492, 498, 448, 1846, 65408, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,321 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a653d077f9e34bd5bd318764c2850a4b-0.
2025-12-19 23:59:18,322 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-be411fc1c4cb437f9fda4f67861117df-0.
2025-12-19 23:59:18,323 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e7a9ca854de04c98864396942e60d177-0.
2025-12-19 23:59:18,326 - vllm.core.scheduler - INFO - Pending queue size: (393)
2025-12-19 23:59:18,338 - vllm.entrypoints.logger - INFO - Received request cmpl-1d2c232e2d1443d4a1ba16aa0433961a-0: prompt: 'Hey, have you noticed that the coffee in the break room has been disappearing lately?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 13686, 429, 279, 10799, 304, 279, 1438, 3054, 702, 1012, 66403, 30345, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,353 - vllm.entrypoints.logger - INFO - Received request cmpl-eb75f40d88d546729d3bf0f90c6e76d7-0: prompt: 'Ow, the lemon juice stings my cut. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [46, 86, 11, 279, 29464, 22815, 357, 819, 847, 3931, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,370 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1d2c232e2d1443d4a1ba16aa0433961a-0.
2025-12-19 23:59:18,371 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eb75f40d88d546729d3bf0f90c6e76d7-0.
2025-12-19 23:59:18,373 - vllm.core.scheduler - WARNING - Sequence group cmpl-b0f66464590d4a96a618f203e60961a1-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201
2025-12-19 23:59:18,374 - vllm.core.scheduler - INFO - Pending queue size: (395)
2025-12-19 23:59:18,382 - vllm.entrypoints.logger - INFO - Received request cmpl-524f09ba501947d5af6ca906d23ea617-0: prompt: 'Hey, have you heard about this amazing new product that I am promoting?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 911, 419, 7897, 501, 1985, 429, 358, 1079, 22136, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,415 - vllm.entrypoints.logger - INFO - Received request cmpl-e76a1277d9cf4e0381e2e8252d1dd964-0: prompt: "Coach, I've been thinking about ways to conserve water lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 3003, 1012, 7274, 911, 5510, 311, 77448, 3015, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,417 - vllm.engine.metrics - INFO - Avg prompt throughput: 378.9 tokens/s, Avg generation throughput: 21.2 tokens/s, Running: 512 reqs, Swapped: 201 reqs, Pending: 193 reqs, GPU KV cache usage: 5.4%, CPU KV cache usage: 2.7%.
2025-12-19 23:59:18,419 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-524f09ba501947d5af6ca906d23ea617-0.
2025-12-19 23:59:18,420 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e76a1277d9cf4e0381e2e8252d1dd964-0.
2025-12-19 23:59:18,422 - vllm.core.scheduler - INFO - Pending queue size: (397)
2025-12-19 23:59:18,467 - vllm.core.scheduler - INFO - Pending queue size: (397)
2025-12-19 23:59:18,467 - vllm.entrypoints.logger - INFO - Received request cmpl-13b4873890534320a153327d16684d92-0: prompt: 'Thank you for noticing my outfit today, I put a lot of effort into it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13060, 498, 369, 61364, 847, 27303, 3351, 11, 358, 2182, 264, 2696, 315, 5041, 1119, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,470 - vllm.entrypoints.logger - INFO - Received request cmpl-2eee74a5ab8e4806bf7e66cebc97ad78-0: prompt: ", I'm sorry about throwing that paper ball at you. I don't know what came over me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 14589, 911, 21244, 429, 5567, 4935, 518, 498, 13, 358, 1513, 944, 1414, 1128, 3697, 916, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,476 - vllm.entrypoints.logger - INFO - Received request cmpl-d04501c326cc4b09bc60e9258fb2bf7d-0: prompt: "Playing the French Horn in the marching band has been so much fun. I feel like I've gained so much confidence from it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [27471, 279, 8585, 26114, 304, 279, 61098, 7055, 702, 1012, 773, 1753, 2464, 13, 358, 2666, 1075, 358, 3003, 18163, 773, 1753, 12136, 504, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,487 - vllm.entrypoints.logger - INFO - Received request cmpl-031b9fd1ce034362bd3f6727235e16b1-0: prompt: '*shows screen grab* Haha, have you seen this?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9, 59456, 4171, 11633, 9, 472, 13546, 11, 614, 498, 3884, 419, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,506 - vllm.entrypoints.logger - INFO - Received request cmpl-9915cb8d1f824e919a1517bce2cdf1a6-0: prompt: "I'm just not enjoying myself lately.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 1101, 537, 21413, 7037, 30345, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,509 - vllm.entrypoints.logger - INFO - Received request cmpl-bee9ca3bbbf1435f9685a3023a691720-0: prompt: "Doctor, I was in a car accident and I'm still feeling quite sore. Do you think I'll be okay?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 572, 304, 264, 1803, 11423, 323, 358, 2776, 2058, 8266, 5008, 35266, 13, 3155, 498, 1744, 358, 3278, 387, 16910, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,511 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-13b4873890534320a153327d16684d92-0.
2025-12-19 23:59:18,513 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2eee74a5ab8e4806bf7e66cebc97ad78-0.
2025-12-19 23:59:18,514 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d04501c326cc4b09bc60e9258fb2bf7d-0.
2025-12-19 23:59:18,515 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-031b9fd1ce034362bd3f6727235e16b1-0.
2025-12-19 23:59:18,516 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9915cb8d1f824e919a1517bce2cdf1a6-0.
2025-12-19 23:59:18,517 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bee9ca3bbbf1435f9685a3023a691720-0.
2025-12-19 23:59:18,519 - vllm.core.scheduler - INFO - Pending queue size: (403)
2025-12-19 23:59:18,564 - vllm.core.scheduler - INFO - Pending queue size: (403)
2025-12-19 23:59:18,569 - vllm.entrypoints.logger - INFO - Received request cmpl-26394651d78b442d808a3f56bd6b5929-0: prompt: "Sorry, I just don't want to shake hands right now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [19152, 11, 358, 1101, 1513, 944, 1366, 311, 26025, 6078, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,571 - vllm.entrypoints.logger - INFO - Received request cmpl-87855d6ffa804d498dc2399390cdd815-0: prompt: 'Hey, remember that time I asked you out and you turned me down?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 6099, 429, 882, 358, 4588, 498, 700, 323, 498, 6519, 752, 1495, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,573 - vllm.entrypoints.logger - INFO - Received request cmpl-69ff4b8ac96d478b81f9873e6c5a870c-0: prompt: 'Hi Mentor! How are you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 0, 2585, 525, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,574 - vllm.entrypoints.logger - INFO - Received request cmpl-4420eaf1501245088ced7a9be8446e57-0: prompt: "Look what I got today! It's the newest video game everyone is talking about.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10380, 1128, 358, 2684, 3351, 0, 1084, 594, 279, 23601, 2766, 1809, 5019, 374, 7404, 911, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,576 - vllm.entrypoints.logger - INFO - Received request cmpl-49c36d545b55417ebc2d7ce7a476c0f0-0: prompt: 'Hi there, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,590 - vllm.entrypoints.logger - INFO - Received request cmpl-269188277fd04aba82ecca1a85998891-0: prompt: "Hey neighbor, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,598 - vllm.entrypoints.logger - INFO - Received request cmpl-be271e4ee06343ea9433448994945fc5-0: prompt: 'You know, I had a really powerful experience the other day. I was feeling lost and overwhelmed, but after praying to God for guidance, I felt a sense of peace and love wash over me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 1030, 264, 2167, 7988, 3139, 279, 1008, 1899, 13, 358, 572, 8266, 5558, 323, 42106, 11, 714, 1283, 51132, 311, 4264, 369, 18821, 11, 358, 6476, 264, 5530, 315, 8919, 323, 2948, 11369, 916, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,599 - vllm.entrypoints.logger - INFO - Received request cmpl-207763127c014362aadff3615983c098-0: prompt: 'Hi Mentor, I just wanted to remind you that I completed the service you asked me to do, and I would like to be paid now.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 1101, 4829, 311, 23974, 498, 429, 358, 8145, 279, 2473, 498, 4588, 752, 311, 653, 11, 323, 358, 1035, 1075, 311, 387, 7171, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,607 - vllm.entrypoints.logger - INFO - Received request cmpl-3ad32a0c8c1f4f24b5895fde5f33e0c8-0: prompt: 'I regret what I did. I feel guilty for hurting my friend and wish I could take it back.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 22231, 1128, 358, 1521, 13, 358, 2666, 16007, 369, 47289, 847, 4238, 323, 6426, 358, 1410, 1896, 432, 1182, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,609 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-26394651d78b442d808a3f56bd6b5929-0.
2025-12-19 23:59:18,610 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-87855d6ffa804d498dc2399390cdd815-0.
2025-12-19 23:59:18,611 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-69ff4b8ac96d478b81f9873e6c5a870c-0.
2025-12-19 23:59:18,612 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4420eaf1501245088ced7a9be8446e57-0.
2025-12-19 23:59:18,613 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-49c36d545b55417ebc2d7ce7a476c0f0-0.
2025-12-19 23:59:18,614 - vllm.entrypoints.logger - INFO - Received request cmpl-2d56a8fffccb4bc4b55b114878ad05d3-0: prompt: 'What just happened? Why did the police officer arrest us?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3838, 1101, 6932, 30, 8429, 1521, 279, 4282, 9452, 8004, 601, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,614 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-269188277fd04aba82ecca1a85998891-0.
2025-12-19 23:59:18,615 - vllm.entrypoints.logger - INFO - Received request cmpl-2b5ca2bb17a24a2c8ee5cad081bdb7de-0: prompt: 'Ugh, did you notice that horrible smell coming from the garbage can?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 1521, 498, 5293, 429, 27102, 22068, 5001, 504, 279, 25878, 646, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,615 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-be271e4ee06343ea9433448994945fc5-0.
2025-12-19 23:59:18,616 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-207763127c014362aadff3615983c098-0.
2025-12-19 23:59:18,617 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3ad32a0c8c1f4f24b5895fde5f33e0c8-0.
2025-12-19 23:59:18,618 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2d56a8fffccb4bc4b55b114878ad05d3-0.
2025-12-19 23:59:18,619 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2b5ca2bb17a24a2c8ee5cad081bdb7de-0.
2025-12-19 23:59:18,620 - vllm.entrypoints.logger - INFO - Received request cmpl-03e3ee8d7fcf4f5db69e496cd237a421-0: prompt: "I've been really thinking lately about going back to school.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 2167, 7274, 30345, 911, 2087, 1182, 311, 2906, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,621 - vllm.core.scheduler - INFO - Pending queue size: (414)
2025-12-19 23:59:18,634 - vllm.entrypoints.logger - INFO - Received request cmpl-ecc9b6db00324848a8824f77ee00cacc-0: prompt: "My life just sucks. I have no friends, my family doesn't care about me, and I'm always the butt of everyone's jokes.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [5050, 2272, 1101, 39099, 13, 358, 614, 902, 4780, 11, 847, 2997, 3171, 944, 2453, 911, 752, 11, 323, 358, 2776, 2677, 279, 29956, 315, 5019, 594, 31420, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,665 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-03e3ee8d7fcf4f5db69e496cd237a421-0.
2025-12-19 23:59:18,647 - vllm.entrypoints.logger - INFO - Received request cmpl-f35cb047d11746e59a238c74e42dcf1e-0: prompt: "Ugh, I shouldn't have eaten all that ice cream. I've got such a bad headache now. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 13133, 944, 614, 34561, 678, 429, 9853, 12644, 13, 358, 3003, 2684, 1741, 264, 3873, 46746, 1431, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,672 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ecc9b6db00324848a8824f77ee00cacc-0.
2025-12-19 23:59:18,678 - vllm.entrypoints.logger - INFO - Received request cmpl-f40f2a2441ad46fc93baf10746226dbc-0: prompt: "I can't believe you punished me like that. It's humiliating.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 498, 40898, 752, 1075, 429, 13, 1084, 594, 91738, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,678 - vllm.core.scheduler - INFO - Pending queue size: (416)
2025-12-19 23:59:18,685 - vllm.entrypoints.logger - INFO - Received request cmpl-307c703e1c644da39f13d9e355c9714c-0: prompt: "Hey there, I noticed your tattoo. It's really cool. Can you tell me more about it?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=33, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 358, 13686, 697, 31794, 13, 1084, 594, 2167, 7010, 13, 2980, 498, 3291, 752, 803, 911, 432, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,689 - vllm.entrypoints.logger - INFO - Received request cmpl-ba845c1023724c22ade278b3c23e33f5-0: prompt: "Hey, check out what I got for my mom's birthday!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1779, 700, 1128, 358, 2684, 369, 847, 3368, 594, 15198, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,695 - vllm.entrypoints.logger - INFO - Received request cmpl-37cdb110d1eb49939f743b6e59cf4af4-0: prompt: "I'm a worrier, you know that?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 264, 4099, 7253, 11, 498, 1414, 429, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,718 - vllm.entrypoints.logger - INFO - Received request cmpl-ad56942c8c3344bb8599c0ef452937fc-0: prompt: 'I really miss those days when I used to work at the library after hours.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 3116, 1846, 2849, 979, 358, 1483, 311, 975, 518, 279, 6733, 1283, 4115, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,720 - vllm.entrypoints.logger - INFO - Received request cmpl-4363ed85d4f34265aa565123b9d4c530-0: prompt: "Mentor, I finally found the perfect coat at the store! I've been searching for so long and this one is just perfect.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 5499, 1730, 279, 4727, 22875, 518, 279, 3553, 0, 358, 3003, 1012, 15039, 369, 773, 1293, 323, 419, 825, 374, 1101, 4727, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,722 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f35cb047d11746e59a238c74e42dcf1e-0.
2025-12-19 23:59:18,723 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f40f2a2441ad46fc93baf10746226dbc-0.
2025-12-19 23:59:18,724 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-307c703e1c644da39f13d9e355c9714c-0.
2025-12-19 23:59:18,725 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ba845c1023724c22ade278b3c23e33f5-0.
2025-12-19 23:59:18,727 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-37cdb110d1eb49939f743b6e59cf4af4-0.
2025-12-19 23:59:18,728 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ad56942c8c3344bb8599c0ef452937fc-0.
2025-12-19 23:59:18,730 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4363ed85d4f34265aa565123b9d4c530-0.
2025-12-19 23:59:18,732 - vllm.core.scheduler - INFO - Pending queue size: (423)
2025-12-19 23:59:18,767 - vllm.entrypoints.logger - INFO - Received request cmpl-13e9d13ea55a4cce836922c643f21673-0: prompt: 'I have a new perspective on life, Classmates B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 264, 501, 13057, 389, 2272, 11, 3228, 16457, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,776 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-13e9d13ea55a4cce836922c643f21673-0.
2025-12-19 23:59:18,777 - vllm.entrypoints.logger - INFO - Received request cmpl-90a09807855b4186b6c7624449d8fc4a-0: prompt: "I can't believe how much I love this new house. Having all this extra space is really nice.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1246, 1753, 358, 2948, 419, 501, 3753, 13, 20035, 678, 419, 4960, 3550, 374, 2167, 6419, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,778 - vllm.core.scheduler - INFO - Pending queue size: (424)
2025-12-19 23:59:18,786 - vllm.entrypoints.logger - INFO - Received request cmpl-aa82fea0c9c64040bfcff0e205764adb-0: prompt: "Hey Doctor, I just wanted to share with you that I played a gig at Joe's Pub last night.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 18635, 11, 358, 1101, 4829, 311, 4332, 448, 498, 429, 358, 6342, 264, 22583, 518, 12846, 594, 22611, 1537, 3729, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,806 - vllm.entrypoints.logger - INFO - Received request cmpl-20cf69aa8a5f474ab938526608f84a21-0: prompt: 'Here, take this cigarette. I know you enjoy smoking.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [8420, 11, 1896, 419, 35113, 13, 358, 1414, 498, 4669, 19578, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,822 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-90a09807855b4186b6c7624449d8fc4a-0.
2025-12-19 23:59:18,823 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-aa82fea0c9c64040bfcff0e205764adb-0.
2025-12-19 23:59:18,824 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-20cf69aa8a5f474ab938526608f84a21-0.
2025-12-19 23:59:18,827 - vllm.core.scheduler - INFO - Pending queue size: (427)
2025-12-19 23:59:18,851 - vllm.entrypoints.logger - INFO - Received request cmpl-7724621e37b74b2687e88834ad1a72c0-0: prompt: ' I recently had to stand up for myself in court and testify against someone who hurt me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 5926, 1030, 311, 2498, 705, 369, 7037, 304, 5473, 323, 48162, 2348, 4325, 879, 12898, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,854 - vllm.entrypoints.logger - INFO - Received request cmpl-d16696d11a9348f6b4d4444c0a3bae6a-0: prompt: "Hey, have you heard about what happened to Co-workers B's goat?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 911, 1128, 6932, 311, 3539, 62284, 425, 594, 53292, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,864 - vllm.entrypoints.logger - INFO - Received request cmpl-683f6367ef4243cb847d25e433746319-0: prompt: "I am so excited for this weekend! I've planned so many fun things to do.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1079, 773, 12035, 369, 419, 9001, 0, 358, 3003, 12909, 773, 1657, 2464, 2513, 311, 653, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,872 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7724621e37b74b2687e88834ad1a72c0-0.
2025-12-19 23:59:18,873 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d16696d11a9348f6b4d4444c0a3bae6a-0.
2025-12-19 23:59:18,874 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-683f6367ef4243cb847d25e433746319-0.
2025-12-19 23:59:18,876 - vllm.core.scheduler - INFO - Pending queue size: (430)
2025-12-19 23:59:18,920 - vllm.entrypoints.logger - INFO - Received request cmpl-dbff85e5ff1940c7ae1863935af3921e-0: prompt: "Hey Mentor, I hope I'm not bothering you. I just wanted to talk to you for a bit.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 3900, 358, 2776, 537, 90159, 498, 13, 358, 1101, 4829, 311, 3061, 311, 498, 369, 264, 2699, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,920 - vllm.core.scheduler - INFO - Pending queue size: (430)
2025-12-19 23:59:18,922 - vllm.entrypoints.logger - INFO - Received request cmpl-90aa4f80f8914079a546dea11a143b74-0: prompt: "Hey, have you thought more about the project we're working on?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3381, 803, 911, 279, 2390, 582, 2299, 3238, 389, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,925 - vllm.entrypoints.logger - INFO - Received request cmpl-add47703d19d4618a38661d59b866d24-0: prompt: "I've been reading a lot about Alaska lately, and I think it's time for a change of scenery.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 5290, 264, 2696, 911, 27267, 30345, 11, 323, 358, 1744, 432, 594, 882, 369, 264, 2297, 315, 50231, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,928 - vllm.entrypoints.logger - INFO - Received request cmpl-95f87170c47e4e03a7c105967ad669b3-0: prompt: 'Oh no, I accidentally drew a line on the wall with my pencil.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 902, 11, 358, 32384, 23554, 264, 1555, 389, 279, 7002, 448, 847, 46118, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,932 - vllm.entrypoints.logger - INFO - Received request cmpl-28180c06691e4892a4b23583fe373c11-0: prompt: "I'm just too lazy to apply for that job. I'd rather just sit around all day and do nothing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 1101, 2238, 15678, 311, 3796, 369, 429, 2618, 13, 358, 4172, 4751, 1101, 2444, 2163, 678, 1899, 323, 653, 4302, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,956 - vllm.entrypoints.logger - INFO - Received request cmpl-591f252b8a3e41938f66b301c8edc6cc-0: prompt: 'I had a great time with you at Dave and Busters today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 264, 2244, 882, 448, 498, 518, 20238, 323, 425, 14605, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:18,966 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dbff85e5ff1940c7ae1863935af3921e-0.
2025-12-19 23:59:18,968 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-90aa4f80f8914079a546dea11a143b74-0.
2025-12-19 23:59:18,969 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-add47703d19d4618a38661d59b866d24-0.
2025-12-19 23:59:18,970 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-95f87170c47e4e03a7c105967ad669b3-0.
2025-12-19 23:59:18,971 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-28180c06691e4892a4b23583fe373c11-0.
2025-12-19 23:59:18,972 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-591f252b8a3e41938f66b301c8edc6cc-0.
2025-12-19 23:59:18,975 - vllm.core.scheduler - INFO - Pending queue size: (436)
2025-12-19 23:59:18,975 - vllm.entrypoints.logger - INFO - Received request cmpl-c3d645fa49cf4b0086364bb23f45877f-0: prompt: ' Do you need a light?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3155, 498, 1184, 264, 3100, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,004 - vllm.entrypoints.logger - INFO - Received request cmpl-1769a24913f14899812ea6d14a5dc022-0: prompt: "It's such a beautiful day today, isn't it?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 1741, 264, 6233, 1899, 3351, 11, 4436, 944, 432, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,013 - vllm.entrypoints.logger - INFO - Received request cmpl-82c96fcf471a434394ebad4f6461b189-0: prompt: 'Hey neighbor, I was thinking about the party we were planning. Do you still need help finding a place?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 9565, 11, 358, 572, 7274, 911, 279, 4614, 582, 1033, 9115, 13, 3155, 498, 2058, 1184, 1492, 9271, 264, 1992, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,019 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c3d645fa49cf4b0086364bb23f45877f-0.
2025-12-19 23:59:19,020 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1769a24913f14899812ea6d14a5dc022-0.
2025-12-19 23:59:19,021 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-82c96fcf471a434394ebad4f6461b189-0.
2025-12-19 23:59:19,024 - vllm.core.scheduler - INFO - Pending queue size: (439)
2025-12-19 23:59:19,050 - vllm.entrypoints.logger - INFO - Received request cmpl-891d3ce71d8649a1983d8538d6a3bd14-0: prompt: "*yawns* Sorry, I don't know what's wrong with me. I'm just so tired.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [33247, 39996, 9, 32286, 11, 358, 1513, 944, 1414, 1128, 594, 4969, 448, 752, 13, 358, 2776, 1101, 773, 19227, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,064 - vllm.entrypoints.logger - INFO - Received request cmpl-f48051aa0bfc49038c363eeeeed8c3d7-0: prompt: "I'm really struggling with writing these essays. No matter how hard I try, I can't seem to find the right words and my grammar is always off.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 19962, 448, 4378, 1493, 22844, 13, 2308, 4925, 1246, 2588, 358, 1430, 11, 358, 646, 944, 2803, 311, 1477, 279, 1290, 4244, 323, 847, 31428, 374, 2677, 1007, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,067 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-891d3ce71d8649a1983d8538d6a3bd14-0.
2025-12-19 23:59:19,069 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f48051aa0bfc49038c363eeeeed8c3d7-0.
2025-12-19 23:59:19,069 - vllm.entrypoints.logger - INFO - Received request cmpl-fa950dabe5d64d59bfd82e307037fd6d-0: prompt: 'I have to confess something to you, Neighbors B.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 311, 47366, 2494, 311, 498, 11, 4182, 24101, 425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,070 - vllm.entrypoints.logger - INFO - Received request cmpl-9c857ae2ed214b53bbde42824fc114af-0: prompt: ", I was wondering if there's any chance I could work on the new cutting-edge project that was just brought in?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 572, 20293, 421, 1052, 594, 894, 6012, 358, 1410, 975, 389, 279, 501, 14376, 47348, 2390, 429, 572, 1101, 7117, 304, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,071 - vllm.core.scheduler - INFO - Pending queue size: (441)
2025-12-19 23:59:19,085 - vllm.entrypoints.logger - INFO - Received request cmpl-ea6e1b82896b413a816ef9acc2d00ab2-0: prompt: 'Hey, have you seen the new machine I built in my backyard?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3884, 279, 501, 5662, 358, 5798, 304, 847, 35660, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,099 - vllm.entrypoints.logger - INFO - Received request cmpl-29fe72b6742a4b8d9406ace2ee28cda4-0: prompt: "I just can't help but get mad when someone tells me I've done something wrong. I feel like I'm being attacked.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 1492, 714, 633, 12796, 979, 4325, 10742, 752, 358, 3003, 2814, 2494, 4969, 13, 358, 2666, 1075, 358, 2776, 1660, 18349, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,115 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fa950dabe5d64d59bfd82e307037fd6d-0.
2025-12-19 23:59:19,117 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9c857ae2ed214b53bbde42824fc114af-0.
2025-12-19 23:59:19,118 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ea6e1b82896b413a816ef9acc2d00ab2-0.
2025-12-19 23:59:19,119 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-29fe72b6742a4b8d9406ace2ee28cda4-0.
2025-12-19 23:59:19,122 - vllm.core.scheduler - INFO - Pending queue size: (445)
2025-12-19 23:59:19,139 - vllm.entrypoints.logger - INFO - Received request cmpl-2f2e0c2fbb1640e491429864a54a6649-0: prompt: 'Ugh, my mom is so annoying sometimes. She interrupted my phone call with my friend just to make me do my homework.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 847, 3368, 374, 773, 29831, 7025, 13, 2932, 36783, 847, 4540, 1618, 448, 847, 4238, 1101, 311, 1281, 752, 653, 847, 28459, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,165 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2f2e0c2fbb1640e491429864a54a6649-0.
2025-12-19 23:59:19,166 - vllm.entrypoints.logger - INFO - Received request cmpl-24f9109bb9af4747ad1c71bd02f05639-0: prompt: 'Hi there, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,168 - vllm.core.scheduler - INFO - Pending queue size: (446)
2025-12-19 23:59:19,201 - vllm.entrypoints.logger - INFO - Received request cmpl-047d4c3c00d34d8ba7d40df9a881275d-0: prompt: "Mentor, I wanted to tell you that I recently became an evangelist and I've started preaching to people about the message that I believe in. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 4829, 311, 3291, 498, 429, 358, 5926, 6116, 458, 38053, 380, 323, 358, 3003, 3855, 68323, 311, 1251, 911, 279, 1943, 429, 358, 4411, 304, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,212 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-24f9109bb9af4747ad1c71bd02f05639-0.
2025-12-19 23:59:19,214 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-047d4c3c00d34d8ba7d40df9a881275d-0.
2025-12-19 23:59:19,217 - vllm.core.scheduler - INFO - Pending queue size: (448)
2025-12-19 23:59:19,260 - vllm.core.scheduler - INFO - Pending queue size: (448)
2025-12-19 23:59:19,281 - vllm.entrypoints.logger - INFO - Received request cmpl-e06cf36f0ff3459ba44670052d6ded14-0: prompt: "Get off of me, Neighbors B! This isn't funny anymore.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1949, 1007, 315, 752, 11, 4182, 24101, 425, 0, 1096, 4436, 944, 15173, 14584, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,291 - vllm.entrypoints.logger - INFO - Received request cmpl-c9a2131a689346989642466e2d8952ae-0: prompt: 'Excuse me, I need to use the restroom.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [41721, 810, 752, 11, 358, 1184, 311, 990, 279, 88809, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,292 - vllm.entrypoints.logger - INFO - Received request cmpl-4f12ed464ba94ebc82a2a9ec33b53f2d-0: prompt: "I'm just really frustrated because I feel like no one is listening to me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 1101, 2167, 32530, 1576, 358, 2666, 1075, 902, 825, 374, 14289, 311, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,302 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e06cf36f0ff3459ba44670052d6ded14-0.
2025-12-19 23:59:19,304 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c9a2131a689346989642466e2d8952ae-0.
2025-12-19 23:59:19,305 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4f12ed464ba94ebc82a2a9ec33b53f2d-0.
2025-12-19 23:59:19,307 - vllm.core.scheduler - INFO - Pending queue size: (451)
2025-12-19 23:59:19,311 - vllm.entrypoints.logger - INFO - Received request cmpl-c57acffb67a14a4793c85e656c107606-0: prompt: 'I really made a significant effort to be friends with everyone in our class.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 1865, 264, 5089, 5041, 311, 387, 4780, 448, 5019, 304, 1039, 536, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,330 - vllm.entrypoints.logger - INFO - Received request cmpl-dc5f304c0e3f4bd8866f5284a255541c-0: prompt: 'Hey, did you have fun at the park earlier today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 614, 2464, 518, 279, 6118, 6788, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,351 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c57acffb67a14a4793c85e656c107606-0.
2025-12-19 23:59:19,352 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dc5f304c0e3f4bd8866f5284a255541c-0.
2025-12-19 23:59:19,355 - vllm.core.scheduler - INFO - Pending queue size: (453)
2025-12-19 23:59:19,378 - vllm.entrypoints.logger - INFO - Received request cmpl-460608a5e8fb486bbd93b32c795a800e-0: prompt: 'Ah, it feels good to stretch my legs after sitting for so long.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 432, 11074, 1661, 311, 14501, 847, 14201, 1283, 11699, 369, 773, 1293, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,397 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-460608a5e8fb486bbd93b32c795a800e-0.
2025-12-19 23:59:19,400 - vllm.core.scheduler - INFO - Pending queue size: (454)
2025-12-19 23:59:19,402 - vllm.entrypoints.logger - INFO - Received request cmpl-b594e001209a4ee7b657c610ec3dadb8-0: prompt: 'Hey B, I wanted to ask you something. You know how sometimes people ask about your family?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 425, 11, 358, 4829, 311, 2548, 498, 2494, 13, 1446, 1414, 1246, 7025, 1251, 2548, 911, 697, 2997, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,404 - vllm.entrypoints.logger - INFO - Received request cmpl-a2c4cebf64194f0db43cc527d8d8064b-0: prompt: 'Doctor, I recently visited a prison and it was a disgusting experience. The walls were covered in graffiti, and the floor was dirty. There was a smell of urine in the air.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 5926, 11994, 264, 9343, 323, 432, 572, 264, 53666, 3139, 13, 576, 14285, 1033, 9761, 304, 64843, 11, 323, 279, 6422, 572, 18595, 13, 2619, 572, 264, 22068, 315, 39235, 304, 279, 3720, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,411 - vllm.entrypoints.logger - INFO - Received request cmpl-753c203cc44a41abb7fa9ebb249b0bfc-0: prompt: 'I was so nervous when I got on that sailboat, but I ended up really enjoying the ride.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 773, 22596, 979, 358, 2684, 389, 429, 29403, 37765, 11, 714, 358, 9482, 705, 2167, 21413, 279, 11877, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,419 - vllm.entrypoints.logger - INFO - Received request cmpl-d5fe0124eef24e1eb20b533925e23d4f-0: prompt: 'Boss, I have a new scarf that I absolutely love. I wear it all the time, even inside the house. It just feels so cozy and makes me feel good.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 358, 614, 264, 501, 67271, 429, 358, 10875, 2948, 13, 358, 9850, 432, 678, 279, 882, 11, 1496, 4766, 279, 3753, 13, 1084, 1101, 11074, 773, 42435, 323, 3643, 752, 2666, 1661, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,430 - vllm.entrypoints.logger - INFO - Received request cmpl-9afd9afb4f6549aa95ef114a0a94f0a8-0: prompt: "Mentor, I'm feeling really grateful for my job as a lawyer. It's incredibly rewarding to fight for justice and help my clients. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 2776, 8266, 2167, 25195, 369, 847, 2618, 438, 264, 15417, 13, 1084, 594, 16815, 40993, 311, 4367, 369, 12161, 323, 1492, 847, 8239, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,433 - vllm.entrypoints.logger - INFO - Received request cmpl-a2f5bbeedef04453b5d03e5dad22096a-0: prompt: 'Boss, I watched a video about families last night and it really gave me a better understanding of how different families function.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 358, 15384, 264, 2766, 911, 8521, 1537, 3729, 323, 432, 2167, 6551, 752, 264, 2664, 8660, 315, 1246, 2155, 8521, 729, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,436 - vllm.entrypoints.logger - INFO - Received request cmpl-aa8911854bf94d6e8ca428f99c9bb839-0: prompt: 'Hey, B, can I talk to you for a minute?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 11, 646, 358, 3061, 311, 498, 369, 264, 9383, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,439 - vllm.entrypoints.logger - INFO - Received request cmpl-435bf0d407b34390a5fccf100fe653c7-0: prompt: "I'm not going to back down, no matter what.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 537, 2087, 311, 1182, 1495, 11, 902, 4925, 1128, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,443 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b594e001209a4ee7b657c610ec3dadb8-0.
2025-12-19 23:59:19,444 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a2c4cebf64194f0db43cc527d8d8064b-0.
2025-12-19 23:59:19,446 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-753c203cc44a41abb7fa9ebb249b0bfc-0.
2025-12-19 23:59:19,447 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d5fe0124eef24e1eb20b533925e23d4f-0.
2025-12-19 23:59:19,448 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9afd9afb4f6549aa95ef114a0a94f0a8-0.
2025-12-19 23:59:19,449 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a2f5bbeedef04453b5d03e5dad22096a-0.
2025-12-19 23:59:19,450 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-aa8911854bf94d6e8ca428f99c9bb839-0.
2025-12-19 23:59:19,451 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-435bf0d407b34390a5fccf100fe653c7-0.
2025-12-19 23:59:19,453 - vllm.core.scheduler - INFO - Pending queue size: (462)
2025-12-19 23:59:19,498 - vllm.core.scheduler - INFO - Pending queue size: (462)
2025-12-19 23:59:19,519 - vllm.entrypoints.logger - INFO - Received request cmpl-2d163b6a40d643c28149cf226d54bc1b-0: prompt: "I'm so excited today! I get to learn something new.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 3351, 0, 358, 633, 311, 3960, 2494, 501, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,522 - vllm.entrypoints.logger - INFO - Received request cmpl-ad523249ac664264b585db12f24c07a5-0: prompt: 'There you go, Teacher. The perfect temperature for your bath.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3862, 498, 728, 11, 29069, 13, 576, 4727, 9315, 369, 697, 8885, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,523 - vllm.entrypoints.logger - INFO - Received request cmpl-0232c7e80ddc4c8293b68e86d93ee379-0: prompt: "Hey there! I'm throwing a party for my friend's birthday next weekend.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 0, 358, 2776, 21244, 264, 4614, 369, 847, 4238, 594, 15198, 1790, 9001, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,525 - vllm.entrypoints.logger - INFO - Received request cmpl-eb5fcec0542e430b92d85f875ea79b5e-0: prompt: "I just can't give up when there's something I want. It's just not in my nature.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 2968, 705, 979, 1052, 594, 2494, 358, 1366, 13, 1084, 594, 1101, 537, 304, 847, 6993, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,526 - vllm.entrypoints.logger - INFO - Received request cmpl-bc07a53501e7426f88f390279b1d1344-0: prompt: 'I wanted to tell you about something I did last weekend. Remember how I always talk about wanting to feel the sensation of falling?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 4829, 311, 3291, 498, 911, 2494, 358, 1521, 1537, 9001, 13, 19881, 1246, 358, 2677, 3061, 911, 19211, 311, 2666, 279, 36292, 315, 15679, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,533 - vllm.entrypoints.logger - INFO - Received request cmpl-0d2db86319b14d0e9411991ad296c149-0: prompt: "I can't believe the competition is tomorrow! I still need to do my hair and makeup.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 279, 10707, 374, 16577, 0, 358, 2058, 1184, 311, 653, 847, 6869, 323, 26551, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,541 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2d163b6a40d643c28149cf226d54bc1b-0.
2025-12-19 23:59:19,542 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ad523249ac664264b585db12f24c07a5-0.
2025-12-19 23:59:19,543 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0232c7e80ddc4c8293b68e86d93ee379-0.
2025-12-19 23:59:19,544 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eb5fcec0542e430b92d85f875ea79b5e-0.
2025-12-19 23:59:19,545 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc07a53501e7426f88f390279b1d1344-0.
2025-12-19 23:59:19,546 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d2db86319b14d0e9411991ad296c149-0.
2025-12-19 23:59:19,548 - vllm.core.scheduler - INFO - Pending queue size: (468)
2025-12-19 23:59:19,558 - vllm.entrypoints.logger - INFO - Received request cmpl-92847626b9ef47ad928dc31335b585db-0: prompt: "Hi there! I'm Neighbors A, and I'm new here. I'm looking for a new place to call home.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 0, 358, 2776, 4182, 24101, 362, 11, 323, 358, 2776, 501, 1588, 13, 358, 2776, 3330, 369, 264, 501, 1992, 311, 1618, 2114, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,571 - vllm.entrypoints.logger - INFO - Received request cmpl-3100252cf7a04d3c9d47650ed49fbe24-0: prompt: 'Hey, are you okay?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 525, 498, 16910, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,592 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-92847626b9ef47ad928dc31335b585db-0.
2025-12-19 23:59:19,594 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3100252cf7a04d3c9d47650ed49fbe24-0.
2025-12-19 23:59:19,596 - vllm.core.scheduler - INFO - Pending queue size: (470)
2025-12-19 23:59:19,646 - vllm.core.scheduler - INFO - Pending queue size: (470)
2025-12-19 23:59:19,651 - vllm.entrypoints.logger - INFO - Received request cmpl-563df788cb2f49d9aebb15e453885614-0: prompt: 'Boss, I have something to talk to you about.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 358, 614, 2494, 311, 3061, 311, 498, 911, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,653 - vllm.entrypoints.logger - INFO - Received request cmpl-bc76ff06241b4ae693a36524699910b9-0: prompt: "This artwork is really impressive, don't you think?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 28309, 374, 2167, 15978, 11, 1513, 944, 498, 1744, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,656 - vllm.entrypoints.logger - INFO - Received request cmpl-95e7dadbe90645d6b465dc8cbf788772-0: prompt: 'Hey, you just got up and left without saying goodbye earlier. That kinda hurt.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 498, 1101, 2684, 705, 323, 2115, 2041, 5488, 46455, 6788, 13, 2938, 33390, 12898, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,663 - vllm.entrypoints.logger - INFO - Received request cmpl-d5b2c001d97d4395865ad8d3a7184ab4-0: prompt: 'This is such a beautiful day to be walking along the beach.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 1741, 264, 6233, 1899, 311, 387, 11435, 3156, 279, 11321, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,684 - vllm.entrypoints.logger - INFO - Received request cmpl-6af9c2d19a90499ebd08ba1c909d2543-0: prompt: "James, I'm appointing you as my successor.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [28084, 11, 358, 2776, 9540, 287, 498, 438, 847, 33565, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,685 - vllm.entrypoints.logger - INFO - Received request cmpl-022bd627cbef42899d74bc0fbe65c9c6-0: prompt: ' I had another nightmare about you last night.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1030, 2441, 37811, 911, 498, 1537, 3729, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,687 - vllm.entrypoints.logger - INFO - Received request cmpl-27075d56a9b74965a4b639d34467642e-0: prompt: 'Good morning, Boss! I just wanted to let you know that I am really excited to start working as a cashier here.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 31569, 0, 358, 1101, 4829, 311, 1077, 498, 1414, 429, 358, 1079, 2167, 12035, 311, 1191, 3238, 438, 264, 90355, 1588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,690 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-563df788cb2f49d9aebb15e453885614-0.
2025-12-19 23:59:19,691 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bc76ff06241b4ae693a36524699910b9-0.
2025-12-19 23:59:19,692 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-95e7dadbe90645d6b465dc8cbf788772-0.
2025-12-19 23:59:19,693 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d5b2c001d97d4395865ad8d3a7184ab4-0.
2025-12-19 23:59:19,694 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6af9c2d19a90499ebd08ba1c909d2543-0.
2025-12-19 23:59:19,695 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-022bd627cbef42899d74bc0fbe65c9c6-0.
2025-12-19 23:59:19,696 - vllm.entrypoints.logger - INFO - Received request cmpl-a72c89b93cf64df08fb1d09ceea5bb8d-0: prompt: "*reaches for salt and grazes Co-workers B's hand* Oh, sorry about that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9, 265, 14242, 369, 12021, 323, 50833, 288, 3539, 62284, 425, 594, 1424, 9, 8670, 11, 14589, 911, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,696 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-27075d56a9b74965a4b639d34467642e-0.
2025-12-19 23:59:19,699 - vllm.core.scheduler - INFO - Pending queue size: (477)
2025-12-19 23:59:19,699 - vllm.entrypoints.logger - INFO - Received request cmpl-dce8f4f4527146e58fa20a6815d1f31b-0: prompt: "I've been thinking a lot about the project we talked about last time we spoke.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 1012, 7274, 264, 2696, 911, 279, 2390, 582, 14897, 911, 1537, 882, 582, 12290, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,714 - vllm.entrypoints.logger - INFO - Received request cmpl-c413aa72987543688a63a54e6d0993cc-0: prompt: 'Hey, did you hear about that guy who got into a fight at the bar last night?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 429, 7412, 879, 2684, 1119, 264, 4367, 518, 279, 3619, 1537, 3729, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,715 - vllm.entrypoints.logger - INFO - Received request cmpl-39f533d28b824d929035d2232c0aadbd-0: prompt: 'Hey, have you heard about that new Animal Crossing game?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 911, 429, 501, 21292, 64993, 1809, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,716 - vllm.entrypoints.logger - INFO - Received request cmpl-357215a5a9cd49c2bcd493f0b2a4663a-0: prompt: "I still can't believe that I finally found my dream girl who swept me off my feet.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2058, 646, 944, 4411, 429, 358, 5499, 1730, 847, 7904, 3743, 879, 40223, 752, 1007, 847, 7541, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,721 - vllm.entrypoints.logger - INFO - Received request cmpl-4bb2f91201b2455cb96dac06b051b4ad-0: prompt: "I just can't seem to calm down. This situation is making me so angry.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 2803, 311, 19300, 1495, 13, 1096, 6534, 374, 3259, 752, 773, 18514, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,727 - vllm.entrypoints.logger - INFO - Received request cmpl-55c4fa88a7b448e3a3e4c340600eb1e7-0: prompt: " I'm so excited for the play! I've been practicing every day.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 773, 12035, 369, 279, 1486, 0, 358, 3003, 1012, 35566, 1449, 1899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,741 - vllm.entrypoints.logger - INFO - Received request cmpl-9ebc92a816f643dfa20f6dc44b6fce7a-0: prompt: 'I joined a beginning running class to improve my stamina. I really want to be able to run for more than five minutes without getting winded.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 10859, 264, 7167, 4303, 536, 311, 7269, 847, 60684, 13, 358, 2167, 1366, 311, 387, 2952, 311, 1598, 369, 803, 1091, 4236, 4420, 2041, 3709, 9956, 291, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,742 - vllm.entrypoints.logger - INFO - Received request cmpl-ab0219ed05b34827a7e897a6edc515bb-0: prompt: "Hey, I need your help. I got a flat tire and my car won't go anywhere.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1184, 697, 1492, 13, 358, 2684, 264, 10063, 27287, 323, 847, 1803, 2765, 944, 728, 12379, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,743 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a72c89b93cf64df08fb1d09ceea5bb8d-0.
2025-12-19 23:59:19,744 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dce8f4f4527146e58fa20a6815d1f31b-0.
2025-12-19 23:59:19,744 - vllm.entrypoints.logger - INFO - Received request cmpl-3f054ce234534fdbadb68cac9c5af40c-0: prompt: "Oh my god, I'm feeling really dizzy right now. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 847, 9886, 11, 358, 2776, 8266, 2167, 84084, 1290, 1431, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,745 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c413aa72987543688a63a54e6d0993cc-0.
2025-12-19 23:59:19,746 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-39f533d28b824d929035d2232c0aadbd-0.
2025-12-19 23:59:19,746 - vllm.entrypoints.logger - INFO - Received request cmpl-dfa9faad595f4525985c29b1ef41add6-0: prompt: "I'm really unhappy with my current salary and position. I've been with this company for four years now, and I feel like I deserve a raise and a promotion.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 42151, 448, 847, 1482, 16107, 323, 2309, 13, 358, 3003, 1012, 448, 419, 2813, 369, 3040, 1635, 1431, 11, 323, 358, 2666, 1075, 358, 22695, 264, 4828, 323, 264, 20249, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,747 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-357215a5a9cd49c2bcd493f0b2a4663a-0.
2025-12-19 23:59:19,748 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4bb2f91201b2455cb96dac06b051b4ad-0.
2025-12-19 23:59:19,749 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-55c4fa88a7b448e3a3e4c340600eb1e7-0.
2025-12-19 23:59:19,750 - vllm.entrypoints.logger - INFO - Received request cmpl-ca9f719654f84152beeb6ce7b9c6272d-0: prompt: "I can't stop thinking about the conflict in Syria. It's been going on for so long and so many innocent people have been hurt or killed.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 2936, 7274, 911, 279, 12055, 304, 12623, 13, 1084, 594, 1012, 2087, 389, 369, 773, 1293, 323, 773, 1657, 24243, 1251, 614, 1012, 12898, 476, 7425, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,750 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9ebc92a816f643dfa20f6dc44b6fce7a-0.
2025-12-19 23:59:19,751 - vllm.entrypoints.logger - INFO - Received request cmpl-d6cb18f8d80f45b0af960cbefb240917-0: prompt: "Hi Neighbors B, it's great to see you here at the beach!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 4182, 24101, 425, 11, 432, 594, 2244, 311, 1490, 498, 1588, 518, 279, 11321, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,751 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ab0219ed05b34827a7e897a6edc515bb-0.
2025-12-19 23:59:19,752 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3f054ce234534fdbadb68cac9c5af40c-0.
2025-12-19 23:59:19,753 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dfa9faad595f4525985c29b1ef41add6-0.
2025-12-19 23:59:19,754 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ca9f719654f84152beeb6ce7b9c6272d-0.
2025-12-19 23:59:19,755 - vllm.entrypoints.logger - INFO - Received request cmpl-7c0ef673eae24fd79bb911cd3966e6f7-0: prompt: 'Hi! How are you doing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 0, 2585, 525, 498, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,755 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d6cb18f8d80f45b0af960cbefb240917-0.
2025-12-19 23:59:19,756 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7c0ef673eae24fd79bb911cd3966e6f7-0.
2025-12-19 23:59:19,758 - vllm.core.scheduler - INFO - Pending queue size: (491)
2025-12-19 23:59:19,778 - vllm.entrypoints.logger - INFO - Received request cmpl-036b13794db14ab1819d07eaae4415c1-0: prompt: "Recently, I've been trying to live more in the moment. It's made such a positive impact on my life.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [45137, 11, 358, 3003, 1012, 4460, 311, 3887, 803, 304, 279, 4445, 13, 1084, 594, 1865, 1741, 264, 6785, 5421, 389, 847, 2272, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,780 - vllm.entrypoints.logger - INFO - Received request cmpl-24f017f17365410d92c947c8c1e402c6-0: prompt: "Hey, have you noticed that I'm wearing this pretty silver necklace with a heart-shaped pendant lately?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 13686, 429, 358, 2776, 12233, 419, 5020, 14961, 54447, 448, 264, 4746, 34731, 41744, 30345, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,782 - vllm.entrypoints.logger - INFO - Received request cmpl-7b55674ddb5d44d581f946bd42416c47-0: prompt: "I love going on walks alone. It's so peaceful and relaxing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 2087, 389, 22479, 7484, 13, 1084, 594, 773, 25650, 323, 33848, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,784 - vllm.entrypoints.logger - INFO - Received request cmpl-a5f89efa03b3444a8ad1eb25b6bb822e-0: prompt: 'I was feeling sick yesterday with a stomachache and now I feel nauseous and have a headache.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 8266, 14036, 13671, 448, 264, 22350, 1777, 323, 1431, 358, 2666, 48294, 782, 323, 614, 264, 46746, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,794 - vllm.entrypoints.logger - INFO - Received request cmpl-c36f339c80354dcfa2560540d93d5b65-0: prompt: 'Hey, have you heard about the new program I wrote? It helps people keep track of their spending.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 6617, 911, 279, 501, 2025, 358, 6139, 30, 1084, 8609, 1251, 2506, 3754, 315, 862, 10164, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,802 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-036b13794db14ab1819d07eaae4415c1-0.
2025-12-19 23:59:19,803 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-24f017f17365410d92c947c8c1e402c6-0.
2025-12-19 23:59:19,804 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7b55674ddb5d44d581f946bd42416c47-0.
2025-12-19 23:59:19,805 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a5f89efa03b3444a8ad1eb25b6bb822e-0.
2025-12-19 23:59:19,806 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c36f339c80354dcfa2560540d93d5b65-0.
2025-12-19 23:59:19,809 - vllm.core.scheduler - INFO - Pending queue size: (496)
2025-12-19 23:59:19,809 - vllm.entrypoints.logger - INFO - Received request cmpl-ef62fe7e22c34f5585585ddca1ec1c00-0: prompt: "Hey, B! Look what I made in my art class today! It's a sculpture made out of clay.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 0, 9192, 1128, 358, 1865, 304, 847, 1947, 536, 3351, 0, 1084, 594, 264, 49967, 1865, 700, 315, 36048, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,815 - vllm.entrypoints.logger - INFO - Received request cmpl-aa5e04b5bc584be1aab10e06130992bd-0: prompt: "I'm really happy to just sit here holding your hands like this.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 6247, 311, 1101, 2444, 1588, 9963, 697, 6078, 1075, 419, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,822 - vllm.entrypoints.logger - INFO - Received request cmpl-b016d2547f4e4b62a0d83dcdaa729537-0: prompt: "I want to talk to you about something that's been bothering me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1366, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 90159, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,836 - vllm.entrypoints.logger - INFO - Received request cmpl-94bbe0a058e24435b34748534048062a-0: prompt: "Oh my god, I can't believe I just dropped all those dishes. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 847, 9886, 11, 358, 646, 944, 4411, 358, 1101, 12226, 678, 1846, 25779, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,850 - vllm.entrypoints.logger - INFO - Received request cmpl-f59bad176bea4a44b836435eb0c227da-0: prompt: "Boss, I'm so happy that we finally got a cat together!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [56975, 11, 358, 2776, 773, 6247, 429, 582, 5499, 2684, 264, 8251, 3786, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,852 - vllm.entrypoints.logger - INFO - Received request cmpl-4d8821bd585442ddb2c7881105cace4c-0: prompt: '*yawning* Man, I must have been really tired. I slept for a few hours on the couch.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [33247, 50846, 9, 2363, 11, 358, 1969, 614, 1012, 2167, 19227, 13, 358, 45398, 369, 264, 2421, 4115, 389, 279, 26148, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,852 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ef62fe7e22c34f5585585ddca1ec1c00-0.
2025-12-19 23:59:19,854 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-aa5e04b5bc584be1aab10e06130992bd-0.
2025-12-19 23:59:19,854 - vllm.entrypoints.logger - INFO - Received request cmpl-8670b889b032443cbd022a5d73a1655d-0: prompt: 'Can you imagine finding the cure for cancer?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 12793, 9271, 279, 26116, 369, 9387, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,855 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b016d2547f4e4b62a0d83dcdaa729537-0.
2025-12-19 23:59:19,856 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-94bbe0a058e24435b34748534048062a-0.
2025-12-19 23:59:19,857 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f59bad176bea4a44b836435eb0c227da-0.
2025-12-19 23:59:19,858 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d8821bd585442ddb2c7881105cace4c-0.
2025-12-19 23:59:19,858 - vllm.entrypoints.logger - INFO - Received request cmpl-22b735bd4c9444a78cf2976d3a57057a-0: prompt: "I appreciate that you always come to me when you need to calm down. It's important to have someone to talk to when anger takes over.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 15401, 429, 498, 2677, 2525, 311, 752, 979, 498, 1184, 311, 19300, 1495, 13, 1084, 594, 2989, 311, 614, 4325, 311, 3061, 311, 979, 19234, 4990, 916, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,859 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8670b889b032443cbd022a5d73a1655d-0.
2025-12-19 23:59:19,860 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-22b735bd4c9444a78cf2976d3a57057a-0.
2025-12-19 23:59:19,862 - vllm.core.scheduler - INFO - Pending queue size: (504)
2025-12-19 23:59:19,871 - vllm.entrypoints.logger - INFO - Received request cmpl-3138664d1743453b8858b9ee7d795469-0: prompt: 'Today, I hugged my mother because I wanted to show her how much I love her.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 93468, 847, 6554, 1576, 358, 4829, 311, 1473, 1059, 1246, 1753, 358, 2948, 1059, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,905 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3138664d1743453b8858b9ee7d795469-0.
2025-12-19 23:59:19,909 - vllm.core.scheduler - INFO - Pending queue size: (505)
2025-12-19 23:59:19,917 - vllm.entrypoints.logger - INFO - Received request cmpl-05a6016824474359865e7f8e2231abbd-0: prompt: 'I have been thinking about how to solve the problem we are facing.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 1012, 7274, 911, 1246, 311, 11625, 279, 3491, 582, 525, 12880, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,921 - vllm.entrypoints.logger - INFO - Received request cmpl-1a1d18dd6a0541f489157e7426f0a5c1-0: prompt: 'Hey, Co-worker B. What are you up to?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 3539, 65516, 425, 13, 3555, 525, 498, 705, 311, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,923 - vllm.entrypoints.logger - INFO - Received request cmpl-08301b86b3a4452fad0aedda3d84701b-0: prompt: ', I have something on my mind that I need to talk to you about.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 614, 2494, 389, 847, 3971, 429, 358, 1184, 311, 3061, 311, 498, 911, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,924 - vllm.entrypoints.logger - INFO - Received request cmpl-7febe842f6e14439940d447ad510b705-0: prompt: "Oh my gosh, I can't even imagine what you went through.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 847, 342, 9267, 11, 358, 646, 944, 1496, 12793, 1128, 498, 3937, 1526, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,926 - vllm.entrypoints.logger - INFO - Received request cmpl-c262bfebe2cb4c089ffb5136318bfeac-0: prompt: "Good morning! I'm already making breakfast, would you like some scrambled eggs, bacon, and toast?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 0, 358, 2776, 2669, 3259, 17496, 11, 1035, 498, 1075, 1045, 70478, 18805, 11, 40352, 11, 323, 22405, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,927 - vllm.entrypoints.logger - INFO - Received request cmpl-0d7d61df7ad44e5d906f2730cd924e11-0: prompt: 'I love you so much, my dear Wife. Your skin feels so soft against my lips.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 498, 773, 1753, 11, 847, 24253, 42408, 13, 4615, 6787, 11074, 773, 8413, 2348, 847, 22877, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,930 - vllm.entrypoints.logger - INFO - Received request cmpl-6ba8de18d83547cca10394a2a9fe328c-0: prompt: ' Hi doctor, how are you?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [21018, 10668, 11, 1246, 525, 498, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,936 - vllm.entrypoints.logger - INFO - Received request cmpl-d4b7161e4e984d06b7db34bd9b5ed808-0: prompt: "Ugh, I can't believe I forgot to buy bread.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 646, 944, 4411, 358, 28595, 311, 3695, 16002, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,938 - vllm.entrypoints.logger - INFO - Received request cmpl-63abb1eef6a44aee85370b8a74fcb86b-0: prompt: ', I have to say that I completely disagree with your philosophy on lying and cheating.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 614, 311, 1977, 429, 358, 6587, 28295, 448, 697, 19128, 389, 20446, 323, 41723, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,950 - vllm.entrypoints.logger - INFO - Received request cmpl-b58fb8afa6a447268cb3217bb9e75003-0: prompt: 'I love walking downtown when the sun is setting. The city just feels so alive.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 11435, 18907, 979, 279, 7015, 374, 6243, 13, 576, 3283, 1101, 11074, 773, 13675, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:19,952 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-05a6016824474359865e7f8e2231abbd-0.
2025-12-19 23:59:19,955 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1a1d18dd6a0541f489157e7426f0a5c1-0.
2025-12-19 23:59:19,956 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-08301b86b3a4452fad0aedda3d84701b-0.
2025-12-19 23:59:19,957 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7febe842f6e14439940d447ad510b705-0.
2025-12-19 23:59:19,958 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c262bfebe2cb4c089ffb5136318bfeac-0.
2025-12-19 23:59:19,959 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d7d61df7ad44e5d906f2730cd924e11-0.
2025-12-19 23:59:19,960 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6ba8de18d83547cca10394a2a9fe328c-0.
2025-12-19 23:59:19,965 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d4b7161e4e984d06b7db34bd9b5ed808-0.
2025-12-19 23:59:19,966 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-63abb1eef6a44aee85370b8a74fcb86b-0.
2025-12-19 23:59:19,969 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b58fb8afa6a447268cb3217bb9e75003-0.
2025-12-19 23:59:19,971 - vllm.core.scheduler - INFO - Pending queue size: (515)
2025-12-19 23:59:19,978 - vllm.entrypoints.logger - INFO - Received request cmpl-54bca051ea8d4822afee30e14e44a56d-0: prompt: "I have something to tell you, B. I realized that I don't want drugs in my life anymore, so I threw away my entire stash.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 2494, 311, 3291, 498, 11, 425, 13, 358, 15043, 429, 358, 1513, 944, 1366, 10975, 304, 847, 2272, 14584, 11, 773, 358, 22192, 3123, 847, 4453, 64037, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,015 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-54bca051ea8d4822afee30e14e44a56d-0.
2025-12-19 23:59:20,017 - vllm.entrypoints.logger - INFO - Received request cmpl-c441c9a9f02e4320882dc6292ebdbe72-0: prompt: 'Excuse me, I need to go into the corner for a bit.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [41721, 810, 752, 11, 358, 1184, 311, 728, 1119, 279, 9131, 369, 264, 2699, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,018 - vllm.core.scheduler - INFO - Pending queue size: (516)
2025-12-19 23:59:20,028 - vllm.entrypoints.logger - INFO - Received request cmpl-ce99049223f84361a89e81f4ae28d4fa-0: prompt: 'Hey, I wanted to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,049 - vllm.entrypoints.logger - INFO - Received request cmpl-8f4ae70bb2cf494f8fbe2d565bc7d166-0: prompt: 'I had the best time at dance class today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 279, 1850, 882, 518, 15254, 536, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,061 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c441c9a9f02e4320882dc6292ebdbe72-0.
2025-12-19 23:59:20,063 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ce99049223f84361a89e81f4ae28d4fa-0.
2025-12-19 23:59:20,064 - vllm.entrypoints.logger - INFO - Received request cmpl-8316421a675a4b06844bd4731c461bf4-0: prompt: ", I wanted to tell you something. I really appreciate how you always think things through before acting. It's something I've noticed about you since we first met.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 4829, 311, 3291, 498, 2494, 13, 358, 2167, 15401, 1246, 498, 2677, 1744, 2513, 1526, 1573, 15358, 13, 1084, 594, 2494, 358, 3003, 13686, 911, 498, 2474, 582, 1156, 2270, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,064 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8f4ae70bb2cf494f8fbe2d565bc7d166-0.
2025-12-19 23:59:20,071 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8316421a675a4b06844bd4731c461bf4-0.
2025-12-19 23:59:20,073 - vllm.core.scheduler - INFO - Pending queue size: (520)
2025-12-19 23:59:20,100 - vllm.entrypoints.logger - INFO - Received request cmpl-3b661db7ce504eea859ceea75246b4f0-0: prompt: 'Hey, I had a great time collaborating with you on that project we finished last week. I was thinking, if you need any assistance with other tasks that involve research and writing reports, I would be happy to help. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1030, 264, 2244, 882, 72201, 448, 498, 389, 429, 2390, 582, 8060, 1537, 2003, 13, 358, 572, 7274, 11, 421, 498, 1184, 894, 12994, 448, 1008, 9079, 429, 21058, 3412, 323, 4378, 6682, 11, 358, 1035, 387, 6247, 311, 1492, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,117 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3b661db7ce504eea859ceea75246b4f0-0.
2025-12-19 23:59:20,120 - vllm.core.scheduler - INFO - Pending queue size: (521)
2025-12-19 23:59:20,129 - vllm.entrypoints.logger - INFO - Received request cmpl-fd27cb24e388489197cd75eba2e2f02f-0: prompt: 'Hey, did you catch the soccer game last night?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 2287, 279, 22174, 1809, 1537, 3729, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,132 - vllm.entrypoints.logger - INFO - Received request cmpl-f4b68d61c017406fbb6f0e3d63c76c53-0: prompt: "Doctor, I've been thinking a lot about buying a new car.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 3003, 1012, 7274, 264, 2696, 911, 11833, 264, 501, 1803, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,136 - vllm.entrypoints.logger - INFO - Received request cmpl-e92d9aae4a1b45668ba6960985a8a94e-0: prompt: 'Hey, I have some exciting news to share with you! I finally became a naturalized citizen of the United States.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 614, 1045, 13245, 3669, 311, 4332, 448, 498, 0, 358, 5499, 6116, 264, 5810, 1506, 21860, 315, 279, 3639, 4180, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,137 - vllm.entrypoints.logger - INFO - Received request cmpl-5e6d262482b94e9dbfd1198fea2c9817-0: prompt: 'Hey, have you ever tried a new studying method?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 6679, 264, 501, 20956, 1714, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,142 - vllm.entrypoints.logger - INFO - Received request cmpl-c0fa9b9b8ba0416bb9ac2f1744a1aec0-0: prompt: "Isn't it amazing how the sunset can just instantly make you feel peaceful and relaxed?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [87941, 944, 432, 7897, 1246, 279, 42984, 646, 1101, 21818, 1281, 498, 2666, 25650, 323, 30367, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,162 - vllm.entrypoints.logger - INFO - Received request cmpl-7154a31ad5b0439fbf704a8f9bd51352-0: prompt: 'Hey, I tried that new chocolate bar you had in your fridge. It was really good!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 6679, 429, 501, 17931, 3619, 498, 1030, 304, 697, 37581, 13, 1084, 572, 2167, 1661, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,163 - vllm.entrypoints.logger - INFO - Received request cmpl-b82bd7f4d1ce4464a3064f6b19e2b99b-0: prompt: "I'm glad we were able to spend time with my sister today. It's been a while since we've seen her.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 15713, 582, 1033, 2952, 311, 8329, 882, 448, 847, 12923, 3351, 13, 1084, 594, 1012, 264, 1393, 2474, 582, 3003, 3884, 1059, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,164 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fd27cb24e388489197cd75eba2e2f02f-0.
2025-12-19 23:59:20,165 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f4b68d61c017406fbb6f0e3d63c76c53-0.
2025-12-19 23:59:20,166 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e92d9aae4a1b45668ba6960985a8a94e-0.
2025-12-19 23:59:20,167 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5e6d262482b94e9dbfd1198fea2c9817-0.
2025-12-19 23:59:20,168 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c0fa9b9b8ba0416bb9ac2f1744a1aec0-0.
2025-12-19 23:59:20,170 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7154a31ad5b0439fbf704a8f9bd51352-0.
2025-12-19 23:59:20,170 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b82bd7f4d1ce4464a3064f6b19e2b99b-0.
2025-12-19 23:59:20,173 - vllm.core.scheduler - INFO - Pending queue size: (528)
2025-12-19 23:59:20,205 - vllm.entrypoints.logger - INFO - Received request cmpl-e900cff5676a45dab7617cec782d2dd9-0: prompt: "Mentor, I've always been someone who enjoys learning from others. That's why I make it a point to ask my colleagues questions and hear about their experiences.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 3003, 2677, 1012, 4325, 879, 31738, 6832, 504, 3800, 13, 2938, 594, 3170, 358, 1281, 432, 264, 1459, 311, 2548, 847, 17639, 4755, 323, 6723, 911, 862, 11449, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,210 - vllm.entrypoints.logger - INFO - Received request cmpl-8bc65c2624194fe7850f7bef820145cd-0: prompt: "Oh no, I feel terrible! I didn't mean to completely soak you like that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 902, 11, 358, 2666, 17478, 0, 358, 3207, 944, 3076, 311, 6587, 60692, 498, 1075, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,217 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e900cff5676a45dab7617cec782d2dd9-0.
2025-12-19 23:59:20,218 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8bc65c2624194fe7850f7bef820145cd-0.
2025-12-19 23:59:20,220 - vllm.core.scheduler - INFO - Pending queue size: (530)
2025-12-19 23:59:20,252 - vllm.entrypoints.logger - INFO - Received request cmpl-bd80213431274c438229bc0d476cc2ae-0: prompt: 'The man who fought in the war was a hero. I really admire people who put their lives on the line for their country.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [785, 883, 879, 20463, 304, 279, 4116, 572, 264, 11821, 13, 358, 2167, 49763, 1251, 879, 2182, 862, 6305, 389, 279, 1555, 369, 862, 3146, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,261 - vllm.entrypoints.logger - INFO - Received request cmpl-6d233ce92b3c400a95460f74c20b80b0-0: prompt: "I need to tell you something, Neighbors B. I've decided to drop out of school.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1184, 311, 3291, 498, 2494, 11, 4182, 24101, 425, 13, 358, 3003, 6635, 311, 5943, 700, 315, 2906, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,264 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bd80213431274c438229bc0d476cc2ae-0.
2025-12-19 23:59:20,270 - vllm.core.scheduler - INFO - Pending queue size: (531)
2025-12-19 23:59:20,271 - vllm.entrypoints.logger - INFO - Received request cmpl-1447b059345d4dd5934e4669ad1a215f-0: prompt: "You wouldn't believe what happened to me last week.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 8270, 944, 4411, 1128, 6932, 311, 752, 1537, 2003, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,274 - vllm.entrypoints.logger - INFO - Received request cmpl-f89dc1018a7a4bcfbaf3cb0956a326bf-0: prompt: 'I really value independence and self-sufficiency.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 897, 23665, 323, 656, 1331, 1362, 10387, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,292 - vllm.entrypoints.logger - INFO - Received request cmpl-207a33b6c69a498ca11ae536d268f8b4-0: prompt: "I'm really excited for my trip next week, but I still need to make sure I've packed everything I need.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 12035, 369, 847, 8411, 1790, 2003, 11, 714, 358, 2058, 1184, 311, 1281, 2704, 358, 3003, 19375, 4297, 358, 1184, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,310 - vllm.entrypoints.logger - INFO - Received request cmpl-25fede951a404110b085316b5bfcffa0-0: prompt: ", I've been feeling really out of sorts lately. My new boss is always piling more work on me and it feels like nothing I do is ever good enough.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 3003, 1012, 8266, 2167, 700, 315, 20853, 30345, 13, 3017, 501, 13392, 374, 2677, 281, 7979, 803, 975, 389, 752, 323, 432, 11074, 1075, 4302, 358, 653, 374, 3512, 1661, 3322, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,314 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6d233ce92b3c400a95460f74c20b80b0-0.
2025-12-19 23:59:20,315 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1447b059345d4dd5934e4669ad1a215f-0.
2025-12-19 23:59:20,316 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f89dc1018a7a4bcfbaf3cb0956a326bf-0.
2025-12-19 23:59:20,324 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-207a33b6c69a498ca11ae536d268f8b4-0.
2025-12-19 23:59:20,325 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-25fede951a404110b085316b5bfcffa0-0.
2025-12-19 23:59:20,328 - vllm.core.scheduler - INFO - Pending queue size: (536)
2025-12-19 23:59:20,333 - vllm.entrypoints.logger - INFO - Received request cmpl-2e10b6e48dc44c1e8045aaee6f632971-0: prompt: "I'm feeling really satisfied with my work lately. I've been putting in a lot of effort and it's paying off.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 2167, 19527, 448, 847, 975, 30345, 13, 358, 3003, 1012, 10687, 304, 264, 2696, 315, 5041, 323, 432, 594, 12515, 1007, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,343 - vllm.entrypoints.logger - INFO - Received request cmpl-7c1ad9d6169c4e0a9c7d9d64add5420a-0: prompt: "Hey, good morning, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1661, 6556, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,357 - vllm.entrypoints.logger - INFO - Received request cmpl-80dc455bbb5a4a248ab7f0a7855d110c-0: prompt: " I can't wait to start making my art project. What are you planning to make, Classmates B?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 646, 944, 3783, 311, 1191, 3259, 847, 1947, 2390, 13, 3555, 525, 498, 9115, 311, 1281, 11, 3228, 16457, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,359 - vllm.entrypoints.logger - INFO - Received request cmpl-504b8b79fae14111a0bfe69277f81415-0: prompt: 'I really value my friendship with person Y. I feel like we have a great connection.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 897, 847, 26509, 448, 1697, 809, 13, 358, 2666, 1075, 582, 614, 264, 2244, 3633, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,369 - vllm.entrypoints.logger - INFO - Received request cmpl-22c03ce726834c7488ae495e80fa5874-0: prompt: "Hey, I wanted to share some news with you. I've decided to switch my profession from being a doctor to being a lawyer.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 4332, 1045, 3669, 448, 498, 13, 358, 3003, 6635, 311, 3398, 847, 4808, 504, 1660, 264, 10668, 311, 1660, 264, 15417, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,373 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2e10b6e48dc44c1e8045aaee6f632971-0.
2025-12-19 23:59:20,374 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7c1ad9d6169c4e0a9c7d9d64add5420a-0.
2025-12-19 23:59:20,375 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-80dc455bbb5a4a248ab7f0a7855d110c-0.
2025-12-19 23:59:20,377 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-504b8b79fae14111a0bfe69277f81415-0.
2025-12-19 23:59:20,378 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-22c03ce726834c7488ae495e80fa5874-0.
2025-12-19 23:59:20,378 - vllm.entrypoints.logger - INFO - Received request cmpl-1c1190f86aa94d6faef19bdab6d9efe7-0: prompt: "You're not going anywhere. I need to talk to you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 2299, 537, 2087, 12379, 13, 358, 1184, 311, 3061, 311, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,380 - vllm.core.scheduler - INFO - Pending queue size: (541)
2025-12-19 23:59:20,392 - vllm.entrypoints.logger - INFO - Received request cmpl-5c8f9888e8fa41b48a80bdcc9e0f2d30-0: prompt: "Doctor, I have been helping the victims of the fire lately. It's sad to see so many people hurt and so much damage.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 614, 1012, 10476, 279, 12415, 315, 279, 3940, 30345, 13, 1084, 594, 12421, 311, 1490, 773, 1657, 1251, 12898, 323, 773, 1753, 5557, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,394 - vllm.entrypoints.logger - INFO - Received request cmpl-45c95603e8ad420980d508e54f68cda0-0: prompt: 'Hey there, B! Have you seen my new nail shop in the town square yet?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 425, 0, 12243, 498, 3884, 847, 501, 30995, 8061, 304, 279, 6290, 9334, 3602, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,420 - vllm.entrypoints.logger - INFO - Received request cmpl-3f6c9083da4848199a872129243bb71a-0: prompt: "I can't believe I'm gone. It all happened so fast.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 2776, 8048, 13, 1084, 678, 6932, 773, 4937, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,424 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1c1190f86aa94d6faef19bdab6d9efe7-0.
2025-12-19 23:59:20,425 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5c8f9888e8fa41b48a80bdcc9e0f2d30-0.
2025-12-19 23:59:20,426 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-45c95603e8ad420980d508e54f68cda0-0.
2025-12-19 23:59:20,426 - vllm.entrypoints.logger - INFO - Received request cmpl-287a51361ade415fb906c43a32c8e32c-0: prompt: 'Hey, did you catch the game last night? My favorite team won!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 2287, 279, 1809, 1537, 3729, 30, 3017, 6930, 2083, 2765, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,427 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3f6c9083da4848199a872129243bb71a-0.
2025-12-19 23:59:20,428 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-287a51361ade415fb906c43a32c8e32c-0.
2025-12-19 23:59:20,430 - vllm.entrypoints.logger - INFO - Received request cmpl-c15f02bdf63044d8833bc94e55212ccd-0: prompt: 'Can you believe it? I found a secret room full of treasure in the park!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 1730, 264, 6234, 3054, 2480, 315, 31626, 304, 279, 6118, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,431 - vllm.core.scheduler - INFO - Pending queue size: (546)
2025-12-19 23:59:20,433 - vllm.entrypoints.logger - INFO - Received request cmpl-4d58d4f60ae24fb78d4bf72a1491e592-0: prompt: "I can't believe it. I just got promoted to lead mechanic at work!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 432, 13, 358, 1101, 2684, 28926, 311, 2990, 44139, 518, 975, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,450 - vllm.entrypoints.logger - INFO - Received request cmpl-0692f540485d4bf2b070fe8c55f126dc-0: prompt: 'I feel so out of place at this dinner party. Do you ever feel that way?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 773, 700, 315, 1992, 518, 419, 13856, 4614, 13, 3155, 498, 3512, 2666, 429, 1616, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,466 - vllm.entrypoints.logger - INFO - Received request cmpl-25b238728e4b41cc9e399375e9f94868-0: prompt: "Doctor, I need to talk to you about something that's been bothering me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 90159, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,468 - vllm.entrypoints.logger - INFO - Received request cmpl-215c2d4a03714c388f0d30d6207fa82d-0: prompt: 'Doctor, have I ever told you about how content I feel when I go fishing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 614, 358, 3512, 3229, 498, 911, 1246, 2213, 358, 2666, 979, 358, 728, 19948, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,474 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c15f02bdf63044d8833bc94e55212ccd-0.
2025-12-19 23:59:20,476 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d58d4f60ae24fb78d4bf72a1491e592-0.
2025-12-19 23:59:20,477 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0692f540485d4bf2b070fe8c55f126dc-0.
2025-12-19 23:59:20,478 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-25b238728e4b41cc9e399375e9f94868-0.
2025-12-19 23:59:20,479 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-215c2d4a03714c388f0d30d6207fa82d-0.
2025-12-19 23:59:20,481 - vllm.entrypoints.logger - INFO - Received request cmpl-d2aef64712a44634aff3f6a4e69a2a52-0: prompt: 'Can you believe it? I won the lottery last night!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 2765, 279, 38239, 1537, 3729, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,482 - vllm.core.scheduler - INFO - Pending queue size: (551)
2025-12-19 23:59:20,483 - vllm.entrypoints.logger - INFO - Received request cmpl-c3235e0a37d643dbbb2cf5e60bea8810-0: prompt: 'Hey, did you get the gift I sent you for your birthday?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 633, 279, 8189, 358, 3208, 498, 369, 697, 15198, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,500 - vllm.entrypoints.logger - INFO - Received request cmpl-9cb3749272524822ab1f3ed2c41a8fd0-0: prompt: 'I just took a job test today. It was challenging, but I feel more confident in my ability to do the job now.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 3867, 264, 2618, 1273, 3351, 13, 1084, 572, 17003, 11, 714, 358, 2666, 803, 16506, 304, 847, 5726, 311, 653, 279, 2618, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,503 - vllm.entrypoints.logger - INFO - Received request cmpl-a668159fe7fe4e668a63a7ad5e28cc1c-0: prompt: 'Ah, that was a decent meal.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 429, 572, 264, 14977, 15145, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,511 - vllm.entrypoints.logger - INFO - Received request cmpl-6f6463f91b6347ed89fb9dc7cbc0eff8-0: prompt: 'Hey, did you hear about my new will?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 847, 501, 686, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,525 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d2aef64712a44634aff3f6a4e69a2a52-0.
2025-12-19 23:59:20,526 - vllm.entrypoints.logger - INFO - Received request cmpl-e4fd2403b37846f3b9cf9cbf7ddde165-0: prompt: 'The destination is at the end of the street, just follow this road.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [785, 9106, 374, 518, 279, 835, 315, 279, 8592, 11, 1101, 1795, 419, 5636, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,527 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c3235e0a37d643dbbb2cf5e60bea8810-0.
2025-12-19 23:59:20,527 - vllm.entrypoints.logger - INFO - Received request cmpl-6f4af924af5d4a4d939421df437c49ba-0: prompt: 'In high school, I was always independent. I spent a year abroad in college, and I loved it. I learned so much about myself and the world around me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [641, 1550, 2906, 11, 358, 572, 2677, 9489, 13, 358, 7391, 264, 1042, 22131, 304, 7770, 11, 323, 358, 10245, 432, 13, 358, 9498, 773, 1753, 911, 7037, 323, 279, 1879, 2163, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,528 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9cb3749272524822ab1f3ed2c41a8fd0-0.
2025-12-19 23:59:20,529 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a668159fe7fe4e668a63a7ad5e28cc1c-0.
2025-12-19 23:59:20,530 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f6463f91b6347ed89fb9dc7cbc0eff8-0.
2025-12-19 23:59:20,532 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e4fd2403b37846f3b9cf9cbf7ddde165-0.
2025-12-19 23:59:20,533 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6f4af924af5d4a4d939421df437c49ba-0.
2025-12-19 23:59:20,535 - vllm.core.scheduler - INFO - Pending queue size: (558)
2025-12-19 23:59:20,546 - vllm.entrypoints.logger - INFO - Received request cmpl-a63b85816cc24244a71c43638af77167-0: prompt: "I grabbed a light beer at lunch today because I didn't want to get too drunk.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 29318, 264, 3100, 12883, 518, 15786, 3351, 1576, 358, 3207, 944, 1366, 311, 633, 2238, 28750, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,557 - vllm.entrypoints.logger - INFO - Received request cmpl-4ac36ede93584ec0b9129e43d5ad2a7c-0: prompt: "Hey, what's up? You seem kind of upset. I'm sorry if we did something to make you mad.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1128, 594, 705, 30, 1446, 2803, 3093, 315, 22459, 13, 358, 2776, 14589, 421, 582, 1521, 2494, 311, 1281, 498, 12796, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,561 - vllm.entrypoints.logger - INFO - Received request cmpl-c37c1b5719cb409b8960a7a415d6fca0-0: prompt: 'Good morning, Boss. I wanted to share some good news with you. Last week, I brainstormed some new ideas to market our company, and the number of applications has increased by 20%.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 31569, 13, 358, 4829, 311, 4332, 1045, 1661, 3669, 448, 498, 13, 7996, 2003, 11, 358, 86781, 291, 1045, 501, 6708, 311, 3081, 1039, 2813, 11, 323, 279, 1372, 315, 8357, 702, 7172, 553, 220, 17, 15, 14360], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,579 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a63b85816cc24244a71c43638af77167-0.
2025-12-19 23:59:20,580 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4ac36ede93584ec0b9129e43d5ad2a7c-0.
2025-12-19 23:59:20,581 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c37c1b5719cb409b8960a7a415d6fca0-0.
2025-12-19 23:59:20,583 - vllm.entrypoints.logger - INFO - Received request cmpl-559b3c2d4c6b4d4eae49f5991b66d1f9-0: prompt: "I'm feeling pretty nervous about starting my new job.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 5020, 22596, 911, 5916, 847, 501, 2618, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,584 - vllm.core.scheduler - INFO - Pending queue size: (561)
2025-12-19 23:59:20,586 - vllm.entrypoints.logger - INFO - Received request cmpl-e7e858e8b5b64c8fa20b3d0f065fb576-0: prompt: 'I had such an early start this morning. I woke up at 6:30, took a shower, got dressed for work, and left at 8:00.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 1741, 458, 4124, 1191, 419, 6556, 13, 358, 38726, 705, 518, 220, 21, 25, 18, 15, 11, 3867, 264, 17196, 11, 2684, 25365, 369, 975, 11, 323, 2115, 518, 220, 23, 25, 15, 15, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,590 - vllm.entrypoints.logger - INFO - Received request cmpl-fa3a5561375742a3b673d2df6bda5f78-0: prompt: "I can't believe it, B. My best friend was cheating on me this whole time. I feel so betrayed.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 432, 11, 425, 13, 3017, 1850, 4238, 572, 41723, 389, 752, 419, 4361, 882, 13, 358, 2666, 773, 64297, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,609 - vllm.entrypoints.logger - INFO - Received request cmpl-feb6a36cb3144dcf93d8e79b63d5df3b-0: prompt: 'Have you ever noticed how easy it is to get what you want by manipulating people?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 3512, 13686, 1246, 4135, 432, 374, 311, 633, 1128, 498, 1366, 553, 62514, 1251, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,628 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-559b3c2d4c6b4d4eae49f5991b66d1f9-0.
2025-12-19 23:59:20,629 - vllm.entrypoints.logger - INFO - Received request cmpl-4d211c2ec8e049348adf36cd3f2b2257-0: prompt: 'Can you believe it? I was waiting for my new shirt to arrive, and instead I got a box of shoes.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 572, 8580, 369, 847, 501, 15478, 311, 17331, 11, 323, 4518, 358, 2684, 264, 3745, 315, 15294, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,629 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e7e858e8b5b64c8fa20b3d0f065fb576-0.
2025-12-19 23:59:20,630 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fa3a5561375742a3b673d2df6bda5f78-0.
2025-12-19 23:59:20,632 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-feb6a36cb3144dcf93d8e79b63d5df3b-0.
2025-12-19 23:59:20,633 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4d211c2ec8e049348adf36cd3f2b2257-0.
2025-12-19 23:59:20,635 - vllm.entrypoints.logger - INFO - Received request cmpl-70e92b2d640e4accb8aab72170f8abaf-0: prompt: "How's your day been so far, neighbor?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4340, 594, 697, 1899, 1012, 773, 3041, 11, 9565, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,635 - vllm.core.scheduler - INFO - Pending queue size: (566)
2025-12-19 23:59:20,673 - vllm.entrypoints.logger - INFO - Received request cmpl-ac409987853b40748d119e7c285167fb-0: prompt: "That was a great dinner last night, wasn't it?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4792, 572, 264, 2244, 13856, 1537, 3729, 11, 5710, 944, 432, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,679 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-70e92b2d640e4accb8aab72170f8abaf-0.
2025-12-19 23:59:20,681 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ac409987853b40748d119e7c285167fb-0.
2025-12-19 23:59:20,684 - vllm.core.scheduler - INFO - Pending queue size: (568)
2025-12-19 23:59:20,714 - vllm.entrypoints.logger - INFO - Received request cmpl-c5b3dce2ee21412985b0967eef2c0980-0: prompt: "Hey, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,723 - vllm.entrypoints.logger - INFO - Received request cmpl-f43b287951cd42b5a2c3450027e5ce0f-0: prompt: 'This is a really good cheeseburger. Thanks for taking me out, Mentor.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 374, 264, 2167, 1661, 17163, 62452, 13, 11114, 369, 4633, 752, 700, 11, 91191, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,727 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c5b3dce2ee21412985b0967eef2c0980-0.
2025-12-19 23:59:20,728 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f43b287951cd42b5a2c3450027e5ce0f-0.
2025-12-19 23:59:20,731 - vllm.core.scheduler - INFO - Pending queue size: (570)
2025-12-19 23:59:20,734 - vllm.entrypoints.logger - INFO - Received request cmpl-bd9612204796436c9e793c832201c881-0: prompt: 'Hi Mentor, I am feeling quite curious today. I have been thinking about how much I love solving problems, and how I enjoy finding new ways to do things. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=35, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 1079, 8266, 5008, 22208, 3351, 13, 358, 614, 1012, 7274, 911, 1246, 1753, 358, 2948, 21828, 5322, 11, 323, 1246, 358, 4669, 9271, 501, 5510, 311, 653, 2513, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,750 - vllm.entrypoints.logger - INFO - Received request cmpl-42b76140c1424fcc825c92cbab2d155c-0: prompt: 'Hey there, do you ever feel like you get stuck when trying to solve a problem?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 653, 498, 3512, 2666, 1075, 498, 633, 15700, 979, 4460, 311, 11625, 264, 3491, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,752 - vllm.entrypoints.logger - INFO - Received request cmpl-18e218ffece34f7aaa9d3f889ad17f6e-0: prompt: ', do you remember the time when I brought some people to see you perform?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 653, 498, 6099, 279, 882, 979, 358, 7117, 1045, 1251, 311, 1490, 498, 2736, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,754 - vllm.entrypoints.logger - INFO - Received request cmpl-fafefbd69f71455d80aa212a8d3e38a8-0: prompt: "I can't believe I actually won the race yesterday!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 3520, 2765, 279, 6957, 13671, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,769 - vllm.entrypoints.logger - INFO - Received request cmpl-2cb09e98a684469fac4bf497ce77faf2-0: prompt: "I tried reading before bed again last night, but I couldn't even make it past the first page.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 6679, 5290, 1573, 4845, 1549, 1537, 3729, 11, 714, 358, 7691, 944, 1496, 1281, 432, 3267, 279, 1156, 2150, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,775 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bd9612204796436c9e793c832201c881-0.
2025-12-19 23:59:20,777 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-42b76140c1424fcc825c92cbab2d155c-0.
2025-12-19 23:59:20,778 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-18e218ffece34f7aaa9d3f889ad17f6e-0.
2025-12-19 23:59:20,779 - vllm.entrypoints.logger - INFO - Received request cmpl-f174c062b75542198f4631d0c6f02c43-0: prompt: " I can't believe I got arrested for carrying a concealed weapon. I never thought it would come to this.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 646, 944, 4411, 358, 2684, 12517, 369, 15331, 264, 43033, 10288, 13, 358, 2581, 3381, 432, 1035, 2525, 311, 419, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,779 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fafefbd69f71455d80aa212a8d3e38a8-0.
2025-12-19 23:59:20,780 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2cb09e98a684469fac4bf497ce77faf2-0.
2025-12-19 23:59:20,781 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f174c062b75542198f4631d0c6f02c43-0.
2025-12-19 23:59:20,784 - vllm.entrypoints.logger - INFO - Received request cmpl-cb106ba84c144476aad01872ec5e989b-0: prompt: 'Hey, did you hear about the charity auction I organized last Saturday?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 911, 279, 22846, 21165, 358, 16645, 1537, 7728, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,784 - vllm.core.scheduler - INFO - Pending queue size: (576)
2025-12-19 23:59:20,790 - vllm.entrypoints.logger - INFO - Received request cmpl-74d339681fec4476a3ee37f6492d3a63-0: prompt: 'Hey, I got a surprise for you!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 2684, 264, 12761, 369, 498, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,793 - vllm.entrypoints.logger - INFO - Received request cmpl-3069038cdb4d4f3094b54366f6e4dde3-0: prompt: 'I felt really hurt today. Everyone was playing the game and I was just left on the sidelines.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 6476, 2167, 12898, 3351, 13, 21455, 572, 5619, 279, 1809, 323, 358, 572, 1101, 2115, 389, 279, 69413, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,808 - vllm.entrypoints.logger - INFO - Received request cmpl-0d0ba388e3334bc49680c2795fe542d7-0: prompt: 'Hey there, how are you feeling today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 1246, 525, 498, 8266, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,817 - vllm.entrypoints.logger - INFO - Received request cmpl-7a76bfffdf9b4a568b78582b9266d3b1-0: prompt: "This steak is delicious! I can't believe how juicy it is.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 48959, 374, 17923, 0, 358, 646, 944, 4411, 1246, 55053, 432, 374, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,828 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cb106ba84c144476aad01872ec5e989b-0.
2025-12-19 23:59:20,829 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-74d339681fec4476a3ee37f6492d3a63-0.
2025-12-19 23:59:20,830 - vllm.entrypoints.logger - INFO - Received request cmpl-5004da0644414db1aa2b20ed7cff941e-0: prompt: "I don't know what to do about my grades, they keep slipping because I keep cutting class.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 1128, 311, 653, 911, 847, 27611, 11, 807, 2506, 63702, 1576, 358, 2506, 14376, 536, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,831 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3069038cdb4d4f3094b54366f6e4dde3-0.
2025-12-19 23:59:20,832 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0d0ba388e3334bc49680c2795fe542d7-0.
2025-12-19 23:59:20,833 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7a76bfffdf9b4a568b78582b9266d3b1-0.
2025-12-19 23:59:20,834 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5004da0644414db1aa2b20ed7cff941e-0.
2025-12-19 23:59:20,834 - vllm.entrypoints.logger - INFO - Received request cmpl-428bdd1f4110426bb87aa0d6c7bda506-0: prompt: "I'm really grateful for our friendship,. I feel like I can be myself around you and share my worries.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 25195, 369, 1039, 26509, 17515, 358, 2666, 1075, 358, 646, 387, 7037, 2163, 498, 323, 4332, 847, 37045, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,836 - vllm.core.scheduler - INFO - Pending queue size: (582)
2025-12-19 23:59:20,857 - vllm.entrypoints.logger - INFO - Received request cmpl-afcd831cf19447d69df7ebef9b225a0c-0: prompt: "Hey man, how's the haircut holding up?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 883, 11, 1246, 594, 279, 85724, 9963, 705, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,874 - vllm.entrypoints.logger - INFO - Received request cmpl-42536d03d72849548cbae60b1c39a0e0-0: prompt: 'I saw the breaking news report about the hurricane and knew I had to do something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 5485, 279, 14719, 3669, 1895, 911, 279, 48684, 323, 6876, 358, 1030, 311, 653, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,880 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-428bdd1f4110426bb87aa0d6c7bda506-0.
2025-12-19 23:59:20,881 - vllm.entrypoints.logger - INFO - Received request cmpl-2c4b603eeefa4a018cb4f4907d8b30be-0: prompt: "I don't know, man, I'm feeling pretty nervous about this party we're going to tonight.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 11, 883, 11, 358, 2776, 8266, 5020, 22596, 911, 419, 4614, 582, 2299, 2087, 311, 17913, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,882 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-afcd831cf19447d69df7ebef9b225a0c-0.
2025-12-19 23:59:20,882 - vllm.entrypoints.logger - INFO - Received request cmpl-cc381d4bd28247b69bc964f96461ee1a-0: prompt: 'It was such an incredible experience performing under the tree today!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 572, 1741, 458, 15050, 3139, 16380, 1212, 279, 4916, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,883 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-42536d03d72849548cbae60b1c39a0e0-0.
2025-12-19 23:59:20,884 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2c4b603eeefa4a018cb4f4907d8b30be-0.
2025-12-19 23:59:20,885 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cc381d4bd28247b69bc964f96461ee1a-0.
2025-12-19 23:59:20,886 - vllm.core.scheduler - WARNING - Sequence group cmpl-3561870f3fc240a7bbaaa24d7e6516cc-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251
2025-12-19 23:59:20,888 - vllm.core.scheduler - INFO - Pending queue size: (587)
2025-12-19 23:59:20,914 - vllm.entrypoints.logger - INFO - Received request cmpl-47154c8b60f045588530907b5e01c905-0: prompt: 'Doctor, I wanted to talk to you about my recent decision to stop using Facebook regularly.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 4829, 311, 3061, 311, 498, 911, 847, 3213, 5480, 311, 2936, 1667, 5573, 15502, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,934 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-47154c8b60f045588530907b5e01c905-0.
2025-12-19 23:59:20,937 - vllm.core.scheduler - INFO - Pending queue size: (588)
2025-12-19 23:59:20,937 - vllm.entrypoints.logger - INFO - Received request cmpl-78954c6e9a1a45b3a57780ee580daeb7-0: prompt: 'I went for a walk in the forest yesterday to escape from my parents. It always helps me clear my head.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3937, 369, 264, 4227, 304, 279, 13638, 13671, 311, 12449, 504, 847, 6562, 13, 1084, 2677, 8609, 752, 2797, 847, 1968, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,952 - vllm.entrypoints.logger - INFO - Received request cmpl-45dd96e210d84c099cec0228d2eded5c-0: prompt: 'Hey, check out this photo of me and my friends from last night! We had so much fun.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1779, 700, 419, 6548, 315, 752, 323, 847, 4780, 504, 1537, 3729, 0, 1205, 1030, 773, 1753, 2464, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:20,980 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-78954c6e9a1a45b3a57780ee580daeb7-0.
2025-12-19 23:59:20,981 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-45dd96e210d84c099cec0228d2eded5c-0.
2025-12-19 23:59:20,984 - vllm.core.scheduler - INFO - Pending queue size: (590)
2025-12-19 23:59:21,011 - vllm.entrypoints.logger - INFO - Received request cmpl-735bbf337c0e43d5a6887fb3862faa84-0: prompt: 'Hey, can we talk for a minute?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 582, 3061, 369, 264, 9383, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,014 - vllm.entrypoints.logger - INFO - Received request cmpl-27ec814e147b4d7a9719a3b79b4cf63c-0: prompt: "Today, I got into trouble because I didn't think before I acted.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 11, 358, 2684, 1119, 12264, 1576, 358, 3207, 944, 1744, 1573, 358, 30432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,027 - vllm.entrypoints.logger - INFO - Received request cmpl-25b90273d53641adb2064dd5b0d73f87-0: prompt: 'Hey, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,028 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-735bbf337c0e43d5a6887fb3862faa84-0.
2025-12-19 23:59:21,029 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-27ec814e147b4d7a9719a3b79b4cf63c-0.
2025-12-19 23:59:21,030 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-25b90273d53641adb2064dd5b0d73f87-0.
2025-12-19 23:59:21,033 - vllm.core.scheduler - INFO - Pending queue size: (593)
2025-12-19 23:59:21,045 - vllm.entrypoints.logger - INFO - Received request cmpl-2c79f2ff3c79490cb3e134cb35510f12-0: prompt: "I just can't shake this feeling of vulnerability. I'm worried about losing my job, not being able to pay my rent, and being all alone.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 646, 944, 26025, 419, 8266, 315, 33004, 13, 358, 2776, 17811, 911, 13188, 847, 2618, 11, 537, 1660, 2952, 311, 2291, 847, 8016, 11, 323, 1660, 678, 7484, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,048 - vllm.entrypoints.logger - INFO - Received request cmpl-9854e882a0704765bacd4c9eb1cf64d6-0: prompt: "I don't know why people give me weird looks when I wear my mask out in public.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 3170, 1251, 2968, 752, 16283, 5868, 979, 358, 9850, 847, 6911, 700, 304, 584, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,053 - vllm.entrypoints.logger - INFO - Received request cmpl-eb59a315ec63403d9b8f88de5531285b-0: prompt: "Hey Mentor, I'm thinking of buying a new car. But I'm not sure if I can afford it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 2776, 7274, 315, 11833, 264, 501, 1803, 13, 1988, 358, 2776, 537, 2704, 421, 358, 646, 9946, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,062 - vllm.entrypoints.logger - INFO - Received request cmpl-436ca14956c948ab8dff51c5f2f2eb1d-0: prompt: 'You know how much I love taking pictures, right?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 1246, 1753, 358, 2948, 4633, 9185, 11, 1290, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,069 - vllm.entrypoints.logger - INFO - Received request cmpl-d8b3f0fced6a42f2b25518dae21a6ee1-0: prompt: " wants to be happy, so she keeps playing even after you leave. She knows that if she stops, she'll just be sad again. Better yet, if she keeps playing, maybe you'll come back.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6801, 311, 387, 6247, 11, 773, 1340, 13598, 5619, 1496, 1283, 498, 5274, 13, 2932, 8788, 429, 421, 1340, 17933, 11, 1340, 3278, 1101, 387, 12421, 1549, 13, 23434, 3602, 11, 421, 1340, 13598, 5619, 11, 7196, 498, 3278, 2525, 1182, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,071 - vllm.entrypoints.logger - INFO - Received request cmpl-74f30ab8d1204ee8991e9bc1e6e55258-0: prompt: ", I'm really sorry about what I said the other day. I shouldn't have let things escalate like that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 2776, 2167, 14589, 911, 1128, 358, 1053, 279, 1008, 1899, 13, 358, 13133, 944, 614, 1077, 2513, 88590, 1075, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,077 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2c79f2ff3c79490cb3e134cb35510f12-0.
2025-12-19 23:59:21,078 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9854e882a0704765bacd4c9eb1cf64d6-0.
2025-12-19 23:59:21,079 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eb59a315ec63403d9b8f88de5531285b-0.
2025-12-19 23:59:21,080 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-436ca14956c948ab8dff51c5f2f2eb1d-0.
2025-12-19 23:59:21,081 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d8b3f0fced6a42f2b25518dae21a6ee1-0.
2025-12-19 23:59:21,083 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-74f30ab8d1204ee8991e9bc1e6e55258-0.
2025-12-19 23:59:21,085 - vllm.core.scheduler - INFO - Pending queue size: (599)
2025-12-19 23:59:21,091 - vllm.entrypoints.logger - INFO - Received request cmpl-5296a77018054677bb322b9493fc4262-0: prompt: "I can't believe you're still bringing up that incident from two years ago.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=31, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 498, 2299, 2058, 12678, 705, 429, 10455, 504, 1378, 1635, 4134, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,097 - vllm.entrypoints.logger - INFO - Received request cmpl-101d65fe012d4ede8c8e506a4bed3b51-0: prompt: "Hi there, do you need some help? It's raining pretty hard and I have an extra umbrella.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 11, 653, 498, 1184, 1045, 1492, 30, 1084, 594, 83253, 5020, 2588, 323, 358, 614, 458, 4960, 47898, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,104 - vllm.entrypoints.logger - INFO - Received request cmpl-0008322c34324f98b3797e999362b380-0: prompt: 'Hi Mentor, can I tell you about what happened to me earlier today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 646, 358, 3291, 498, 911, 1128, 6932, 311, 752, 6788, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,105 - vllm.entrypoints.logger - INFO - Received request cmpl-5882ac0d617e42a0a441fca7dcb3c4dd-0: prompt: "Hey, do you want to come with me to the shelter downtown? I'm planning to give out some food and clothes to the homeless.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 653, 498, 1366, 311, 2525, 448, 752, 311, 279, 22906, 18907, 30, 358, 2776, 9115, 311, 2968, 700, 1045, 3607, 323, 15097, 311, 279, 22994, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,108 - vllm.entrypoints.logger - INFO - Received request cmpl-bb6762120036449ebd40c1632819d79f-0: prompt: "Hey, it looks like you're interested in that video game system. If you want, I could go inside and ask the price for you.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 432, 5868, 1075, 498, 2299, 8014, 304, 429, 2766, 1809, 1849, 13, 1416, 498, 1366, 11, 358, 1410, 728, 4766, 323, 2548, 279, 3349, 369, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,129 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5296a77018054677bb322b9493fc4262-0.
2025-12-19 23:59:21,130 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-101d65fe012d4ede8c8e506a4bed3b51-0.
2025-12-19 23:59:21,131 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0008322c34324f98b3797e999362b380-0.
2025-12-19 23:59:21,132 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5882ac0d617e42a0a441fca7dcb3c4dd-0.
2025-12-19 23:59:21,133 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bb6762120036449ebd40c1632819d79f-0.
2025-12-19 23:59:21,136 - vllm.core.scheduler - INFO - Pending queue size: (604)
2025-12-19 23:59:21,141 - vllm.entrypoints.logger - INFO - Received request cmpl-eedb99a1956c4030924bb7ca08b7bf44-0: prompt: ", I can't believe I ran into you here.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 358, 646, 944, 4411, 358, 10613, 1119, 498, 1588, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,144 - vllm.entrypoints.logger - INFO - Received request cmpl-66caae8fca6d48ce855a6266aa0d97d8-0: prompt: "Wow, I can't believe the treasures I found in your caves, Classmates B! It was such an exciting adventure.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=33, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 358, 646, 944, 4411, 279, 58849, 358, 1730, 304, 697, 65564, 11, 3228, 16457, 425, 0, 1084, 572, 1741, 458, 13245, 17943, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,166 - vllm.entrypoints.logger - INFO - Received request cmpl-3c707e04a18a4c12a6b169b3aed81ced-0: prompt: 'Hi neighbor, I got a letter today from my best friend Lily.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 9565, 11, 358, 2684, 264, 6524, 3351, 504, 847, 1850, 4238, 47290, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,180 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-eedb99a1956c4030924bb7ca08b7bf44-0.
2025-12-19 23:59:21,181 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-66caae8fca6d48ce855a6266aa0d97d8-0.
2025-12-19 23:59:21,181 - vllm.entrypoints.logger - INFO - Received request cmpl-be8a4d5d9a8446dbbfc250909308921b-0: prompt: "Hi there! How's the birdhouse doing?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 1052, 0, 2585, 594, 279, 11958, 7675, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,182 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3c707e04a18a4c12a6b169b3aed81ced-0.
2025-12-19 23:59:21,183 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-be8a4d5d9a8446dbbfc250909308921b-0.
2025-12-19 23:59:21,186 - vllm.core.scheduler - INFO - Pending queue size: (608)
2025-12-19 23:59:21,189 - vllm.entrypoints.logger - INFO - Received request cmpl-06e9fc89acc74e9d8da853cb102aa816-0: prompt: 'Thanks for spending the day with me, I really enjoyed it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12658, 369, 10164, 279, 1899, 448, 752, 11, 358, 2167, 14006, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,207 - vllm.entrypoints.logger - INFO - Received request cmpl-a97a2e9c2f3b48a2b8186d36fb277f68-0: prompt: 'I was so angry earlier, I could feel my face turning red and my hands shaking.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 773, 18514, 6788, 11, 358, 1410, 2666, 847, 3579, 13054, 2518, 323, 847, 6078, 37739, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,215 - vllm.entrypoints.logger - INFO - Received request cmpl-09ffef812b074c9ba3d5ace6e6fde88d-0: prompt: 'I was walking home from school yesterday when I saw a black car following me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 11435, 2114, 504, 2906, 13671, 979, 358, 5485, 264, 3691, 1803, 2701, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,230 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-06e9fc89acc74e9d8da853cb102aa816-0.
2025-12-19 23:59:21,231 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a97a2e9c2f3b48a2b8186d36fb277f68-0.
2025-12-19 23:59:21,232 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-09ffef812b074c9ba3d5ace6e6fde88d-0.
2025-12-19 23:59:21,235 - vllm.core.scheduler - INFO - Pending queue size: (611)
2025-12-19 23:59:21,242 - vllm.entrypoints.logger - INFO - Received request cmpl-c42608c98aad4bff9c7cc5311839e675-0: prompt: ' Ah, this feels so good. The cool water is just what I needed today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [16366, 11, 419, 11074, 773, 1661, 13, 576, 7010, 3015, 374, 1101, 1128, 358, 4362, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,246 - vllm.entrypoints.logger - INFO - Received request cmpl-a37f902f1fb94565956ec39c2040e2cd-0: prompt: 'Can you believe I just shook hands with Tony Blair?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 358, 1101, 34914, 6078, 448, 18528, 41869, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,256 - vllm.entrypoints.logger - INFO - Received request cmpl-df97cfd26364489da06d9dca01b5ed8d-0: prompt: "Hey B, I wanted to let you know that I really appreciate your services. You've always been so professional and respectful towards me, and I wanted to acknowledge that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 425, 11, 358, 4829, 311, 1077, 498, 1414, 429, 358, 2167, 15401, 697, 3516, 13, 1446, 3003, 2677, 1012, 773, 6584, 323, 48050, 6974, 752, 11, 323, 358, 4829, 311, 24645, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,271 - vllm.entrypoints.logger - INFO - Received request cmpl-51a57c5b1f2c41dc9e32c3109fb3782e-0: prompt: "Mentor, I'm not sure what to do with my day today. I can't decide if I should go out for a walk, read a book, or just watch some TV.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 2776, 537, 2704, 1128, 311, 653, 448, 847, 1899, 3351, 13, 358, 646, 944, 10279, 421, 358, 1265, 728, 700, 369, 264, 4227, 11, 1349, 264, 2311, 11, 476, 1101, 3736, 1045, 5883, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,278 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c42608c98aad4bff9c7cc5311839e675-0.
2025-12-19 23:59:21,280 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a37f902f1fb94565956ec39c2040e2cd-0.
2025-12-19 23:59:21,281 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-df97cfd26364489da06d9dca01b5ed8d-0.
2025-12-19 23:59:21,282 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-51a57c5b1f2c41dc9e32c3109fb3782e-0.
2025-12-19 23:59:21,285 - vllm.core.scheduler - INFO - Pending queue size: (615)
2025-12-19 23:59:21,300 - vllm.entrypoints.logger - INFO - Received request cmpl-2f0296309dac49268a3783a075e920c7-0: prompt: 'I really enjoy being a leader. It feels good to have Neighbors B listen to my ideas and follow my suggestions.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2167, 4669, 1660, 264, 7653, 13, 1084, 11074, 1661, 311, 614, 4182, 24101, 425, 8844, 311, 847, 6708, 323, 1795, 847, 18225, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,305 - vllm.entrypoints.logger - INFO - Received request cmpl-2954dab662364d95b09051e59a301d61-0: prompt: 'Can you believe it? Someone actually cut in front of me in line at the grocery store today. I had to yell at them to get them to apologize.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 34172, 3520, 3931, 304, 4065, 315, 752, 304, 1555, 518, 279, 29587, 3553, 3351, 13, 358, 1030, 311, 64313, 518, 1105, 311, 633, 1105, 311, 36879, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,308 - vllm.entrypoints.logger - INFO - Received request cmpl-f5c74247d6c14a95b9c101ffbcf5b381-0: prompt: 'Hey Mentor, I was wondering if I could cook for you tonight?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=25, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 572, 20293, 421, 358, 1410, 4296, 369, 498, 17913, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,322 - vllm.entrypoints.logger - INFO - Received request cmpl-777c028ac6cd42de952289afab30605f-0: prompt: "I just ordered some General Tso's chicken from the Chinese restaurant down the street. Do you want me to order some for you too, Neighbors B?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=30, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 11457, 1045, 3251, 350, 704, 594, 16158, 504, 279, 8453, 10729, 1495, 279, 8592, 13, 3155, 498, 1366, 752, 311, 1973, 1045, 369, 498, 2238, 11, 4182, 24101, 425, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,329 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2f0296309dac49268a3783a075e920c7-0.
2025-12-19 23:59:21,330 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2954dab662364d95b09051e59a301d61-0.
2025-12-19 23:59:21,330 - vllm.entrypoints.logger - INFO - Received request cmpl-c882edd1338942b5885ef339da8926e9-0: prompt: "Mentor, I feel like I'm always working. Even when I'm tired, I can't seem to stop myself from pushing forward and getting things done.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=37, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 2666, 1075, 358, 2776, 2677, 3238, 13, 7418, 979, 358, 2776, 19227, 11, 358, 646, 944, 2803, 311, 2936, 7037, 504, 17461, 4637, 323, 3709, 2513, 2814, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,331 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f5c74247d6c14a95b9c101ffbcf5b381-0.
2025-12-19 23:59:21,332 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-777c028ac6cd42de952289afab30605f-0.
2025-12-19 23:59:21,333 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c882edd1338942b5885ef339da8926e9-0.
2025-12-19 23:59:21,336 - vllm.core.scheduler - INFO - Pending queue size: (620)
2025-12-19 23:59:21,339 - vllm.entrypoints.logger - INFO - Received request cmpl-da93c95dd55b4ab7bbc4830ca5e15e94-0: prompt: 'Ahh, this feels so good. I love swimming.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [32, 20367, 11, 419, 11074, 773, 1661, 13, 358, 2948, 23380, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,367 - vllm.entrypoints.logger - INFO - Received request cmpl-0b310ca21695408a90dded115f88d504-0: prompt: "I had a great time painting last week. I feel like I'm really starting to improve.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 264, 2244, 882, 18824, 1537, 2003, 13, 358, 2666, 1075, 358, 2776, 2167, 5916, 311, 7269, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,376 - vllm.entrypoints.logger - INFO - Received request cmpl-5e64c0c9278046ebadb02beb2641c810-0: prompt: 'I always have a plan B and C ready, just in case something goes wrong. It has helped me out so many times.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2677, 614, 264, 3119, 425, 323, 356, 5527, 11, 1101, 304, 1142, 2494, 5780, 4969, 13, 1084, 702, 8910, 752, 700, 773, 1657, 3039, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,377 - vllm.entrypoints.logger - INFO - Received request cmpl-27f04bfec2904ce695181d60cdc1ec35-0: prompt: "Sorry, Teacher, I just took the last piece of food. I was really hungry and there wasn't any more left.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=45, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [19152, 11, 29069, 11, 358, 1101, 3867, 279, 1537, 6573, 315, 3607, 13, 358, 572, 2167, 28956, 323, 1052, 5710, 944, 894, 803, 2115, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,380 - vllm.entrypoints.logger - INFO - Received request cmpl-d978c0bb0e074a98bd6dcef1f499a280-0: prompt: 'Mentor, I have been getting compliments from my friends recently about how organized I am.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 1012, 3709, 71639, 504, 847, 4780, 5926, 911, 1246, 16645, 358, 1079, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,380 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-da93c95dd55b4ab7bbc4830ca5e15e94-0.
2025-12-19 23:59:21,381 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0b310ca21695408a90dded115f88d504-0.
2025-12-19 23:59:21,383 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5e64c0c9278046ebadb02beb2641c810-0.
2025-12-19 23:59:21,384 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-27f04bfec2904ce695181d60cdc1ec35-0.
2025-12-19 23:59:21,385 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d978c0bb0e074a98bd6dcef1f499a280-0.
2025-12-19 23:59:21,388 - vllm.core.scheduler - INFO - Pending queue size: (625)
2025-12-19 23:59:21,418 - vllm.entrypoints.logger - INFO - Received request cmpl-d36b3a6a167b409ba91f49313a8c6638-0: prompt: "So what's your favorite band right now, B? You always seem to know the coolest new tunes.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=28, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 1128, 594, 697, 6930, 7055, 1290, 1431, 11, 425, 30, 1446, 2677, 2803, 311, 1414, 279, 73434, 501, 53990, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,431 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d36b3a6a167b409ba91f49313a8c6638-0.
2025-12-19 23:59:21,434 - vllm.core.scheduler - INFO - Pending queue size: (626)
2025-12-19 23:59:21,466 - vllm.entrypoints.logger - INFO - Received request cmpl-734b396b56f54f1395c3ad3f2d60c84f-0: prompt: " I don't know why I'm even here. I don't need help from a counselor.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1513, 944, 1414, 3170, 358, 2776, 1496, 1588, 13, 358, 1513, 944, 1184, 1492, 504, 264, 61375, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,468 - vllm.entrypoints.logger - INFO - Received request cmpl-5e1e73091e324c249ac62190d6a7b385-0: prompt: "Thank you for asking, but I'm going to decline the offer.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13060, 498, 369, 10161, 11, 714, 358, 2776, 2087, 311, 17704, 279, 3010, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,472 - vllm.entrypoints.logger - INFO - Received request cmpl-5b7835f74ad64f10823e1396167bf5d7-0: prompt: "I'm sorry I was late for our lunch date yesterday. I got sidetracked and ended up doing something completely different.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=24, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 14589, 358, 572, 3309, 369, 1039, 15786, 2400, 13671, 13, 358, 2684, 14084, 17366, 11191, 323, 9482, 705, 3730, 2494, 6587, 2155, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,473 - vllm.entrypoints.logger - INFO - Received request cmpl-b7393c049e934010aab6ee2af0109e5a-0: prompt: "I'm so glad you liked the necklace I gave you! ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 15713, 498, 14915, 279, 54447, 358, 6551, 498, 0, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,475 - vllm.entrypoints.logger - INFO - Received request cmpl-4c5a992d47164029b7b3d110376d7af1-0: prompt: 'Hey, Neighbors B, how are you today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 4182, 24101, 425, 11, 1246, 525, 498, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,479 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-734b396b56f54f1395c3ad3f2d60c84f-0.
2025-12-19 23:59:21,481 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5e1e73091e324c249ac62190d6a7b385-0.
2025-12-19 23:59:21,481 - vllm.entrypoints.logger - INFO - Received request cmpl-1eca1cde2ec0408197dd887039972d64-0: prompt: "I still don't understand why you stopped smoking. It's not like it's hurting anyone else.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=39, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2058, 1513, 944, 3535, 3170, 498, 10497, 19578, 13, 1084, 594, 537, 1075, 432, 594, 47289, 5489, 770, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,482 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5b7835f74ad64f10823e1396167bf5d7-0.
2025-12-19 23:59:21,483 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b7393c049e934010aab6ee2af0109e5a-0.
2025-12-19 23:59:21,484 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4c5a992d47164029b7b3d110376d7af1-0.
2025-12-19 23:59:21,485 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1eca1cde2ec0408197dd887039972d64-0.
2025-12-19 23:59:21,488 - vllm.core.scheduler - INFO - Pending queue size: (632)
2025-12-19 23:59:21,499 - vllm.entrypoints.logger - INFO - Received request cmpl-c4ba5469e0e545e8ac4dee9ff94287ea-0: prompt: "I can't believe you did that to me, Neighbors B. You had no right to hurt me like that.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 498, 1521, 429, 311, 752, 11, 4182, 24101, 425, 13, 1446, 1030, 902, 1290, 311, 12898, 752, 1075, 429, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,501 - vllm.entrypoints.logger - INFO - Received request cmpl-aeea1e29c076462684bb654cc8c0e43e-0: prompt: "Whew, finally finished packing. I'm getting excited for my trip!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1639, 365, 11, 5499, 8060, 35713, 13, 358, 2776, 3709, 12035, 369, 847, 8411, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,508 - vllm.entrypoints.logger - INFO - Received request cmpl-79aa557ea9d44fa8a8db5eb737573d60-0: prompt: 'Hey, guess what? I went to the library earlier and learned how to use the Internet! ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 7942, 1128, 30, 358, 3937, 311, 279, 6733, 6788, 323, 9498, 1246, 311, 990, 279, 8031, 0, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,531 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c4ba5469e0e545e8ac4dee9ff94287ea-0.
2025-12-19 23:59:21,533 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-aeea1e29c076462684bb654cc8c0e43e-0.
2025-12-19 23:59:21,534 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-79aa557ea9d44fa8a8db5eb737573d60-0.
2025-12-19 23:59:21,536 - vllm.core.scheduler - INFO - Pending queue size: (635)
2025-12-19 23:59:21,548 - vllm.entrypoints.logger - INFO - Received request cmpl-9e65aa8d40cc451ca2235bd296ecc9b8-0: prompt: 'Neighbors B, I have to admit something to you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=4, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [58086, 425, 11, 358, 614, 311, 16698, 2494, 311, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,552 - vllm.entrypoints.logger - INFO - Received request cmpl-86e5a983ee1a4ec2bfa0400082a6af8e-0: prompt: 'Look at my new flower! I just planted it in the garden.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [10380, 518, 847, 501, 22351, 0, 358, 1101, 38341, 432, 304, 279, 13551, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,580 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9e65aa8d40cc451ca2235bd296ecc9b8-0.
2025-12-19 23:59:21,581 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-86e5a983ee1a4ec2bfa0400082a6af8e-0.
2025-12-19 23:59:21,584 - vllm.core.scheduler - INFO - Pending queue size: (637)
2025-12-19 23:59:21,602 - vllm.entrypoints.logger - INFO - Received request cmpl-04d193df1fec49be928cf7c85bc4b285-0: prompt: "Hey, have you got some time tomorrow? I'm throwing a party for my colleagues and I could use your help.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 2684, 1045, 882, 16577, 30, 358, 2776, 21244, 264, 4614, 369, 847, 17639, 323, 358, 1410, 990, 697, 1492, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,622 - vllm.entrypoints.logger - INFO - Received request cmpl-e27adf2e7c534dfa8d464efaf24f2e1b-0: prompt: "Hey Mentor, I had a bit of an accident today. I was on the roof and I slipped and hurt my leg. It's really throbbing and I think I might need to see a doctor. ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=26, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 1030, 264, 2699, 315, 458, 11423, 3351, 13, 358, 572, 389, 279, 15134, 323, 358, 42478, 323, 12898, 847, 2472, 13, 1084, 594, 2167, 59087, 67341, 323, 358, 1744, 358, 2578, 1184, 311, 1490, 264, 10668, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,628 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-04d193df1fec49be928cf7c85bc4b285-0.
2025-12-19 23:59:21,629 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e27adf2e7c534dfa8d464efaf24f2e1b-0.
2025-12-19 23:59:21,631 - vllm.core.scheduler - INFO - Pending queue size: (639)
2025-12-19 23:59:21,675 - vllm.core.scheduler - INFO - Pending queue size: (639)
2025-12-19 23:59:21,718 - vllm.core.scheduler - INFO - Pending queue size: (639)
2025-12-19 23:59:21,717 - vllm.entrypoints.logger - INFO - Received request cmpl-3af8a8dac60b4f9ba2927c6336667bf7-0: prompt: 'Hey, I need to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 1184, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,738 - vllm.entrypoints.logger - INFO - Received request cmpl-08a8e90191874e999fb2ba746028754b-0: prompt: 'When I was younger, I used to feel so helpless when my friends would have fights.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4498, 358, 572, 14650, 11, 358, 1483, 311, 2666, 773, 62528, 979, 847, 4780, 1035, 614, 27433, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,763 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3af8a8dac60b4f9ba2927c6336667bf7-0.
2025-12-19 23:59:21,764 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-08a8e90191874e999fb2ba746028754b-0.
2025-12-19 23:59:21,767 - vllm.core.scheduler - INFO - Pending queue size: (641)
2025-12-19 23:59:21,781 - vllm.entrypoints.logger - INFO - Received request cmpl-dc4b7cb5313c4d68a7270ac8a7df5b02-0: prompt: "This burger is really good. I'm glad we ordered the same thing.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1986, 44623, 374, 2167, 1661, 13, 358, 2776, 15713, 582, 11457, 279, 1852, 3166, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,794 - vllm.entrypoints.logger - INFO - Received request cmpl-cf69454e8b984c2b89e719d53a722624-0: prompt: "Hey, so I've been thinking about working at the gallery full-time.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 773, 358, 3003, 1012, 7274, 911, 3238, 518, 279, 18046, 2480, 7246, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,797 - vllm.entrypoints.logger - INFO - Received request cmpl-4dfa77e7a91a4a90b3a116ff42d9e9dc-0: prompt: ' I had such a rough day yesterday. Kendall and I got into a huge fight.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1030, 1741, 264, 11165, 1899, 13671, 13, 73076, 323, 358, 2684, 1119, 264, 6765, 4367, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,799 - vllm.entrypoints.logger - INFO - Received request cmpl-82e62102d15d4edfa52330906c9c77e1-0: prompt: 'I saw something really strange earlier today.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 5485, 2494, 2167, 14888, 6788, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,811 - vllm.entrypoints.logger - INFO - Received request cmpl-bfa268dbf8f644158459c9589105fae4-0: prompt: 'Have you read any good books lately?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 1349, 894, 1661, 6467, 30345, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,811 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dc4b7cb5313c4d68a7270ac8a7df5b02-0.
2025-12-19 23:59:21,813 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cf69454e8b984c2b89e719d53a722624-0.
2025-12-19 23:59:21,814 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4dfa77e7a91a4a90b3a116ff42d9e9dc-0.
2025-12-19 23:59:21,815 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-82e62102d15d4edfa52330906c9c77e1-0.
2025-12-19 23:59:21,816 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bfa268dbf8f644158459c9589105fae4-0.
2025-12-19 23:59:21,819 - vllm.core.scheduler - INFO - Pending queue size: (646)
2025-12-19 23:59:21,837 - vllm.entrypoints.logger - INFO - Received request cmpl-a7dcd5eb72d14dd9ab7c61047dbd7c65-0: prompt: 'I made it home safely today, even though it was a bit of a struggle.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1865, 432, 2114, 21000, 3351, 11, 1496, 3498, 432, 572, 264, 2699, 315, 264, 14651, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,845 - vllm.entrypoints.logger - INFO - Received request cmpl-0b34b0f8e6604adc890d33f0a8504837-0: prompt: "I can't wait to go jet skiing today!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 311, 728, 16594, 62017, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,855 - vllm.entrypoints.logger - INFO - Received request cmpl-db4c1b8218d04988b0c78803c6d10a43-0: prompt: "I can't believe how much my confidence has grown!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1246, 1753, 847, 12136, 702, 14700, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,863 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a7dcd5eb72d14dd9ab7c61047dbd7c65-0.
2025-12-19 23:59:21,864 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0b34b0f8e6604adc890d33f0a8504837-0.
2025-12-19 23:59:21,865 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-db4c1b8218d04988b0c78803c6d10a43-0.
2025-12-19 23:59:21,868 - vllm.core.scheduler - INFO - Pending queue size: (649)
2025-12-19 23:59:21,871 - vllm.entrypoints.logger - INFO - Received request cmpl-b2439300ee96401a8c17a59889eb3ac1-0: prompt: "I just heard that my favorite author died. I can't believe it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 6617, 429, 847, 6930, 3150, 8469, 13, 358, 646, 944, 4411, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,873 - vllm.entrypoints.logger - INFO - Received request cmpl-480e6d00e1794798a6fd05df16cbfd59-0: prompt: 'Hey, did I tell you that I had to change trains at Central station today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 429, 358, 1030, 311, 2297, 27688, 518, 10684, 8056, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,876 - vllm.entrypoints.logger - INFO - Received request cmpl-f005dfb4e4464059957f9fef17ec4d72-0: prompt: "Hey, Classmates B, I wanted to talk to you about something that's been on my mind. I've noticed that I've been dressing similar to you lately and even copied your hairstyle. I hope you don't mind.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 3228, 16457, 425, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 429, 594, 1012, 389, 847, 3971, 13, 358, 3003, 13686, 429, 358, 3003, 1012, 31523, 4428, 311, 498, 30345, 323, 1496, 21774, 697, 95135, 13, 358, 3900, 498, 1513, 944, 3971, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,882 - vllm.entrypoints.logger - INFO - Received request cmpl-673f7d0f1ea345429aad9982dd7f5717-0: prompt: "I burned down a church because they didn't accept my lifestyle.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 26626, 1495, 264, 8817, 1576, 807, 3207, 944, 4193, 847, 18899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,912 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b2439300ee96401a8c17a59889eb3ac1-0.
2025-12-19 23:59:21,913 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-480e6d00e1794798a6fd05df16cbfd59-0.
2025-12-19 23:59:21,914 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f005dfb4e4464059957f9fef17ec4d72-0.
2025-12-19 23:59:21,916 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-673f7d0f1ea345429aad9982dd7f5717-0.
2025-12-19 23:59:21,916 - vllm.entrypoints.logger - INFO - Received request cmpl-416c0f93eeb4432e949d83d469a825cd-0: prompt: "Hey, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,918 - vllm.core.scheduler - INFO - Pending queue size: (653)
2025-12-19 23:59:21,935 - vllm.entrypoints.logger - INFO - Received request cmpl-cd6e009d885344c3b48950432c1a63f8-0: prompt: 'Doctor, I have something to share with you. I met a woman at a bar and we went home together. I felt happy and content as I fell asleep next to her.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 614, 2494, 311, 4332, 448, 498, 13, 358, 2270, 264, 5220, 518, 264, 3619, 323, 582, 3937, 2114, 3786, 13, 358, 6476, 6247, 323, 2213, 438, 358, 11052, 32073, 1790, 311, 1059, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,941 - vllm.entrypoints.logger - INFO - Received request cmpl-f04db1c254f242a9960a15fa63ac50dc-0: prompt: 'When I first met you, I was determined to prove that I was better than you.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4498, 358, 1156, 2270, 498, 11, 358, 572, 10838, 311, 12118, 429, 358, 572, 2664, 1091, 498, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,949 - vllm.entrypoints.logger - INFO - Received request cmpl-22fb4f0de5b54cfba3f79b2395afdf8c-0: prompt: 'I just wanted to say how grateful I am for the work you do,. It always brings a smile to my face to see you so happy when we finish a project together.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 4829, 311, 1977, 1246, 25195, 358, 1079, 369, 279, 975, 498, 653, 17515, 1084, 2677, 12434, 264, 15289, 311, 847, 3579, 311, 1490, 498, 773, 6247, 979, 582, 6248, 264, 2390, 3786, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,953 - vllm.entrypoints.logger - INFO - Received request cmpl-b7b92ad89f304e92bca19fbcaad43643-0: prompt: 'Hi Doctor, how are you today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 18635, 11, 1246, 525, 498, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,960 - vllm.entrypoints.logger - INFO - Received request cmpl-a3a3f0bb5b89479b963a8016ca1cf103-0: prompt: "Hey there, Co-workers B! I just wanted to let you know how grateful I am for the work you do. It's great seeing you so happy after we finish a project together.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 1052, 11, 3539, 62284, 425, 0, 358, 1101, 4829, 311, 1077, 498, 1414, 1246, 25195, 358, 1079, 369, 279, 975, 498, 653, 13, 1084, 594, 2244, 9120, 498, 773, 6247, 1283, 582, 6248, 264, 2390, 3786, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,962 - vllm.entrypoints.logger - INFO - Received request cmpl-3d3e97d93ea34cbe82a2b1a56bd8727d-0: prompt: 'Hey, I wanted to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,963 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-416c0f93eeb4432e949d83d469a825cd-0.
2025-12-19 23:59:21,964 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cd6e009d885344c3b48950432c1a63f8-0.
2025-12-19 23:59:21,965 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f04db1c254f242a9960a15fa63ac50dc-0.
2025-12-19 23:59:21,966 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-22fb4f0de5b54cfba3f79b2395afdf8c-0.
2025-12-19 23:59:21,967 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b7b92ad89f304e92bca19fbcaad43643-0.
2025-12-19 23:59:21,968 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a3a3f0bb5b89479b963a8016ca1cf103-0.
2025-12-19 23:59:21,969 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3d3e97d93ea34cbe82a2b1a56bd8727d-0.
2025-12-19 23:59:21,972 - vllm.core.scheduler - INFO - Pending queue size: (660)
2025-12-19 23:59:21,974 - vllm.entrypoints.logger - INFO - Received request cmpl-9cb2b73243904f0dbe3bc6cd4905aa1f-0: prompt: 'Hi B, how are you doing?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 425, 11, 1246, 525, 498, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:21,982 - vllm.entrypoints.logger - INFO - Received request cmpl-c7f32ed7ee994611b38ea468ef60833a-0: prompt: 'Hey, have you had a chance to look at the new program I wrote?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 1030, 264, 6012, 311, 1401, 518, 279, 501, 2025, 358, 6139, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,000 - vllm.entrypoints.logger - INFO - Received request cmpl-94099df264d7458284ee955f2783dddf-0: prompt: "It's really great to be here with you today, Neighbors B. How are you doing?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 2167, 2244, 311, 387, 1588, 448, 498, 3351, 11, 4182, 24101, 425, 13, 2585, 525, 498, 3730, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,002 - vllm.entrypoints.logger - INFO - Received request cmpl-8cfcad9afdc84b98948b83a36c72ec0c-0: prompt: 'Did you notice that one of the students left the classroom early today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6986, 498, 5293, 429, 825, 315, 279, 4143, 2115, 279, 24017, 4124, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,015 - vllm.entrypoints.logger - INFO - Received request cmpl-a3aab30cb11945e08616fddef75661d1-0: prompt: 'Can you believe it? I met a celebrity and shook their hand!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [6713, 498, 4411, 432, 30, 358, 2270, 264, 30444, 323, 34914, 862, 1424, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,015 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9cb2b73243904f0dbe3bc6cd4905aa1f-0.
2025-12-19 23:59:22,016 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c7f32ed7ee994611b38ea468ef60833a-0.
2025-12-19 23:59:22,017 - vllm.entrypoints.logger - INFO - Received request cmpl-b9f48fdae44a42af84a614ed77c1657f-0: prompt: 'I was so happy when I moved back to my old apartment. The new one was just too cramped for me.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 572, 773, 6247, 979, 358, 7726, 1182, 311, 847, 2310, 13154, 13, 576, 501, 825, 572, 1101, 2238, 98732, 369, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,018 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-94099df264d7458284ee955f2783dddf-0.
2025-12-19 23:59:22,019 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8cfcad9afdc84b98948b83a36c72ec0c-0.
2025-12-19 23:59:22,020 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a3aab30cb11945e08616fddef75661d1-0.
2025-12-19 23:59:22,021 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b9f48fdae44a42af84a614ed77c1657f-0.
2025-12-19 23:59:22,023 - vllm.core.scheduler - INFO - Pending queue size: (666)
2025-12-19 23:59:22,029 - vllm.entrypoints.logger - INFO - Received request cmpl-23836d787fea4660888834f5d4e04378-0: prompt: 'Hey, I bought a new muffler for my car!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 10788, 264, 501, 54304, 1536, 369, 847, 1803, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,032 - vllm.entrypoints.logger - INFO - Received request cmpl-b604e26b449f4c89ad50af71fcef9802-0: prompt: "I'm loving this dress I'm wearing today. It's so flowy and comfortable.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 21258, 419, 8511, 358, 2776, 12233, 3351, 13, 1084, 594, 773, 6396, 88, 323, 10655, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,045 - vllm.entrypoints.logger - INFO - Received request cmpl-73476c2eee9d487c9cca0414448f16da-0: prompt: 'You know, sometimes I wonder if being patient is a good thing or a bad thing.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 7025, 358, 5775, 421, 1660, 8720, 374, 264, 1661, 3166, 476, 264, 3873, 3166, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,047 - vllm.entrypoints.logger - INFO - Received request cmpl-004508d048da4a25b1db96ccf19cc17d-0: prompt: " Doctor, I wanted to update you on something. I've been attending anger management classes and I'm starting to feel like I have a better handle on controlling my rage.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=20, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18635, 11, 358, 4829, 311, 2647, 498, 389, 2494, 13, 358, 3003, 1012, 23218, 19234, 6240, 6846, 323, 358, 2776, 5916, 311, 2666, 1075, 358, 614, 264, 2664, 3705, 389, 25902, 847, 32949, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,067 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-23836d787fea4660888834f5d4e04378-0.
2025-12-19 23:59:22,068 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b604e26b449f4c89ad50af71fcef9802-0.
2025-12-19 23:59:22,069 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-73476c2eee9d487c9cca0414448f16da-0.
2025-12-19 23:59:22,070 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-004508d048da4a25b1db96ccf19cc17d-0.
2025-12-19 23:59:22,073 - vllm.core.scheduler - INFO - Pending queue size: (670)
2025-12-19 23:59:22,085 - vllm.entrypoints.logger - INFO - Received request cmpl-5e3bd0af43904bcbb147e6cac46bc66c-0: prompt: "Whew, it's really hot today. I think I need to take off my hat to cool down.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1639, 365, 11, 432, 594, 2167, 4017, 3351, 13, 358, 1744, 358, 1184, 311, 1896, 1007, 847, 8896, 311, 7010, 1495, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,114 - vllm.entrypoints.logger - INFO - Received request cmpl-720b55f6e7154521b4cb01f8098d3745-0: prompt: "Wow, I'm so relieved I made it home safely today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [35881, 11, 358, 2776, 773, 50412, 358, 1865, 432, 2114, 21000, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,116 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5e3bd0af43904bcbb147e6cac46bc66c-0.
2025-12-19 23:59:22,117 - vllm.entrypoints.logger - INFO - Received request cmpl-8b2ed910af6c48a98737b60f477f22cc-0: prompt: "I can't believe I lost that game! I'm usually so good at it.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 358, 5558, 429, 1809, 0, 358, 2776, 5990, 773, 1661, 518, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,118 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-720b55f6e7154521b4cb01f8098d3745-0.
2025-12-19 23:59:22,119 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8b2ed910af6c48a98737b60f477f22cc-0.
2025-12-19 23:59:22,121 - vllm.core.scheduler - INFO - Pending queue size: (673)
2025-12-19 23:59:22,129 - vllm.entrypoints.logger - INFO - Received request cmpl-afab4ceb177f438082b6f2aa796326cc-0: prompt: 'Hey, did I tell you about what happened to me on the way to the store yesterday?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 358, 3291, 498, 911, 1128, 6932, 311, 752, 389, 279, 1616, 311, 279, 3553, 13671, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,138 - vllm.entrypoints.logger - INFO - Received request cmpl-6680340543b144168b6f9053adf9bab8-0: prompt: 'Hello, Boss.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [9707, 11, 31569, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,165 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-afab4ceb177f438082b6f2aa796326cc-0.
2025-12-19 23:59:22,167 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6680340543b144168b6f9053adf9bab8-0.
2025-12-19 23:59:22,169 - vllm.core.scheduler - INFO - Pending queue size: (675)
2025-12-19 23:59:22,180 - vllm.entrypoints.logger - INFO - Received request cmpl-a263c00c15014397bb1a041829eea652-0: prompt: 'I feel really bad about what I said about not liking kids.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 2167, 3873, 911, 1128, 358, 1053, 911, 537, 48737, 6837, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,184 - vllm.entrypoints.logger - INFO - Received request cmpl-40567f704e9c487ab5e5af029124cb7f-0: prompt: 'I am so happy that we have a new puppy in the family.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1079, 773, 6247, 429, 582, 614, 264, 501, 41189, 304, 279, 2997, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,187 - vllm.entrypoints.logger - INFO - Received request cmpl-69d3e83f8b1f43c9996ab96c04ab5abb-0: prompt: 'Today was not a good day for me. I was in a bad mood and snapped at my friend.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 572, 537, 264, 1661, 1899, 369, 752, 13, 358, 572, 304, 264, 3873, 19671, 323, 47010, 518, 847, 4238, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,203 - vllm.entrypoints.logger - INFO - Received request cmpl-9a8711c05afa4d96ac591587b5397f37-0: prompt: "Hey Boss, what's up? ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 31569, 11, 1128, 594, 705, 30, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,213 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a263c00c15014397bb1a041829eea652-0.
2025-12-19 23:59:22,214 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-40567f704e9c487ab5e5af029124cb7f-0.
2025-12-19 23:59:22,216 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-69d3e83f8b1f43c9996ab96c04ab5abb-0.
2025-12-19 23:59:22,217 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9a8711c05afa4d96ac591587b5397f37-0.
2025-12-19 23:59:22,219 - vllm.core.scheduler - INFO - Pending queue size: (679)
2025-12-19 23:59:22,230 - vllm.entrypoints.logger - INFO - Received request cmpl-cfeb2c6f59ee45dba542dda5a53fa533-0: prompt: 'Hey, how have you been?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 614, 498, 1012, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,254 - vllm.entrypoints.logger - INFO - Received request cmpl-d336197af4b6450daf950f8f1ca3412a-0: prompt: 'Oh man, that was such a painful fall. My back is still hurting.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 883, 11, 429, 572, 1741, 264, 25118, 4399, 13, 3017, 1182, 374, 2058, 47289, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,265 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cfeb2c6f59ee45dba542dda5a53fa533-0.
2025-12-19 23:59:22,266 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d336197af4b6450daf950f8f1ca3412a-0.
2025-12-19 23:59:22,269 - vllm.core.scheduler - INFO - Pending queue size: (681)
2025-12-19 23:59:22,292 - vllm.entrypoints.logger - INFO - Received request cmpl-2092383ab76a46e8b4b3dbc3ea915a64-0: prompt: "Hey, what's up?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1128, 594, 705, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,294 - vllm.entrypoints.logger - INFO - Received request cmpl-68c699f8c6a6435587b9d1e39005207f-0: prompt: 'Good morning, Mentor. I woke up early today to get a head start on my work.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15216, 6556, 11, 91191, 13, 358, 38726, 705, 4124, 3351, 311, 633, 264, 1968, 1191, 389, 847, 975, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,297 - vllm.entrypoints.logger - INFO - Received request cmpl-31bf8c52e1574d55af3ec34515193e0d-0: prompt: 'Hey Neighbor B, I was finally able to buy all the supplies I needed for my project.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 97163, 425, 11, 358, 572, 5499, 2952, 311, 3695, 678, 279, 16720, 358, 4362, 369, 847, 2390, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,298 - vllm.entrypoints.logger - INFO - Received request cmpl-2ee0c459fe1947c48e08b605c1421208-0: prompt: " I don't know what to do. My grades are slipping so much because I keep cutting class. It's causing me a lot of stress.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 1513, 944, 1414, 1128, 311, 653, 13, 3017, 27611, 525, 63702, 773, 1753, 1576, 358, 2506, 14376, 536, 13, 1084, 594, 14381, 752, 264, 2696, 315, 8464, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,303 - vllm.entrypoints.logger - INFO - Received request cmpl-e94a7f41874c436b81c3d4950f6fb668-0: prompt: " This door won't open.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1096, 6006, 2765, 944, 1787, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,311 - vllm.entrypoints.logger - INFO - Received request cmpl-4f73332e94f64874bbb6ccd67be1b838-0: prompt: "Hey, how's it going?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 594, 432, 2087, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,313 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2092383ab76a46e8b4b3dbc3ea915a64-0.
2025-12-19 23:59:22,314 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-68c699f8c6a6435587b9d1e39005207f-0.
2025-12-19 23:59:22,315 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-31bf8c52e1574d55af3ec34515193e0d-0.
2025-12-19 23:59:22,316 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2ee0c459fe1947c48e08b605c1421208-0.
2025-12-19 23:59:22,317 - vllm.entrypoints.logger - INFO - Received request cmpl-3e3fdb6b5f544a79beb2197fdf5b9a43-0: prompt: "Mentor, I've been feeling really upset lately. I got reprimanded at work because I didn't listen to reason and went ahead with my own idea.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 3003, 1012, 8266, 2167, 22459, 30345, 13, 358, 2684, 312, 17893, 6465, 518, 975, 1576, 358, 3207, 944, 8844, 311, 2874, 323, 3937, 8305, 448, 847, 1828, 4522, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,317 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-e94a7f41874c436b81c3d4950f6fb668-0.
2025-12-19 23:59:22,318 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4f73332e94f64874bbb6ccd67be1b838-0.
2025-12-19 23:59:22,319 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3e3fdb6b5f544a79beb2197fdf5b9a43-0.
2025-12-19 23:59:22,322 - vllm.core.scheduler - INFO - Pending queue size: (688)
2025-12-19 23:59:22,328 - vllm.entrypoints.logger - INFO - Received request cmpl-61c9cb35926445a29ddaab7d6fef64df-0: prompt: 'Hi, I found your car at the mechanic. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 11, 358, 1730, 697, 1803, 518, 279, 44139, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,331 - vllm.entrypoints.logger - INFO - Received request cmpl-2fb93759a90b4b46afb5b9105018f0b6-0: prompt: "Hey, have you noticed how much more responsible I've become since being elected as class president?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 13686, 1246, 1753, 803, 8480, 358, 3003, 3635, 2474, 1660, 16290, 438, 536, 4767, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,366 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-61c9cb35926445a29ddaab7d6fef64df-0.
2025-12-19 23:59:22,367 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2fb93759a90b4b46afb5b9105018f0b6-0.
2025-12-19 23:59:22,368 - vllm.entrypoints.logger - INFO - Received request cmpl-5543b16f48d041bc9473615fe2fa0d63-0: prompt: 'You know, I was just thinking about something. Forgiveness is such a powerful thing.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2610, 1414, 11, 358, 572, 1101, 7274, 911, 2494, 13, 69575, 12781, 374, 1741, 264, 7988, 3166, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,369 - vllm.core.scheduler - INFO - Pending queue size: (690)
2025-12-19 23:59:22,402 - vllm.entrypoints.logger - INFO - Received request cmpl-307743fba4154fcea830b4ea08d9306c-0: prompt: "I can't believe what I did at that party last night. I got so drunk and stupidly drove myself home.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1128, 358, 1521, 518, 429, 4614, 1537, 3729, 13, 358, 2684, 773, 28750, 323, 18253, 398, 23108, 7037, 2114, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,412 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-5543b16f48d041bc9473615fe2fa0d63-0.
2025-12-19 23:59:22,413 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-307743fba4154fcea830b4ea08d9306c-0.
2025-12-19 23:59:22,416 - vllm.core.scheduler - INFO - Pending queue size: (692)
2025-12-19 23:59:22,435 - vllm.entrypoints.logger - INFO - Received request cmpl-0b0ec53396dc4c668a5e0a447feee5cb-0: prompt: "Today I helped grandma with her grocery shopping. She can't really leave the house these days.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [15364, 358, 8910, 82677, 448, 1059, 29587, 11919, 13, 2932, 646, 944, 2167, 5274, 279, 3753, 1493, 2849, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,439 - vllm.entrypoints.logger - INFO - Received request cmpl-59d952319e4548a3837bc7824d78ad42-0: prompt: "I don't think I can do it. I can't leave California just yet.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1744, 358, 646, 653, 432, 13, 358, 646, 944, 5274, 7043, 1101, 3602, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,447 - vllm.entrypoints.logger - INFO - Received request cmpl-0444aba209f94a52ac4044261a0fb220-0: prompt: "I don't want to talk to anyone right now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1366, 311, 3061, 311, 5489, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,451 - vllm.entrypoints.logger - INFO - Received request cmpl-50849c5d12674c479d6e0c32e5508b87-0: prompt: "Hey Mentor, I'm feeling restless and I want to go outside and throw some rocks. Do you want to come with me?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 91191, 11, 358, 2776, 8266, 88258, 323, 358, 1366, 311, 728, 4889, 323, 2510, 1045, 23035, 13, 3155, 498, 1366, 311, 2525, 448, 752, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,461 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0b0ec53396dc4c668a5e0a447feee5cb-0.
2025-12-19 23:59:22,462 - vllm.entrypoints.logger - INFO - Received request cmpl-6d540e29f8db4e9486975242ee16a1e0-0: prompt: "I just don't like it when people try to tell me what to do. I like to be independent and make my own decisions.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=19, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1101, 1513, 944, 1075, 432, 979, 1251, 1430, 311, 3291, 752, 1128, 311, 653, 13, 358, 1075, 311, 387, 9489, 323, 1281, 847, 1828, 11181, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,462 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-59d952319e4548a3837bc7824d78ad42-0.
2025-12-19 23:59:22,464 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0444aba209f94a52ac4044261a0fb220-0.
2025-12-19 23:59:22,465 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-50849c5d12674c479d6e0c32e5508b87-0.
2025-12-19 23:59:22,466 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6d540e29f8db4e9486975242ee16a1e0-0.
2025-12-19 23:59:22,468 - vllm.core.scheduler - INFO - Pending queue size: (697)
2025-12-19 23:59:22,501 - vllm.entrypoints.logger - INFO - Received request cmpl-c2615dd2c91c4824b330fb4066d6b4f7-0: prompt: 'I finally feel motivated to write again.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 5499, 2666, 26664, 311, 3270, 1549, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,504 - vllm.entrypoints.logger - INFO - Received request cmpl-71b48f6888be493aa0c2c008b0035bbb-0: prompt: "I'm sorry, Teacher. I feel so upset about what happened.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 14589, 11, 29069, 13, 358, 2666, 773, 22459, 911, 1128, 6932, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,512 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-c2615dd2c91c4824b330fb4066d6b4f7-0.
2025-12-19 23:59:22,513 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-71b48f6888be493aa0c2c008b0035bbb-0.
2025-12-19 23:59:22,515 - vllm.core.scheduler - INFO - Pending queue size: (699)
2025-12-19 23:59:22,517 - vllm.entrypoints.logger - INFO - Received request cmpl-76f730eb50d24598a9ce7988721f6f7c-0: prompt: "I'm glad I was able to get you out of that burning building. Are you doing okay now?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 15713, 358, 572, 2952, 311, 633, 498, 700, 315, 429, 19675, 4752, 13, 8713, 498, 3730, 16910, 1431, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,534 - vllm.entrypoints.logger - INFO - Received request cmpl-b8014def6246416c9e552bb0c6162016-0: prompt: "I'm really enjoying this job. It feels like a perfect fit for me, you know?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 21413, 419, 2618, 13, 1084, 11074, 1075, 264, 4727, 4946, 369, 752, 11, 498, 1414, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,557 - vllm.entrypoints.logger - INFO - Received request cmpl-693d055f8e5d4ee4b3786eac48954138-0: prompt: ', can I talk to you for a second?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11, 646, 358, 3061, 311, 498, 369, 264, 2086, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,559 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-76f730eb50d24598a9ce7988721f6f7c-0.
2025-12-19 23:59:22,560 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b8014def6246416c9e552bb0c6162016-0.
2025-12-19 23:59:22,562 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-693d055f8e5d4ee4b3786eac48954138-0.
2025-12-19 23:59:22,564 - vllm.core.scheduler - INFO - Pending queue size: (702)
2025-12-19 23:59:22,601 - vllm.entrypoints.logger - INFO - Received request cmpl-fb50c9162bd84aee9ca3a125f64643f9-0: prompt: "Ah, it's good to be back home after a relaxing week at the beach.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [24765, 11, 432, 594, 1661, 311, 387, 1182, 2114, 1283, 264, 33848, 2003, 518, 279, 11321, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,607 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fb50c9162bd84aee9ca3a125f64643f9-0.
2025-12-19 23:59:22,609 - vllm.entrypoints.logger - INFO - Received request cmpl-07cd24b9dc9c446f9e1830f69cebffe0-0: prompt: 'Guess what, I took a course at the local community college.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [45730, 1128, 11, 358, 3867, 264, 3308, 518, 279, 2205, 3942, 7770, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,610 - vllm.core.scheduler - INFO - Pending queue size: (703)
2025-12-19 23:59:22,635 - vllm.entrypoints.logger - INFO - Received request cmpl-87c57048c46742e7b4ea3ae5b7297063-0: prompt: 'Hey, did you hear that cute little dog barking last night?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 429, 18838, 2632, 5562, 293, 33452, 1537, 3729, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,638 - vllm.entrypoints.logger - INFO - Received request cmpl-47f5bc6b12db4186b6ad54385ab3d61f-0: prompt: 'Hey, what are you working on over here?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1128, 525, 498, 3238, 389, 916, 1588, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,649 - vllm.entrypoints.logger - INFO - Received request cmpl-8b442b5afc1240d480fccada4477cb24-0: prompt: ' It was amazing to see all those famous landmarks on my road trip.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [1084, 572, 7897, 311, 1490, 678, 1846, 11245, 59924, 389, 847, 5636, 8411, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,654 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-07cd24b9dc9c446f9e1830f69cebffe0-0.
2025-12-19 23:59:22,655 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-87c57048c46742e7b4ea3ae5b7297063-0.
2025-12-19 23:59:22,656 - vllm.entrypoints.logger - INFO - Received request cmpl-6b2bd055c51b48ec81e6080bd9444071-0: prompt: 'Have you ever seen a hummingbird up close? I saw one yesterday and it was absolutely amazing!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [12116, 498, 3512, 3884, 264, 86327, 22592, 705, 3265, 30, 358, 5485, 825, 13671, 323, 432, 572, 10875, 7897, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,656 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-47f5bc6b12db4186b6ad54385ab3d61f-0.
2025-12-19 23:59:22,657 - vllm.entrypoints.logger - INFO - Received request cmpl-0b3a182769e84e09a86823afacc2c6c3-0: prompt: 'Hey Neighbor B, check out this cool plant I found in my backyard!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 97163, 425, 11, 1779, 700, 419, 7010, 6008, 358, 1730, 304, 847, 35660, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,657 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8b442b5afc1240d480fccada4477cb24-0.
2025-12-19 23:59:22,659 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6b2bd055c51b48ec81e6080bd9444071-0.
2025-12-19 23:59:22,660 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0b3a182769e84e09a86823afacc2c6c3-0.
2025-12-19 23:59:22,662 - vllm.core.scheduler - INFO - Pending queue size: (709)
2025-12-19 23:59:22,664 - vllm.entrypoints.logger - INFO - Received request cmpl-09e0057f07634e3791d3bb976537422a-0: prompt: "I can't believe our tomato plants are already producing fruit. It feels so rewarding to grow our own food.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 1039, 41020, 10779, 525, 2669, 17387, 13779, 13, 1084, 11074, 773, 40993, 311, 3063, 1039, 1828, 3607, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,674 - vllm.entrypoints.logger - INFO - Received request cmpl-cf4ea3b824324afa9ba3c6b0c3f15c73-0: prompt: "I am so excited to wear my green sneakers today! They're my favorite pair of shoes.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1079, 773, 12035, 311, 9850, 847, 6176, 67191, 3351, 0, 2379, 2299, 847, 6930, 6716, 315, 15294, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,675 - vllm.entrypoints.logger - INFO - Received request cmpl-11a09453c5804c5fb7953a07c0096f37-0: prompt: "I don't want to talk about it anymore. You're just being irrational.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1366, 311, 3061, 911, 432, 14584, 13, 1446, 2299, 1101, 1660, 60654, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,684 - vllm.entrypoints.logger - INFO - Received request cmpl-dc02c1a3a9c443debf9fa4869b0355d5-0: prompt: "Hey, did you hear the news? I'm getting my own cooking show!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 6723, 279, 3669, 30, 358, 2776, 3709, 847, 1828, 17233, 1473, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,686 - vllm.entrypoints.logger - INFO - Received request cmpl-f0205bf30a9346f9a8cee72f764e969b-0: prompt: "I think it's important to always show respect to others, especially our elders. That's how I was raised.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1744, 432, 594, 2989, 311, 2677, 1473, 5091, 311, 3800, 11, 5310, 1039, 60676, 13, 2938, 594, 1246, 358, 572, 9226, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,694 - vllm.entrypoints.logger - INFO - Received request cmpl-72276d4d6db44a7782c20d242ef4d046-0: prompt: 'Hey, I have some great news to share with you today!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 614, 1045, 2244, 3669, 311, 4332, 448, 498, 3351, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,702 - vllm.entrypoints.logger - INFO - Received request cmpl-332f4e477e2946d5b7cb188cfadb35cc-0: prompt: 'Hey, I needed to talk to you about something. I recently bought some drugs for personY.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4362, 311, 3061, 311, 498, 911, 2494, 13, 358, 5926, 10788, 1045, 10975, 369, 1697, 56, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,706 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-09e0057f07634e3791d3bb976537422a-0.
2025-12-19 23:59:22,707 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cf4ea3b824324afa9ba3c6b0c3f15c73-0.
2025-12-19 23:59:22,708 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-11a09453c5804c5fb7953a07c0096f37-0.
2025-12-19 23:59:22,709 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-dc02c1a3a9c443debf9fa4869b0355d5-0.
2025-12-19 23:59:22,710 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f0205bf30a9346f9a8cee72f764e969b-0.
2025-12-19 23:59:22,711 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-72276d4d6db44a7782c20d242ef4d046-0.
2025-12-19 23:59:22,712 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-332f4e477e2946d5b7cb188cfadb35cc-0.
2025-12-19 23:59:22,715 - vllm.core.scheduler - INFO - Pending queue size: (716)
2025-12-19 23:59:22,718 - vllm.entrypoints.logger - INFO - Received request cmpl-b786fa55b7fc4bc299168f2612901d73-0: prompt: "Teacher, I raised the flag today to celebrate our country's independence! ", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [45065, 11, 358, 9226, 279, 5181, 3351, 311, 18383, 1039, 3146, 594, 23665, 0, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,721 - vllm.entrypoints.logger - INFO - Received request cmpl-1bd7d2b2154446c18346c94b0c51eb8d-0: prompt: 'Hey, neighbor B! How are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 9565, 425, 0, 2585, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,724 - vllm.entrypoints.logger - INFO - Received request cmpl-d0362998ff3a41da88be6192bf2557fd-0: prompt: "I can't wait for the party tonight! I have my whole outfit planned out and everything.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 369, 279, 4614, 17913, 0, 358, 614, 847, 4361, 27303, 12909, 700, 323, 4297, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,759 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b786fa55b7fc4bc299168f2612901d73-0.
2025-12-19 23:59:22,765 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1bd7d2b2154446c18346c94b0c51eb8d-0.
2025-12-19 23:59:22,737 - vllm.entrypoints.logger - INFO - Received request cmpl-1ce6fd7bcb9646b4a1faa6252b6c8475-0: prompt: "I'm feeling really good today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 8266, 2167, 1661, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,766 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d0362998ff3a41da88be6192bf2557fd-0.
2025-12-19 23:59:22,769 - vllm.core.scheduler - INFO - Pending queue size: (719)
2025-12-19 23:59:22,769 - vllm.entrypoints.logger - INFO - Received request cmpl-499adcaa2b0f499691d2273fc946d8bd-0: prompt: "I'm really excited because I've decided to travel the world.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 2167, 12035, 1576, 358, 3003, 6635, 311, 5821, 279, 1879, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,771 - vllm.entrypoints.logger - INFO - Received request cmpl-0f63e8c0ef544fa2976dab4fd1024f87-0: prompt: "I can't wait to propose to her. I found the perfect ring!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 3783, 311, 29614, 311, 1059, 13, 358, 1730, 279, 4727, 10058, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,773 - vllm.entrypoints.logger - INFO - Received request cmpl-066f7ed30ea84eaaa5a41c004e20596b-0: prompt: "Ugh, I don't want to get out of bed.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [52, 866, 11, 358, 1513, 944, 1366, 311, 633, 700, 315, 4845, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,774 - vllm.entrypoints.logger - INFO - Received request cmpl-2ddf089872834d4fa110d85f12bd378a-0: prompt: 'Hi Mentor, how are you doing today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 1246, 525, 498, 3730, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,776 - vllm.entrypoints.logger - INFO - Received request cmpl-a26ffb1f4d62423caed2a931bed2eb09-0: prompt: 'I had so much fun at the party last night! I danced until I was sweating.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1030, 773, 1753, 2464, 518, 279, 4614, 1537, 3729, 0, 358, 83752, 3080, 358, 572, 80266, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,777 - vllm.entrypoints.logger - INFO - Received request cmpl-7342fbacad6d4ffd880bf4ec6fd828c0-0: prompt: "I can't believe it! They actually used the piece of evidence I found to solve the case.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 646, 944, 4411, 432, 0, 2379, 3520, 1483, 279, 6573, 315, 5904, 358, 1730, 311, 11625, 279, 1142, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,779 - vllm.entrypoints.logger - INFO - Received request cmpl-23217297ab5144d8a16fe1c0170fa97e-0: prompt: 'I am so sorry for what I did in the hallway earlier.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1079, 773, 14589, 369, 1128, 358, 1521, 304, 279, 50802, 6788, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,781 - vllm.entrypoints.logger - INFO - Received request cmpl-ea54a0770cf14d828645a7e7925029f4-0: prompt: "It's such a beautiful day today, perfect for gardening.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=18, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [2132, 594, 1741, 264, 6233, 1899, 3351, 11, 4727, 369, 59199, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,791 - vllm.entrypoints.logger - INFO - Received request cmpl-2d065f8172a64ad6bb6f4c91558a4ce3-0: prompt: "I have to say, I'm really happy with how my business is doing. It's great to be in control of my own life and have the independence that comes with owning my own company.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=23, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 614, 311, 1977, 11, 358, 2776, 2167, 6247, 448, 1246, 847, 2562, 374, 3730, 13, 1084, 594, 2244, 311, 387, 304, 2524, 315, 847, 1828, 2272, 323, 614, 279, 23665, 429, 4041, 448, 40277, 847, 1828, 2813, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,812 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-1ce6fd7bcb9646b4a1faa6252b6c8475-0.
2025-12-19 23:59:22,813 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-499adcaa2b0f499691d2273fc946d8bd-0.
2025-12-19 23:59:22,814 - vllm.entrypoints.logger - INFO - Received request cmpl-ae8aa48bf8b74d7babf5b9cba4a951ff-0: prompt: 'Doctor, I did something amazing yesterday. I sang in front of a crowd at the park and people actually clapped and cheered for me!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=15, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [42724, 11, 358, 1521, 2494, 7897, 13671, 13, 358, 28240, 304, 4065, 315, 264, 13428, 518, 279, 6118, 323, 1251, 3520, 1185, 5677, 323, 84523, 369, 752, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,815 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-0f63e8c0ef544fa2976dab4fd1024f87-0.
2025-12-19 23:59:22,816 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-066f7ed30ea84eaaa5a41c004e20596b-0.
2025-12-19 23:59:22,817 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2ddf089872834d4fa110d85f12bd378a-0.
2025-12-19 23:59:22,818 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-a26ffb1f4d62423caed2a931bed2eb09-0.
2025-12-19 23:59:22,819 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7342fbacad6d4ffd880bf4ec6fd828c0-0.
2025-12-19 23:59:22,820 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-23217297ab5144d8a16fe1c0170fa97e-0.
2025-12-19 23:59:22,821 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ea54a0770cf14d828645a7e7925029f4-0.
2025-12-19 23:59:22,821 - vllm.entrypoints.logger - INFO - Received request cmpl-075d7c1fabb2483997cb6bf07799afec-0: prompt: "I'm glad you liked the list of books I gave you!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 15713, 498, 14915, 279, 1140, 315, 6467, 358, 6551, 498, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,822 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-2d065f8172a64ad6bb6f4c91558a4ce3-0.
2025-12-19 23:59:22,822 - vllm.entrypoints.logger - INFO - Received request cmpl-d9bda0d505064f639dcc03bf5735bcc7-0: prompt: "Hey, I went to James's house today.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 3937, 311, 7801, 594, 3753, 3351, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,823 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ae8aa48bf8b74d7babf5b9cba4a951ff-0.
2025-12-19 23:59:22,824 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-075d7c1fabb2483997cb6bf07799afec-0.
2025-12-19 23:59:22,825 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d9bda0d505064f639dcc03bf5735bcc7-0.
2025-12-19 23:59:22,827 - vllm.core.scheduler - INFO - Pending queue size: (732)
2025-12-19 23:59:22,839 - vllm.entrypoints.logger - INFO - Received request cmpl-4ae2abbf9b644901825353c4320c88ff-0: prompt: 'That chocolate bar was really delicious, thanks for letting me have it.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4792, 17931, 3619, 572, 2167, 17923, 11, 9339, 369, 20194, 752, 614, 432, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,850 - vllm.entrypoints.logger - INFO - Received request cmpl-825dee79990344dc846addc67d346bba-0: prompt: 'Child, I have something to tell you. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=6, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [3652, 11, 358, 614, 2494, 311, 3291, 498, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,855 - vllm.entrypoints.logger - INFO - Received request cmpl-ff75bca6928f445d9bf907640ccf36cf-0: prompt: 'Hey Boss, I wanted to talk to you about something.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 31569, 11, 358, 4829, 311, 3061, 311, 498, 911, 2494, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,858 - vllm.entrypoints.logger - INFO - Received request cmpl-9d8dd861e3e64752ad97102a6e25a31c-0: prompt: "Coach, I've been swimming more regularly lately and I feel great after each session!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [72694, 11, 358, 3003, 1012, 23380, 803, 15502, 30345, 323, 358, 2666, 2244, 1283, 1817, 3797, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,861 - vllm.entrypoints.logger - INFO - Received request cmpl-bfbfe48566904d10a78f4207bee4397b-0: prompt: "Hey, I came over to your house but it's empty. Do you know where you moved?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=22, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 3697, 916, 311, 697, 3753, 714, 432, 594, 4287, 13, 3155, 498, 1414, 1380, 498, 7726, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,871 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-4ae2abbf9b644901825353c4320c88ff-0.
2025-12-19 23:59:22,872 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-825dee79990344dc846addc67d346bba-0.
2025-12-19 23:59:22,873 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ff75bca6928f445d9bf907640ccf36cf-0.
2025-12-19 23:59:22,874 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9d8dd861e3e64752ad97102a6e25a31c-0.
2025-12-19 23:59:22,875 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bfbfe48566904d10a78f4207bee4397b-0.
2025-12-19 23:59:22,878 - vllm.core.scheduler - INFO - Pending queue size: (737)
2025-12-19 23:59:22,887 - vllm.entrypoints.logger - INFO - Received request cmpl-65d0f62d38e241b6834f41ed542768da-0: prompt: "I'm so relieved that our neighborhood is finally becoming a safer place.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 50412, 429, 1039, 12534, 374, 5499, 10454, 264, 29449, 1992, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,891 - vllm.entrypoints.logger - INFO - Received request cmpl-9effa46cb9654e0e969990f73e3577fd-0: prompt: 'Hey, can you go back to your own apartment for a bit? I need some alone time to think.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 646, 498, 728, 1182, 311, 697, 1828, 13154, 369, 264, 2699, 30, 358, 1184, 1045, 7484, 882, 311, 1744, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,908 - vllm.entrypoints.logger - INFO - Received request cmpl-73af01757fe24e40825e4c17a4f64c44-0: prompt: " I'm kind of avoiding you lately...", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [358, 2776, 3093, 315, 30426, 498, 30345, 1112], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,910 - vllm.entrypoints.logger - INFO - Received request cmpl-fa9e1577f680482d9f30f11a1090c0b9-0: prompt: 'Hey, I rejected that assignment yesterday. ', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 17551, 429, 16319, 13671, 13, 220], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,922 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-65d0f62d38e241b6834f41ed542768da-0.
2025-12-19 23:59:22,923 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-9effa46cb9654e0e969990f73e3577fd-0.
2025-12-19 23:59:22,923 - vllm.entrypoints.logger - INFO - Received request cmpl-7f0677f2530c4bae8cc8e56a320ec871-0: prompt: "Hey, have you listened to Classmates B's album yet? It's really good!", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 33693, 311, 3228, 16457, 425, 594, 8017, 3602, 30, 1084, 594, 2167, 1661, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,924 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-73af01757fe24e40825e4c17a4f64c44-0.
2025-12-19 23:59:22,925 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fa9e1577f680482d9f30f11a1090c0b9-0.
2025-12-19 23:59:22,926 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-7f0677f2530c4bae8cc8e56a320ec871-0.
2025-12-19 23:59:22,928 - vllm.core.scheduler - INFO - Pending queue size: (742)
2025-12-19 23:59:22,935 - vllm.entrypoints.logger - INFO - Received request cmpl-d6f9d53a5f0f42f49da0b771ec6748ae-0: prompt: 'Hey, have you ever had a situation where you needed to be patient?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3512, 1030, 264, 6534, 1380, 498, 4362, 311, 387, 8720, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,946 - vllm.entrypoints.logger - INFO - Received request cmpl-cd436abc7c0e42c18c934ffdd5083319-0: prompt: 'Hey, how was your day today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1246, 572, 697, 1899, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,957 - vllm.entrypoints.logger - INFO - Received request cmpl-f242c5c6be574519ba8c01343e4bfb9a-0: prompt: "I love living on the coast. It's so peaceful and relaxing to just sit on my porch and watch the waves.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2948, 5382, 389, 279, 13648, 13, 1084, 594, 773, 25650, 323, 33848, 311, 1101, 2444, 389, 847, 44647, 323, 3736, 279, 16876, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,963 - vllm.entrypoints.logger - INFO - Received request cmpl-f41b250cc521418e9eac6bf9211ce414-0: prompt: "So, this is how you enter the data into the system. It's pretty easy.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [4416, 11, 419, 374, 1246, 498, 3725, 279, 821, 1119, 279, 1849, 13, 1084, 594, 5020, 4135, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,972 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d6f9d53a5f0f42f49da0b771ec6748ae-0.
2025-12-19 23:59:22,974 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-cd436abc7c0e42c18c934ffdd5083319-0.
2025-12-19 23:59:22,975 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f242c5c6be574519ba8c01343e4bfb9a-0.
2025-12-19 23:59:22,976 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f41b250cc521418e9eac6bf9211ce414-0.
2025-12-19 23:59:22,978 - vllm.core.scheduler - INFO - Pending queue size: (746)
2025-12-19 23:59:22,982 - vllm.entrypoints.logger - INFO - Received request cmpl-b00f98e6dab244729adce9f2397b83aa-0: prompt: 'Hey, have you thought about our future lately?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=10, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 614, 498, 3381, 911, 1039, 3853, 30345, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,985 - vllm.entrypoints.logger - INFO - Received request cmpl-6dd11078aa364b59b752fcbb393ce7cc-0: prompt: "I feel like I'm constantly on edge. It's like I have no control over my anger anymore.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2666, 1075, 358, 2776, 14971, 389, 6821, 13, 1084, 594, 1075, 358, 614, 902, 2524, 916, 847, 19234, 14584, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:22,994 - vllm.entrypoints.logger - INFO - Received request cmpl-69f2aca9332b4916aef967bbb940dfee-0: prompt: "Oh man, I have a stomach-ache. I feel like I'm going to be sick.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [11908, 883, 11, 358, 614, 264, 22350, 12, 1777, 13, 358, 2666, 1075, 358, 2776, 2087, 311, 387, 14036, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,000 - vllm.entrypoints.logger - INFO - Received request cmpl-d9de5668ab2844f79351b12299cb9d9c-0: prompt: 'Hey, did you notice anything different about me today?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 1521, 498, 5293, 4113, 2155, 911, 752, 3351, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,017 - vllm.entrypoints.logger - INFO - Received request cmpl-3b5f2c2e5fb54f3aa8f22b8d04598149-0: prompt: 'Hey, mind if I sit here?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=5, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 3971, 421, 358, 2444, 1588, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,022 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b00f98e6dab244729adce9f2397b83aa-0.
2025-12-19 23:59:23,023 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-6dd11078aa364b59b752fcbb393ce7cc-0.
2025-12-19 23:59:23,024 - vllm.entrypoints.logger - INFO - Received request cmpl-fb806050a1f247b1bb62b60c0ea1675b-0: prompt: 'Hey, I wanted to show you this photo I took of you the other day.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=8, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 358, 4829, 311, 1473, 498, 419, 6548, 358, 3867, 315, 498, 279, 1008, 1899, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,025 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-69f2aca9332b4916aef967bbb940dfee-0.
2025-12-19 23:59:23,026 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-d9de5668ab2844f79351b12299cb9d9c-0.
2025-12-19 23:59:23,027 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-3b5f2c2e5fb54f3aa8f22b8d04598149-0.
2025-12-19 23:59:23,028 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-fb806050a1f247b1bb62b60c0ea1675b-0.
2025-12-19 23:59:23,028 - vllm.entrypoints.logger - INFO - Received request cmpl-24eb9902c87d4bd19b85f3c2f2ea9cf2-0: prompt: "I talked to the police yesterday and they said they're investigating the break-in.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=12, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 14897, 311, 279, 4282, 13671, 323, 807, 1053, 807, 2299, 23890, 279, 1438, 3419, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,030 - vllm.core.scheduler - INFO - Pending queue size: (752)
2025-12-19 23:59:23,035 - vllm.entrypoints.logger - INFO - Received request cmpl-f7c91e04b96d4af5b4ecd6e39f97c3ed-0: prompt: 'Hey, B! Guess what? I went on my first solo trip!', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=9, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [18665, 11, 425, 0, 54279, 1128, 30, 358, 3937, 389, 847, 1156, 13529, 8411, 0], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,037 - vllm.entrypoints.logger - INFO - Received request cmpl-f9d1a598d4bf420ab65f006e3eb8c322-0: prompt: "Hi Mentor, I've been traveling to Western Europe quite frequently and I absolutely love it there.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=17, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 91191, 11, 358, 3003, 1012, 20971, 311, 10867, 4505, 5008, 13814, 323, 358, 10875, 2948, 432, 1052, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,040 - vllm.entrypoints.logger - INFO - Received request cmpl-da81d4331ff1442fbdcc0d24d629da3f-0: prompt: "Mentor, I just wanted to let you know that I'm here for you if you need any help. I know that you're going through a tough time right now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=14, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1101, 4829, 311, 1077, 498, 1414, 429, 358, 2776, 1588, 369, 498, 421, 498, 1184, 894, 1492, 13, 358, 1414, 429, 498, 2299, 2087, 1526, 264, 11045, 882, 1290, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,043 - vllm.entrypoints.logger - INFO - Received request cmpl-ec2fc79c8ca64cc291a08e3852b25e40-0: prompt: 'Mentor, I just wanted to let you know that I appreciate our friendship. I consider myself a very loyal friend, and I always have your back.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=41, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 1101, 4829, 311, 1077, 498, 1414, 429, 358, 15401, 1039, 26509, 13, 358, 2908, 7037, 264, 1602, 28847, 4238, 11, 323, 358, 2677, 614, 697, 1182, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,054 - vllm.entrypoints.logger - INFO - Received request cmpl-aaa80394a5fe476fac63e18941f73c59-0: prompt: "I'm so excited for the state track meet next week, I can't believe I was chosen as one of the school's representatives.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=21, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 12035, 369, 279, 1584, 3754, 3367, 1790, 2003, 11, 358, 646, 944, 4411, 358, 572, 11882, 438, 825, 315, 279, 2906, 594, 23130, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,056 - vllm.entrypoints.logger - INFO - Received request cmpl-57adb97d0ddc4d8283021e70eb2b2920-0: prompt: "I'm so sorry for what I did, Neighbors B. I don't even know what came over me.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=29, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 2776, 773, 14589, 369, 1128, 358, 1521, 11, 4182, 24101, 425, 13, 358, 1513, 944, 1496, 1414, 1128, 3697, 916, 752, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,074 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-24eb9902c87d4bd19b85f3c2f2ea9cf2-0.
2025-12-19 23:59:23,075 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f7c91e04b96d4af5b4ecd6e39f97c3ed-0.
2025-12-19 23:59:23,076 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-f9d1a598d4bf420ab65f006e3eb8c322-0.
2025-12-19 23:59:23,077 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-da81d4331ff1442fbdcc0d24d629da3f-0.
2025-12-19 23:59:23,078 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-ec2fc79c8ca64cc291a08e3852b25e40-0.
2025-12-19 23:59:23,079 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-aaa80394a5fe476fac63e18941f73c59-0.
2025-12-19 23:59:23,080 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-57adb97d0ddc4d8283021e70eb2b2920-0.
2025-12-19 23:59:23,083 - vllm.core.scheduler - INFO - Pending queue size: (759)
2025-12-19 23:59:23,129 - vllm.core.scheduler - INFO - Pending queue size: (759)
2025-12-19 23:59:23,150 - vllm.entrypoints.logger - INFO - Received request cmpl-48ffa855204446359f5b89451381fe92-0: prompt: "I don't know what came over me. I just couldn't control my temper.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 1513, 944, 1414, 1128, 3697, 916, 752, 13, 358, 1101, 7691, 944, 2524, 847, 6797, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,154 - vllm.entrypoints.logger - INFO - Received request cmpl-988c08ee6c5f42cf985e9d4260fa5eda-0: prompt: "I've started walking to school every day now.", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=11, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [40, 3003, 3855, 11435, 311, 2906, 1449, 1899, 1431, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,159 - vllm.entrypoints.logger - INFO - Received request cmpl-bd4246237dda45dc94c00599c2e8077a-0: prompt: 'Hi Teacher! I had so much fun over the weekend creating Halloween costumes for my friends.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=32, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [13048, 29069, 0, 358, 1030, 773, 1753, 2464, 916, 279, 9001, 6825, 26613, 44088, 369, 847, 4780, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,160 - vllm.entrypoints.logger - INFO - Received request cmpl-b5a52ab783c34eae8821d2c48364a07b-0: prompt: 'Mentor, I have been thinking a lot about what will happen to me when I die.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=27, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [44, 306, 269, 11, 358, 614, 1012, 7274, 264, 2696, 911, 1128, 686, 3537, 311, 752, 979, 358, 2746, 13], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,161 - vllm.entrypoints.logger - INFO - Received request cmpl-8067ab8371cd4816b16fe56822c0934b-0: prompt: "Isn't the sunset beautiful?", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=13, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [87941, 944, 279, 42984, 6233, 30], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
2025-12-19 23:59:23,173 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-48ffa855204446359f5b89451381fe92-0.
2025-12-19 23:59:23,174 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-988c08ee6c5f42cf985e9d4260fa5eda-0.
2025-12-19 23:59:23,176 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-bd4246237dda45dc94c00599c2e8077a-0.
2025-12-19 23:59:23,177 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-b5a52ab783c34eae8821d2c48364a07b-0.
2025-12-19 23:59:23,178 - vllm.engine.multiprocessing.engine - INFO - Added request cmpl-8067ab8371cd4816b16fe56822c0934b-0.
2025-12-19 23:59:23,180 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,226 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,270 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,314 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,356 - vllm.core.scheduler - WARNING - Sequence group cmpl-3145c01785854586a825ef2de0483b3c-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301
2025-12-19 23:59:23,359 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,402 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,443 - vllm.engine.metrics - INFO - Avg prompt throughput: 370.1 tokens/s, Avg generation throughput: 20.1 tokens/s, Running: 512 reqs, Swapped: 302 reqs, Pending: 461 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 3.8%.
2025-12-19 23:59:23,450 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,492 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,536 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,577 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,622 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,666 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,709 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,751 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,794 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,839 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,881 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,923 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:23,965 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,007 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,053 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,096 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,140 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,182 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,225 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,266 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,309 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,354 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,396 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,442 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,484 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,526 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,568 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,609 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,653 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,695 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,736 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,778 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,821 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,863 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,905 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,948 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:24,991 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,035 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,077 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,120 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,162 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,203 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,245 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,288 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,330 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,373 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,415 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,458 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,498 - vllm.core.scheduler - WARNING - Sequence group cmpl-e2fcad807b484164ba7dc4d45e602386-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351
2025-12-19 23:59:25,500 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,541 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,584 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,626 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,667 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,710 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,752 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,794 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,838 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,881 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,923 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:25,965 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,006 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,050 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,092 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,135 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,178 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,219 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,261 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,303 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,345 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,388 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,430 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,474 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,516 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,558 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,600 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,643 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,685 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,727 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,771 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,813 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,855 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,899 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,942 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:26,985 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,028 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,070 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,112 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,154 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,201 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,243 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,287 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,329 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,371 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,418 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,463 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,507 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,552 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,596 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,640 - vllm.core.scheduler - WARNING - Sequence group cmpl-031b9fd1ce034362bd3f6727235e16b1-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401
2025-12-19 23:59:27,642 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,689 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,734 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,777 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,820 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,863 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,909 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,951 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:27,991 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,035 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,077 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,119 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,162 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,204 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,245 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,287 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,331 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,377 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,418 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,459 - vllm.engine.metrics - INFO - Avg prompt throughput: 437.8 tokens/s, Avg generation throughput: 23.3 tokens/s, Running: 512 reqs, Swapped: 419 reqs, Pending: 344 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 4.9%.
2025-12-19 23:59:28,461 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,504 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,547 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,590 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,631 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,673 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,715 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,758 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,799 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,842 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,886 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,938 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:28,980 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,023 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,065 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,107 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,148 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,190 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,233 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,276 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,318 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,360 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,402 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,444 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,486 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,529 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,570 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,612 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,655 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,697 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,739 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,780 - vllm.core.scheduler - WARNING - Sequence group cmpl-4f12ed464ba94ebc82a2a9ec33b53f2d-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451
2025-12-19 23:59:29,783 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,824 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,865 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,909 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,956 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:29,998 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,042 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,085 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,127 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,170 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,211 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,254 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,296 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,338 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,381 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,425 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,468 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,510 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,552 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,594 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,635 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,677 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,718 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,761 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,802 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,844 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,885 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,928 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:30,988 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,033 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,075 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,118 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,159 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,200 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,241 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,285 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,327 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,369 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,411 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,453 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,495 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,538 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,587 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,629 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,671 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,712 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,754 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,797 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,839 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,880 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,920 - vllm.core.scheduler - WARNING - Sequence group cmpl-f59bad176bea4a44b836435eb0c227da-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501
2025-12-19 23:59:31,925 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:31,970 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,015 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,058 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,104 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,148 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,193 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,234 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,282 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,323 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,364 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,407 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,451 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,494 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,536 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,580 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,623 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,664 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,704 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,746 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,791 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,833 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,875 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,917 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:32,960 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,003 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,048 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,089 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,138 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,180 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,223 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,265 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,307 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,349 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,392 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,434 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,475 - vllm.engine.metrics - INFO - Avg prompt throughput: 433.6 tokens/s, Avg generation throughput: 23.3 tokens/s, Running: 512 reqs, Swapped: 536 reqs, Pending: 227 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 6.1%.
2025-12-19 23:59:33,478 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,520 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,565 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,608 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,649 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,691 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,733 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,775 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,816 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,858 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,900 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,942 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:33,985 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,029 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,072 - vllm.core.scheduler - WARNING - Sequence group cmpl-215c2d4a03714c388f0d30d6207fa82d-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551
2025-12-19 23:59:34,074 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,115 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,156 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,198 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,240 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,282 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,325 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,366 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,408 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,448 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,490 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,531 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,574 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,615 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,660 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,702 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,745 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,787 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,828 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,870 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,912 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,953 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:34,995 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,039 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,084 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,126 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,167 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,209 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,251 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,293 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,334 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,376 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,418 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,460 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,501 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,542 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,585 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,628 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,671 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,712 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,753 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,797 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,841 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,882 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,923 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:35,965 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,006 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,050 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,093 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,134 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,174 - vllm.core.scheduler - WARNING - Sequence group cmpl-101d65fe012d4ede8c8e506a4bed3b51-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601
2025-12-19 23:59:36,177 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,218 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,260 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,302 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,344 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,386 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,427 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,470 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,511 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,552 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,594 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,636 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,678 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,721 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,763 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,805 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,848 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,891 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,932 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:36,974 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,017 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,060 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,103 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,145 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,187 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,229 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,271 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,313 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,355 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,396 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,437 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,479 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,521 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,563 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,605 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,647 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,690 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,732 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,774 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,815 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,857 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,899 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,944 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:37,986 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,028 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,069 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,113 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,153 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,197 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,239 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,280 - vllm.core.scheduler - WARNING - Sequence group cmpl-480e6d00e1794798a6fd05df16cbfd59-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651
2025-12-19 23:59:38,282 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,324 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,366 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,407 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,449 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,490 - vllm.engine.metrics - INFO - Avg prompt throughput: 448.4 tokens/s, Avg generation throughput: 23.7 tokens/s, Running: 512 reqs, Swapped: 655 reqs, Pending: 108 reqs, GPU KV cache usage: 5.3%, CPU KV cache usage: 7.3%.
2025-12-19 23:59:38,493 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,535 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,577 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,619 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,661 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,702 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,746 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,788 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,830 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,871 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,912 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,954 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:38,996 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,040 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,082 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,125 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,177 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,219 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,260 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,302 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,343 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,385 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,426 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,469 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,512 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,554 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,596 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,638 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,680 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,725 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,769 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,814 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,859 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,904 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,948 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:39,991 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,034 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,075 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,119 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,161 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,203 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,245 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,287 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,328 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,369 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,410 - vllm.core.scheduler - WARNING - Sequence group cmpl-b8014def6246416c9e552bb0c6162016-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701
2025-12-19 23:59:40,412 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,453 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,494 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,537 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,579 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,621 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,662 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,703 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,745 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,787 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,830 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,871 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,912 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,955 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:40,996 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,040 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,082 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,124 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,166 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,208 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,249 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,290 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,332 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,374 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,416 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,457 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,500 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,541 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,583 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,623 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,665 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,706 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,747 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,788 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,829 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,871 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,912 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,953 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:41,995 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,037 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,080 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,123 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,167 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,211 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,254 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,296 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,337 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,380 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,423 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,464 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,505 - vllm.core.scheduler - WARNING - Sequence group cmpl-3b5f2c2e5fb54f3aa8f22b8d04598149-0 is preempted by PreemptionMode.SWAP mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751
2025-12-19 23:59:42,507 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,548 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,589 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,631 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,673 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,714 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,755 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,797 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,838 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,880 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,921 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:42,962 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:43,004 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:43,049 - vllm.core.scheduler - INFO - Pending queue size: (763)
2025-12-19 23:59:43,160 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:43,269 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:43,376 - vllm.core.scheduler - INFO - Pending queue size: (764)
2025-12-19 23:59:43,495 - vllm.core.scheduler - INFO - Pending queue size: (758)
2025-12-19 23:59:43,620 - vllm.engine.metrics - INFO - Avg prompt throughput: 352.7 tokens/s, Avg generation throughput: 520.2 tokens/s, Running: 478 reqs, Swapped: 745 reqs, Pending: 0 reqs, GPU KV cache usage: 6.1%, CPU KV cache usage: 8.0%.
2025-12-19 23:59:43,629 - vllm.core.scheduler - INFO - Pending queue size: (746)
2025-12-19 23:59:43,795 - vllm.core.scheduler - INFO - Pending queue size: (712)
2025-12-19 23:59:43,934 - vllm.core.scheduler - INFO - Pending queue size: (689)
2025-12-19 23:59:44,086 - vllm.core.scheduler - INFO - Pending queue size: (666)
2025-12-19 23:59:44,249 - vllm.core.scheduler - INFO - Pending queue size: (636)
2025-12-19 23:59:44,427 - vllm.core.scheduler - INFO - Pending queue size: (596)
2025-12-19 23:59:44,598 - vllm.core.scheduler - INFO - Pending queue size: (555)
2025-12-19 23:59:44,774 - vllm.core.scheduler - INFO - Pending queue size: (506)
2025-12-19 23:59:44,927 - vllm.core.scheduler - INFO - Pending queue size: (470)
2025-12-19 23:59:45,108 - vllm.core.scheduler - INFO - Pending queue size: (413)
2025-12-19 23:59:45,281 - vllm.core.scheduler - INFO - Pending queue size: (361)
2025-12-19 23:59:45,450 - vllm.core.scheduler - INFO - Pending queue size: (316)
2025-12-19 23:59:45,602 - vllm.core.scheduler - INFO - Pending queue size: (282)
2025-12-19 23:59:45,754 - vllm.core.scheduler - INFO - Pending queue size: (246)
2025-12-19 23:59:45,907 - vllm.core.scheduler - INFO - Pending queue size: (214)
2025-12-19 23:59:46,063 - vllm.core.scheduler - INFO - Pending queue size: (175)
2025-12-19 23:59:46,208 - vllm.core.scheduler - INFO - Pending queue size: (147)
2025-12-19 23:59:46,364 - vllm.core.scheduler - INFO - Pending queue size: (110)
2025-12-19 23:59:46,518 - vllm.core.scheduler - INFO - Pending queue size: (71)
2025-12-19 23:59:46,662 - vllm.core.scheduler - INFO - Pending queue size: (39)
2025-12-19 23:59:46,817 - vllm.core.scheduler - INFO - Pending queue size: (13)
2025-12-19 23:59:46,954 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,050 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,142 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,226 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,307 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,383 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,456 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,523 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,588 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,649 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,708 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,761 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,816 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,867 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,917 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:47,963 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,011 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,057 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,101 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,145 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,187 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,230 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,271 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,314 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,357 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,400 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,444 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,487 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,528 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,570 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,611 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,652 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2856.2 tokens/s, Running: 3 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.1%, CPU KV cache usage: 0.0%.
2025-12-19 23:59:48,654 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,696 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,738 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,780 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,822 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,864 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:48,904 - vllm.core.scheduler - INFO - Pending queue size: (0)
2025-12-19 23:59:58,956 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 1.5 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
2025-12-20 00:00:08,970 - vllm.engine.metrics - INFO - Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.

============ Serving Benchmark Result ============
Successful requests:                     1500      
Benchmark duration (s):                  83.57     
Total input tokens:                      27355     
Total generated tokens:                  22032     
Request throughput (req/s):              17.95     
Input token throughput (tok/s):          327.34    
Output token throughput (tok/s):         263.64    
---------------Time to First Token----------------
Mean TTFT (ms):                          4863.70   
Median TTFT (ms):                        701.16    
P99 TTFT (ms):                           19466.36  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          2170.60   
Median TPOT (ms):                        1882.44   
P99 TPOT (ms):                           8893.83   
---------------Inter-token Latency----------------
Mean ITL (ms):                           1902.71   
Median ITL (ms):                         145.99    
P99 ITL (ms):                            37988.90  
==================================================